{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c722b7b",
   "metadata": {},
   "source": [
    "# Semantic Retrieval for Scientific Documents\n",
    "\n",
    "This notebook implements a deep learning-based semantic retrieval system, trained and evaluated on the SciFact dataset.\n",
    "\n",
    "## Project Overview\n",
    "- Fine-tune embedding models using sentence-transformers\n",
    "- Use MultipleNegativesRankingLoss loss function\n",
    "- Train and evaluate on BEIR-format datasets\n",
    "- Compare with traditional methods like BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca51d11",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8aa886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Colab, uncomment the line below to install dependencies\n",
    "!pip install sentence-transformers datasets pandas scikit-learn torch accelerate beir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa2b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Set, List, Tuple, Optional, Any, Union\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b917558d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "We use the BeIR/scifact-generated-queries dataset, a scientific literature retrieval dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb3ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scifact_raw(output_dir: str = \"./data/raw\"):\n",
    "    \"\"\"\n",
    "    Load BeIR/scifact-generated-queries dataset from HuggingFace\n",
    "    \"\"\"\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    raw_dir = Path(output_dir)\n",
    "    raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load dataset\n",
    "    logger.info(\"Loading SciFact dataset from HuggingFace...\")\n",
    "    ds = load_dataset(\"BeIR/scifact-generated-queries\")\n",
    "    df = ds[\"train\"].to_pandas()  # type: ignore\n",
    "\n",
    "    # Save raw data\n",
    "    raw_path = raw_dir / \"scifact_raw.csv\"\n",
    "    df.to_csv(raw_path, index=False)  # type: ignore\n",
    "    logger.info(f\"Raw data saved to {raw_path}\")\n",
    "    logger.info(f\"Dataset size: {len(df)} rows\")  # type: ignore\n",
    "\n",
    "    return df  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b6b378",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def preprocess_scifact(raw_df: pd.DataFrame, output_dir: str = \"./data/processed\"):\n",
    "    \"\"\"\n",
    "    Preprocess SciFact dataset and generate BEIR-format training data\n",
    "\n",
    "    Output files:\n",
    "    - scifact_pairs.csv: Query-document pairs\n",
    "    - scifact_corpus.csv: Deduplicated document corpus\n",
    "    - beir_format/corpus.jsonl: BEIR-format corpus\n",
    "    - beir_format/queries.jsonl: BEIR-format queries\n",
    "    - beir_format/qrels/train.tsv: Training set relevance labels (70%)\n",
    "    - beir_format/qrels/dev.tsv: Validation set relevance labels (10%)\n",
    "    - beir_format/qrels/test.tsv: Test set relevance labels (20%)\n",
    "    \"\"\"\n",
    "    processed_dir = Path(output_dir)\n",
    "    processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Process data\n",
    "    df = raw_df[[\"_id\", \"title\", \"text\", \"query\"]].copy()\n",
    "    df = df.dropna(subset=[\"text\", \"query\"])  # type: ignore\n",
    "    df = df.rename(columns={\"_id\": \"doc_id\"})\n",
    "\n",
    "    # Data cleaning\n",
    "    df[\"title\"] = df[\"title\"].fillna(\"\").astype(str).str.strip()\n",
    "    df[\"text\"] = df[\"text\"].fillna(\"\").astype(str).str.strip()\n",
    "    df[\"query\"] = df[\"query\"].astype(str).str.strip()\n",
    "\n",
    "    # Combine title and text\n",
    "    df[\"content\"] = df[\"title\"] + \". \" + df[\"text\"]\n",
    "\n",
    "    # Assign ID to each unique query\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"query_id\"] = pd.factorize(df[\"query\"])[0]\n",
    "\n",
    "    # Save query-document pairs\n",
    "    pairs_path = processed_dir / \"scifact_pairs.csv\"\n",
    "    df.to_csv(pairs_path, index=False)\n",
    "    logger.info(f\"Query-doc pairs saved to {pairs_path}\")\n",
    "\n",
    "    # Save deduplicated corpus\n",
    "    corpus = (\n",
    "        df[[\"doc_id\", \"content\"]]\n",
    "        .drop_duplicates(subset=[\"doc_id\"])  # type: ignore\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    corpus_path = processed_dir / \"scifact_corpus.csv\"\n",
    "    corpus.to_csv(corpus_path, index=False)\n",
    "    logger.info(f\"Corpus saved to {corpus_path} ({len(corpus)} documents)\")\n",
    "\n",
    "    # Create BEIR-format data\n",
    "    beir_dir = processed_dir / \"beir_format\"\n",
    "    beir_dir.mkdir(parents=True, exist_ok=True)\n",
    "    qrels_dir = beir_dir / \"qrels\"\n",
    "    qrels_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Write corpus.jsonl\n",
    "    corpus_jsonl_path = beir_dir / \"corpus.jsonl\"\n",
    "    with open(corpus_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in corpus.iterrows():\n",
    "            doc = {\n",
    "                \"_id\": str(row[\"doc_id\"]),\n",
    "                \"text\": row[\"content\"],\n",
    "                \"title\": \"\",\n",
    "            }\n",
    "            f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "    logger.info(f\"BEIR corpus saved to {corpus_jsonl_path}\")\n",
    "\n",
    "    # Write queries.jsonl\n",
    "    queries_jsonl_path = beir_dir / \"queries.jsonl\"\n",
    "    unique_queries = df[[\"query_id\", \"query\"]].drop_duplicates(subset=[\"query_id\"])  # type: ignore\n",
    "    with open(queries_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in unique_queries.iterrows():\n",
    "            query = {\"_id\": str(row[\"query_id\"]), \"text\": row[\"query\"]}\n",
    "            f.write(json.dumps(query, ensure_ascii=False) + \"\\n\")\n",
    "    logger.info(f\"BEIR queries saved to {queries_jsonl_path}\")\n",
    "\n",
    "    # Split into train/dev/test sets (70/10/20)\n",
    "    unique_query_ids = df[\"query_id\"].unique()\n",
    "    train_dev_query_ids, test_query_ids = train_test_split(\n",
    "        unique_query_ids, test_size=0.2, random_state=42\n",
    "    )\n",
    "    train_query_ids, dev_query_ids = train_test_split(\n",
    "        train_dev_query_ids, test_size=0.125, random_state=42\n",
    "    )\n",
    "\n",
    "    train_df = df[df[\"query_id\"].isin(train_query_ids)]\n",
    "    dev_df = df[df[\"query_id\"].isin(dev_query_ids)]\n",
    "    test_df = df[df[\"query_id\"].isin(test_query_ids)]\n",
    "\n",
    "    # Write qrels files\n",
    "    for split_name, split_df in [\n",
    "        (\"train\", train_df),\n",
    "        (\"dev\", dev_df),\n",
    "        (\"test\", test_df),\n",
    "    ]:\n",
    "        qrels_path = qrels_dir / f\"{split_name}.tsv\"\n",
    "        with open(qrels_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for _, row in split_df.iterrows():\n",
    "                f.write(f\"{row['query_id']}\\t{row['doc_id']}\\t1\\n\")\n",
    "        logger.info(f\"{split_name} qrels saved to {qrels_path} ({len(split_df)} pairs)\")\n",
    "\n",
    "    logger.info(f\"\\nData preprocessing completed!\")\n",
    "    logger.info(f\"Train: {len(train_query_ids)} queries, {len(train_df)} pairs\")\n",
    "    logger.info(f\"Dev: {len(dev_query_ids)} queries, {len(dev_df)} pairs\")\n",
    "    logger.info(f\"Test: {len(test_query_ids)} queries, {len(test_df)} pairs\")\n",
    "\n",
    "    return beir_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8908afcd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 3. Data Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qrels(qrels_path: Path) -> Dict[str, Set[str]]:\n",
    "    \"\"\"Load qrels file and return query_id -> {doc_ids} mapping\"\"\"\n",
    "    query_to_docs = defaultdict(set)\n",
    "\n",
    "    with open(qrels_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) >= 2:\n",
    "                query_id = parts[0]\n",
    "                doc_id = parts[1]\n",
    "                query_to_docs[query_id].add(doc_id)\n",
    "\n",
    "    return dict(query_to_docs)\n",
    "\n",
    "\n",
    "def load_queries(queries_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Load queries.jsonl file and return query_id -> query_text mapping\"\"\"\n",
    "    queries = {}\n",
    "\n",
    "    with open(queries_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            query = json.loads(line)\n",
    "            queries[query[\"_id\"]] = query[\"text\"]\n",
    "\n",
    "    return queries\n",
    "\n",
    "\n",
    "def load_corpus(corpus_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Load corpus.jsonl file and return doc_id -> doc_text mapping\"\"\"\n",
    "    corpus = {}\n",
    "\n",
    "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            corpus[doc[\"_id\"]] = doc[\"text\"]\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f82354",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Fine-tune embedding models using sentence-transformers library with MultipleNegativesRankingLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "MODEL_NAME = \"BAAI/bge-small-en-v1.5\"  # Base model\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 6\n",
    "WARMUP_RATIO = 0.1\n",
    "LEARNING_RATE = 3e-5\n",
    "OUTPUT_DIR = \"./models/finetuned-mnrl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54224d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(data_dir: Path) -> list:\n",
    "    \"\"\"\n",
    "    Load training data and convert to InputExample format\n",
    "\n",
    "    For MultipleNegativesRankingLoss, we only need (query, positive_doc) pairs.\n",
    "    Negative samples are drawn from other examples in the same batch.\n",
    "    \"\"\"\n",
    "    from sentence_transformers import InputExample\n",
    "\n",
    "    qrels_path = data_dir / \"qrels\" / \"train.tsv\"\n",
    "    queries_path = data_dir / \"queries.jsonl\"\n",
    "    corpus_path = data_dir / \"corpus.jsonl\"\n",
    "\n",
    "    logger.info(\"Loading training data...\")\n",
    "    query_to_docs = load_qrels(qrels_path)\n",
    "    queries = load_queries(queries_path)\n",
    "    corpus = load_corpus(corpus_path)\n",
    "\n",
    "    # Create training examples\n",
    "    examples = []\n",
    "    for query_id, doc_ids in query_to_docs.items():\n",
    "        if query_id not in queries:\n",
    "            continue\n",
    "\n",
    "        query_text = queries[query_id]\n",
    "\n",
    "        for doc_id in doc_ids:\n",
    "            if doc_id not in corpus:\n",
    "                continue\n",
    "\n",
    "            doc_text = corpus[doc_id]\n",
    "            examples.append(InputExample(texts=[query_text, doc_text]))\n",
    "\n",
    "    logger.info(f\"Created {len(examples)} training examples\")\n",
    "    return examples\n",
    "\n",
    "\n",
    "def create_evaluator(data_dir: Path, split: str = \"dev\"):\n",
    "    \"\"\"\n",
    "    Create InformationRetrievalEvaluator for validation during training\n",
    "    \"\"\"\n",
    "    from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "    qrels_path = data_dir / \"qrels\" / f\"{split}.tsv\"\n",
    "    queries_path = data_dir / \"queries.jsonl\"\n",
    "    corpus_path = data_dir / \"corpus.jsonl\"\n",
    "\n",
    "    logger.info(f\"Loading {split} data for evaluation...\")\n",
    "    query_to_docs = load_qrels(qrels_path)\n",
    "    queries = load_queries(queries_path)\n",
    "    corpus = load_corpus(corpus_path)\n",
    "\n",
    "    # Filter queries and documents\n",
    "    eval_queries = {qid: queries[qid] for qid in query_to_docs if qid in queries}\n",
    "    eval_corpus = corpus\n",
    "\n",
    "    # Convert qrels format\n",
    "    eval_qrels: Dict[str, Set[str]] = {\n",
    "        qid: set(doc_ids)\n",
    "        for qid, doc_ids in query_to_docs.items()\n",
    "        if qid in eval_queries\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Evaluator: {len(eval_queries)} queries, {len(eval_corpus)} documents\")\n",
    "\n",
    "    return InformationRetrievalEvaluator(\n",
    "        queries=eval_queries,\n",
    "        corpus=eval_corpus,\n",
    "        relevant_docs=eval_qrels,\n",
    "        name=split,\n",
    "        ndcg_at_k=[10, 100],\n",
    "        precision_recall_at_k=[10, 100],\n",
    "        map_at_k=[100],\n",
    "        mrr_at_k=[10],\n",
    "        show_progress_bar=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879c7f2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def train_model(data_dir: Path, output_dir: str = OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Train the embedding model\n",
    "    \"\"\"\n",
    "    from sentence_transformers import SentenceTransformer, losses\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load model\n",
    "    logger.info(f\"Loading model: {MODEL_NAME}\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "    # Load training data\n",
    "    train_examples = load_training_data(data_dir)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_examples,  # type: ignore\n",
    "        shuffle=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    # Setup loss function\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "    # Create validation evaluator\n",
    "    dev_evaluator = create_evaluator(data_dir, split=\"dev\")\n",
    "\n",
    "    # Calculate training steps\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "    logger.info(f\"\\nTraining configuration:\")\n",
    "    logger.info(f\"  Model: {MODEL_NAME}\")\n",
    "    logger.info(f\"  Batch size: {BATCH_SIZE}\")\n",
    "    logger.info(f\"  Epochs: {EPOCHS}\")\n",
    "    logger.info(f\"  Total steps: {total_steps}\")\n",
    "    logger.info(f\"  Warmup steps: {warmup_steps}\")\n",
    "    logger.info(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "    logger.info(f\"  Output directory: {output_path}\")\n",
    "\n",
    "    # Start training\n",
    "    logger.info(\"\\nStarting training...\")\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        evaluator=dev_evaluator,\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=warmup_steps,\n",
    "        optimizer_params={\"lr\": LEARNING_RATE},\n",
    "        output_path=str(output_path),\n",
    "        evaluation_steps=len(train_dataloader) // 2,  # Evaluate twice per epoch\n",
    "        save_best_model=True,\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "\n",
    "    logger.info(f\"\\nTraining completed! Model saved to: {output_path}\")\n",
    "\n",
    "    # Final evaluation on test set\n",
    "    logger.info(\"\\nRunning final evaluation on test set...\")\n",
    "    test_evaluator = create_evaluator(data_dir, split=\"test\")\n",
    "    test_results = test_evaluator(model, output_path=str(output_path))\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\" * 50)\n",
    "    logger.info(\"Final Test Results:\")\n",
    "    for metric, value in test_results.items():\n",
    "        logger.info(f\"  {metric}: {value:.4f}\")\n",
    "    logger.info(\"=\" * 50)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ab78bb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 5. Evaluation Metrics\n",
    "\n",
    "Implement standard information retrieval evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63006846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_eval(samples: List[Dict[str, Any]], k: int = 10) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate retrieval results\n",
    "\n",
    "    Args:\n",
    "        samples: List of samples containing queries and retrieval results\n",
    "            [{\n",
    "                \"question\": str,\n",
    "                \"contexts\": List[str],  # Ranked top-K doc_ids\n",
    "                \"ground_truth\": str | list | set  # Ground truth doc_id(s)\n",
    "            }]\n",
    "        k: Cutoff position for evaluation\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing various metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def to_set(gt: Union[str, List[str], Set[str]]) -> Set[str]:\n",
    "        if isinstance(gt, set):\n",
    "            return set(str(x) for x in gt)\n",
    "        if isinstance(gt, list):\n",
    "            return set(str(x) for x in gt)\n",
    "        return {str(gt)}\n",
    "\n",
    "    def hit_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
    "        return 1.0 if any(r in gt_set for r in results[:k]) else 0.0\n",
    "\n",
    "    def precision_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
    "        if k <= 0:\n",
    "            return 0.0\n",
    "        hits = sum(1 for r in results[:k] if r in gt_set)\n",
    "        return hits / k\n",
    "\n",
    "    def recall_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
    "        if not gt_set:\n",
    "            return 0.0\n",
    "        hits = sum(1 for r in results[:k] if r in gt_set)\n",
    "        return hits / len(gt_set)\n",
    "\n",
    "    def mrr(gt_set: Set[str], results: List[str]) -> float:\n",
    "        for rank, r in enumerate(results, start=1):\n",
    "            if r in gt_set:\n",
    "                return 1.0 / rank\n",
    "        return 0.0\n",
    "\n",
    "    def average_precision_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
    "        if not gt_set:\n",
    "            return 0.0\n",
    "\n",
    "        hits = 0\n",
    "        s = 0.0\n",
    "        for i, r in enumerate(results[:k], start=1):\n",
    "            if r in gt_set:\n",
    "                hits += 1\n",
    "                s += hits / i\n",
    "        return s / min(len(gt_set), k)\n",
    "\n",
    "    def ndcg_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
    "        # DCG\n",
    "        dcg = 0.0\n",
    "        for i, r in enumerate(results[:k], start=1):\n",
    "            rel = 1.0 if r in gt_set else 0.0\n",
    "            dcg += rel / math.log2(i + 1)\n",
    "\n",
    "        # IDCG\n",
    "        ideal_rels = [1.0] * min(len(gt_set), k)\n",
    "        idcg = 0.0\n",
    "        for i, rel in enumerate(ideal_rels, start=1):\n",
    "            idcg += rel / math.log2(i + 1)\n",
    "\n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    # Calculate metrics for all samples\n",
    "    hits, precisions, recalls, mrrs, aps, ndcgs = [], [], [], [], [], []\n",
    "\n",
    "    for s in samples:\n",
    "        gt_set = to_set(s[\"ground_truth\"])\n",
    "        results = [str(x) for x in s[\"contexts\"]]\n",
    "\n",
    "        hits.append(hit_at_k(gt_set, results, k))\n",
    "        precisions.append(precision_at_k(gt_set, results, k))\n",
    "        recalls.append(recall_at_k(gt_set, results, k))\n",
    "        mrrs.append(mrr(gt_set, results))\n",
    "        aps.append(average_precision_at_k(gt_set, results, k))\n",
    "        ndcgs.append(ndcg_at_k(gt_set, results, k))\n",
    "\n",
    "    n = len(samples) if samples else 1\n",
    "    return {\n",
    "        f\"Hit@{k}\": sum(hits) / n,\n",
    "        f\"Precision@{k}\": sum(precisions) / n,\n",
    "        f\"Recall@{k}\": sum(recalls) / n,\n",
    "        \"MRR\": sum(mrrs) / n,\n",
    "        f\"MAP@{k}\": sum(aps) / n,\n",
    "        f\"NDCG@{k}\": sum(ndcgs) / n,\n",
    "        \"N\": float(len(samples)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d206f",
   "metadata": {},
   "source": [
    "## 6. Complete Training and Evaluation Pipeline\n",
    "\n",
    "Run the code below to execute the complete workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce263780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load raw data\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"Step 1: Loading raw data\")\n",
    "logger.info(\"=\" * 60)\n",
    "raw_df = load_scifact_raw()\n",
    "\n",
    "# Display sample data\n",
    "logger.info(\"\\nSample data:\")\n",
    "print(raw_df.head())  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d86d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocess data\n",
    "logger.info(\"\\n\" + \"=\" * 60)\n",
    "logger.info(\"Step 2: Preprocessing data\")\n",
    "logger.info(\"=\" * 60)\n",
    "beir_dir = preprocess_scifact(raw_df)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train model\n",
    "logger.info(\"\\n\" + \"=\" * 60)\n",
    "logger.info(\"Step 3: Training model\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "# Comment out the line below if you want to skip training\n",
    "model = train_model(beir_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccdb728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load pre-trained model (if already trained)\n",
    "# Comment out this section if you already trained above\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer(OUTPUT_DIR)\n",
    "# logger.info(f\"Model loaded from {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5103c76",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 7. Semantic Retrieval with Trained Model\n",
    "\n",
    "Demonstration of how to use the trained model for semantic retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774b7f5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def semantic_search_demo(model, data_dir: Path, num_queries: int = 5):\n",
    "    \"\"\"\n",
    "    Demonstrate semantic search functionality\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sentence_transformers import util\n",
    "\n",
    "    # Load test data\n",
    "    queries_path = data_dir / \"queries.jsonl\"\n",
    "    corpus_path = data_dir / \"corpus.jsonl\"\n",
    "    qrels_path = data_dir / \"qrels\" / \"test.tsv\"\n",
    "\n",
    "    queries = load_queries(queries_path)\n",
    "    corpus = load_corpus(corpus_path)\n",
    "    query_to_docs = load_qrels(qrels_path)\n",
    "\n",
    "    # Encode corpus\n",
    "    logger.info(\"Encoding corpus...\")\n",
    "    corpus_ids = list(corpus.keys())\n",
    "    corpus_texts = [corpus[doc_id] for doc_id in corpus_ids]\n",
    "    corpus_embeddings = model.encode(\n",
    "        corpus_texts, convert_to_tensor=True, show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # Select test queries\n",
    "    test_queries = list(query_to_docs.keys())[:num_queries]\n",
    "\n",
    "    logger.info(f\"\\nRunning semantic search on {num_queries} test queries...\")\n",
    "\n",
    "    for query_id in test_queries:\n",
    "        query_text = queries[query_id]\n",
    "        ground_truth = query_to_docs[query_id]\n",
    "\n",
    "        # Encode query\n",
    "        query_embedding = model.encode(query_text, convert_to_tensor=True)\n",
    "\n",
    "        # Compute similarity and retrieve\n",
    "        cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "        top_results = np.argsort(-cos_scores.cpu().numpy())[:10]\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"Query: {query_text}\")\n",
    "        print(f\"Ground truth docs: {ground_truth}\")\n",
    "        print(\"\\nTop 10 retrieved documents:\")\n",
    "\n",
    "        for rank, idx in enumerate(top_results, 1):\n",
    "            doc_id = corpus_ids[idx]\n",
    "            score = cos_scores[idx].item()\n",
    "            is_relevant = \"✓\" if doc_id in ground_truth else \"✗\"\n",
    "            doc_preview = corpus[doc_id][:100] + \"...\"\n",
    "            print(f\"\\n{rank}. [{is_relevant}] Doc {doc_id} (score: {score:.4f})\")\n",
    "            print(f\"   {doc_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de742449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run retrieval demo\n",
    "# semantic_search_demo(model, beir_dir, num_queries=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd7cca",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 8. Results Visualization\n",
    "\n",
    "Visualize training results and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c8c95",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def plot_evaluation_results(results_csv_path: str):\n",
    "    \"\"\"\n",
    "    Plot evaluation results\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if not Path(results_csv_path).exists():\n",
    "        logger.warning(f\"Results file not found: {results_csv_path}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(results_csv_path)\n",
    "\n",
    "    # Plot main metrics\n",
    "    metrics = [\"ndcg_at_10\", \"map_at_100\", \"recall_at_10\", \"precision_at_10\"]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for i, metric in enumerate(metrics, 1):\n",
    "        if metric in df.columns:\n",
    "            plt.subplot(2, 2, i)\n",
    "            plt.plot(df[\"epoch\"], df[metric], marker=\"o\")\n",
    "            plt.title(metric.replace(\"_\", \" \").title())\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Score\")\n",
    "            plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e0225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results (if training logs exist)\n",
    "# results_path = f\"{OUTPUT_DIR}/eval/Information-Retrieval_evaluation_dev_results.csv\"\n",
    "# plot_evaluation_results(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f9bb86",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "This notebook implements a complete semantic retrieval system, including:\n",
    "\n",
    "1. **Data Loading and Preprocessing**: Load SciFact dataset from HuggingFace and convert to BEIR format\n",
    "2. **Model Training**: Fine-tune sentence-transformers model using MultipleNegativesRankingLoss\n",
    "3. **Evaluation**: Evaluate model performance on test set with multiple retrieval metrics\n",
    "4. **Inference**: Perform semantic retrieval using the trained model\n",
    "\n",
    "### Custom Configuration\n",
    "\n",
    "You can modify the following parameters to customize training:\n",
    "\n",
    "- `MODEL_NAME`: Base model (default: \"BAAI/bge-small-en-v1.5\")\n",
    "- `BATCH_SIZE`: Batch size (default: 32)\n",
    "- `EPOCHS`: Number of epochs (default: 6)\n",
    "- `LEARNING_RATE`: Learning rate (default: 3e-5)\n",
    "- `OUTPUT_DIR`: Model save path"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
