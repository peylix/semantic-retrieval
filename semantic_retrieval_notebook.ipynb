{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d58868",
   "metadata": {},
   "source": [
    "# Semantic Retrieval for Scientific Documents\n",
    "\n",
    "This notebook implements a deep learning-based semantic retrieval system, trained and evaluated on the SciFact dataset.\n",
    "\n",
    "## Project Overview\n",
    "- Fine-tune embedding models using sentence-transformers\n",
    "- Use MultipleNegativesRankingLoss loss function\n",
    "- Train and evaluate on BEIR-format datasets\n",
    "- Compare with traditional methods like BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1efa3b",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ba886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Colab, uncomment the line below to install dependencies\n",
    "!pip install sentence-transformers datasets pandas scikit-learn torch accelerate beir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d0fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Set, List, Tuple, Optional, Any, Union\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad87766",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "We use the BeIR/scifact-generated-queries dataset, a scientific literature retrieval dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a81f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scifact_raw(output_dir: str = \"./data/raw\"):\n",
    "    \"\"\"\n",
    "    Load BeIR/scifact-generated-queries dataset from HuggingFace\n",
    "    \"\"\"\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    raw_dir = Path(output_dir)\n",
    "    raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load dataset\n",
    "    logger.info(\"Loading SciFact dataset from HuggingFace...\")\n",
    "    ds = load_dataset(\"BeIR/scifact-generated-queries\")\n",
    "    df = ds[\"train\"].to_pandas()  # type: ignore\n",
    "\n",
    "    # Save raw data\n",
    "    raw_path = raw_dir / \"scifact_raw.csv\"\n",
    "    df.to_csv(raw_path, index=False)  # type: ignore\n",
    "    logger.info(f\"Raw data saved to {raw_path}\")\n",
    "    logger.info(f\"Dataset size: {len(df)} rows\")  # type: ignore\n",
    "\n",
    "    return df  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7124c79",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def preprocess_scifact(raw_df: pd.DataFrame, output_dir: str = \"./data/processed\"):\n",
    "    \"\"\"\n",
    "    Preprocess SciFact dataset and generate BEIR-format training data\n",
    "\n",
    "    Output files:\n",
    "    - scifact_pairs.csv: Query-document pairs\n",
    "    - scifact_corpus.csv: Deduplicated document corpus\n",
    "    - beir_format/corpus.jsonl: BEIR-format corpus\n",
    "    - beir_format/queries.jsonl: BEIR-format queries\n",
    "    - beir_format/qrels/train.tsv: Training set relevance labels (70%)\n",
    "    - beir_format/qrels/dev.tsv: Validation set relevance labels (10%)\n",
    "    - beir_format/qrels/test.tsv: Test set relevance labels (20%)\n",
    "    \"\"\"\n",
    "    processed_dir = Path(output_dir)\n",
    "    processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Process data\n",
    "    df = raw_df[[\"_id\", \"title\", \"text\", \"query\"]].copy()\n",
    "    df = df.dropna(subset=[\"text\", \"query\"])  # type: ignore\n",
    "    df = df.rename(columns={\"_id\": \"doc_id\"})\n",
    "\n",
    "    # Data cleaning\n",
    "    df[\"title\"] = df[\"title\"].fillna(\"\").astype(str).str.strip()\n",
    "    df[\"text\"] = df[\"text\"].fillna(\"\").astype(str).str.strip()\n",
    "    df[\"query\"] = df[\"query\"].astype(str).str.strip()\n",
    "\n",
    "    # Combine title and text\n",
    "    df[\"content\"] = df[\"title\"] + \". \" + df[\"text\"]\n",
    "\n",
    "    # Assign ID to each unique query\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"query_id\"] = pd.factorize(df[\"query\"])[0]\n",
    "\n",
    "    # Save query-document pairs\n",
    "    pairs_path = processed_dir / \"scifact_pairs.csv\"\n",
    "    df.to_csv(pairs_path, index=False)\n",
    "    logger.info(f\"Query-doc pairs saved to {pairs_path}\")\n",
    "\n",
    "    # Save deduplicated corpus\n",
    "    corpus = (\n",
    "        df[[\"doc_id\", \"content\"]]\n",
    "        .drop_duplicates(subset=[\"doc_id\"])  # type: ignore\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    corpus_path = processed_dir / \"scifact_corpus.csv\"\n",
    "    corpus.to_csv(corpus_path, index=False)\n",
    "    logger.info(f\"Corpus saved to {corpus_path} ({len(corpus)} documents)\")\n",
    "\n",
    "    # Create BEIR-format data\n",
    "    beir_dir = processed_dir / \"beir_format\"\n",
    "    beir_dir.mkdir(parents=True, exist_ok=True)\n",
    "    qrels_dir = beir_dir / \"qrels\"\n",
    "    qrels_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Write corpus.jsonl\n",
    "    corpus_jsonl_path = beir_dir / \"corpus.jsonl\"\n",
    "    with open(corpus_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in corpus.iterrows():\n",
    "            doc = {\n",
    "                \"_id\": str(row[\"doc_id\"]),\n",
    "                \"text\": row[\"content\"],\n",
    "                \"title\": \"\",\n",
    "            }\n",
    "            f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "    logger.info(f\"BEIR corpus saved to {corpus_jsonl_path}\")\n",
    "\n",
    "    # Write queries.jsonl\n",
    "    queries_jsonl_path = beir_dir / \"queries.jsonl\"\n",
    "    unique_queries = df[[\"query_id\", \"query\"]].drop_duplicates(subset=[\"query_id\"])  # type: ignore\n",
    "    with open(queries_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in unique_queries.iterrows():\n",
    "            query = {\"_id\": str(row[\"query_id\"]), \"text\": row[\"query\"]}\n",
    "            f.write(json.dumps(query, ensure_ascii=False) + \"\\n\")\n",
    "    logger.info(f\"BEIR queries saved to {queries_jsonl_path}\")\n",
    "\n",
    "    # Split into train/dev/test sets (70/10/20)\n",
    "    unique_query_ids = df[\"query_id\"].unique()\n",
    "    train_dev_query_ids, test_query_ids = train_test_split(\n",
    "        unique_query_ids, test_size=0.2, random_state=42\n",
    "    )\n",
    "    train_query_ids, dev_query_ids = train_test_split(\n",
    "        train_dev_query_ids, test_size=0.125, random_state=42\n",
    "    )\n",
    "\n",
    "    train_df = df[df[\"query_id\"].isin(train_query_ids)]\n",
    "    dev_df = df[df[\"query_id\"].isin(dev_query_ids)]\n",
    "    test_df = df[df[\"query_id\"].isin(test_query_ids)]\n",
    "\n",
    "    # Write qrels files\n",
    "    for split_name, split_df in [\n",
    "        (\"train\", train_df),\n",
    "        (\"dev\", dev_df),\n",
    "        (\"test\", test_df),\n",
    "    ]:\n",
    "        qrels_path = qrels_dir / f\"{split_name}.tsv\"\n",
    "        with open(qrels_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for _, row in split_df.iterrows():\n",
    "                f.write(f\"{row['query_id']}\\t{row['doc_id']}\\t1\\n\")\n",
    "        logger.info(f\"{split_name} qrels saved to {qrels_path} ({len(split_df)} pairs)\")\n",
    "\n",
    "    logger.info(f\"\\nData preprocessing completed!\")\n",
    "    logger.info(f\"Train: {len(train_query_ids)} queries, {len(train_df)} pairs\")\n",
    "    logger.info(f\"Dev: {len(dev_query_ids)} queries, {len(dev_df)} pairs\")\n",
    "    logger.info(f\"Test: {len(test_query_ids)} queries, {len(test_df)} pairs\")\n",
    "\n",
    "    return beir_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0041aa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 3. Data Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c28262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qrels(qrels_path: Path) -> Dict[str, Set[str]]:\n",
    "    \"\"\"Load qrels file and return query_id -> {doc_ids} mapping\"\"\"\n",
    "    query_to_docs = defaultdict(set)\n",
    "\n",
    "    with open(qrels_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) >= 2:\n",
    "                query_id = parts[0]\n",
    "                doc_id = parts[1]\n",
    "                query_to_docs[query_id].add(doc_id)\n",
    "\n",
    "    return dict(query_to_docs)\n",
    "\n",
    "\n",
    "def load_queries(queries_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Load queries.jsonl file and return query_id -> query_text mapping\"\"\"\n",
    "    queries = {}\n",
    "\n",
    "    with open(queries_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            query = json.loads(line)\n",
    "            queries[query[\"_id\"]] = query[\"text\"]\n",
    "\n",
    "    return queries\n",
    "\n",
    "\n",
    "def load_corpus(corpus_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Load corpus.jsonl file and return doc_id -> doc_text mapping\"\"\"\n",
    "    corpus = {}\n",
    "\n",
    "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            corpus[doc[\"_id\"]] = doc[\"text\"]\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b3acf",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Fine-tune embedding models using sentence-transformers library with MultipleNegativesRankingLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e1ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "MODEL_NAME = \"BAAI/bge-small-en-v1.5\"  # Base model\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "WARMUP_RATIO = 0.1\n",
    "LEARNING_RATE = 3e-5\n",
    "OUTPUT_DIR = \"/coontent/drive/semantic-retrieval/models/finetuned-mnrl\"\n",
    "\n",
    "IN_COLAB = \"google.colab\" in str(get_ipython())\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    BASE_DIR = Path(\"/content/drive/MyDrive/semantic-retrieval\")\n",
    "    OUTPUT_DIR = str(BASE_DIR / \"models/finetuned-mnrl\")\n",
    "    DATA_DIR = str(BASE_DIR / \"data\")\n",
    "    RESULTS_DIR = str(BASE_DIR / \"results\")\n",
    "else:\n",
    "    BASE_DIR = Path(\".\")\n",
    "    OUTPUT_DIR = \"./models/finetuned-mnrl\"\n",
    "    DATA_DIR = \"./data\"\n",
    "    RESULTS_DIR = \"./results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e758fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(data_dir: Path) -> list:\n",
    "    \"\"\"\n",
    "    Load training data and convert to InputExample format\n",
    "\n",
    "    For MultipleNegativesRankingLoss, we only need (query, positive_doc) pairs.\n",
    "    Negative samples are drawn from other examples in the same batch.\n",
    "    \"\"\"\n",
    "    from sentence_transformers import InputExample\n",
    "\n",
    "    qrels_path = data_dir / \"qrels\" / \"train.tsv\"\n",
    "    queries_path = data_dir / \"queries.jsonl\"\n",
    "    corpus_path = data_dir / \"corpus.jsonl\"\n",
    "\n",
    "    logger.info(\"Loading training data...\")\n",
    "    query_to_docs = load_qrels(qrels_path)\n",
    "    queries = load_queries(queries_path)\n",
    "    corpus = load_corpus(corpus_path)\n",
    "\n",
    "    # Create training examples\n",
    "    examples = []\n",
    "    for query_id, doc_ids in query_to_docs.items():\n",
    "        if query_id not in queries:\n",
    "            continue\n",
    "\n",
    "        query_text = queries[query_id]\n",
    "\n",
    "        for doc_id in doc_ids:\n",
    "            if doc_id not in corpus:\n",
    "                continue\n",
    "\n",
    "            doc_text = corpus[doc_id]\n",
    "            examples.append(InputExample(texts=[query_text, doc_text]))\n",
    "\n",
    "    logger.info(f\"Created {len(examples)} training examples\")\n",
    "    return examples\n",
    "\n",
    "\n",
    "def create_evaluator(data_dir: Path, split: str = \"dev\"):\n",
    "    \"\"\"\n",
    "    Create InformationRetrievalEvaluator for validation during training\n",
    "    \"\"\"\n",
    "    from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "    qrels_path = data_dir / \"qrels\" / f\"{split}.tsv\"\n",
    "    queries_path = data_dir / \"queries.jsonl\"\n",
    "    corpus_path = data_dir / \"corpus.jsonl\"\n",
    "\n",
    "    logger.info(f\"Loading {split} data for evaluation...\")\n",
    "    query_to_docs = load_qrels(qrels_path)\n",
    "    queries = load_queries(queries_path)\n",
    "    corpus = load_corpus(corpus_path)\n",
    "\n",
    "    # Filter queries and documents\n",
    "    eval_queries = {qid: queries[qid] for qid in query_to_docs if qid in queries}\n",
    "    eval_corpus = corpus\n",
    "\n",
    "    # Convert qrels format\n",
    "    eval_qrels: Dict[str, Set[str]] = {\n",
    "        qid: set(doc_ids)\n",
    "        for qid, doc_ids in query_to_docs.items()\n",
    "        if qid in eval_queries\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Evaluator: {len(eval_queries)} queries, {len(eval_corpus)} documents\")\n",
    "\n",
    "    return InformationRetrievalEvaluator(\n",
    "        queries=eval_queries,\n",
    "        corpus=eval_corpus,\n",
    "        relevant_docs=eval_qrels,\n",
    "        name=split,\n",
    "        ndcg_at_k=[10, 100],\n",
    "        precision_recall_at_k=[10, 100],\n",
    "        map_at_k=[100],\n",
    "        mrr_at_k=[10],\n",
    "        show_progress_bar=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir: Path, output_dir: str = OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Train the embedding model\n",
    "    \"\"\"\n",
    "    from sentence_transformers import SentenceTransformer, losses\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load model\n",
    "    logger.info(f\"Loading model: {MODEL_NAME}\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "    # Load training data\n",
    "    train_examples = load_training_data(data_dir)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_examples,  # type: ignore\n",
    "        shuffle=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    # Setup loss function\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "    # Create validation evaluator\n",
    "    dev_evaluator = create_evaluator(data_dir, split=\"dev\")\n",
    "\n",
    "    # Calculate training steps\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "    logger.info(f\"\\nTraining configuration:\")\n",
    "    logger.info(f\"  Model: {MODEL_NAME}\")\n",
    "    logger.info(f\"  Batch size: {BATCH_SIZE}\")\n",
    "    logger.info(f\"  Epochs: {EPOCHS}\")\n",
    "    logger.info(f\"  Total steps: {total_steps}\")\n",
    "    logger.info(f\"  Warmup steps: {warmup_steps}\")\n",
    "    logger.info(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "    logger.info(f\"  Output directory: {output_path}\")\n",
    "\n",
    "    # Start training\n",
    "    logger.info(\"\\nStarting training...\")\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        evaluator=dev_evaluator,\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=warmup_steps,\n",
    "        optimizer_params={\"lr\": LEARNING_RATE},\n",
    "        output_path=str(output_path),\n",
    "        evaluation_steps=len(train_dataloader) // 2,  # Evaluate twice per epoch\n",
    "        save_best_model=True,\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "\n",
    "    logger.info(f\"\\nTraining completed! Model saved to: {output_path}\")\n",
    "\n",
    "    # Final evaluation on test set\n",
    "    logger.info(\"\\nRunning final evaluation on test set...\")\n",
    "    test_evaluator = create_evaluator(data_dir, split=\"test\")\n",
    "    test_results = test_evaluator(model, output_path=str(output_path))\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\" * 50)\n",
    "    logger.info(\"Final Test Results:\")\n",
    "    for metric, value in test_results.items():\n",
    "        logger.info(f\"  {metric}: {value:.4f}\")\n",
    "    logger.info(\"=\" * 50)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a1d20",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics\n",
    "\n",
    "Implement standard information retrieval evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb204c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type alias for ground truth\n",
    "GroundTruth = Union[str, List[str], Set[str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2198a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_eval(samples: List[Dict[str, Any]], k: int = 10) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluation for retrieval with single or multi-ground-truth.\n",
    "\n",
    "    Args:\n",
    "        samples: List of samples in the format:\n",
    "            [{\n",
    "                \"question\": str,\n",
    "                \"contexts\": List[str],  # ranked top-K doc_ids\n",
    "                \"ground_truth\": str | list | set  # gold doc_id(s)\n",
    "            }]\n",
    "        k: Cutoff position for evaluation\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing various metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def to_set(gt: GroundTruth) -> Set[str]:\n",
    "        if isinstance(gt, set):\n",
    "            return set(str(x) for x in gt)\n",
    "        if isinstance(gt, list):\n",
    "            return set(str(x) for x in gt)\n",
    "        # single string / single id\n",
    "        return {str(gt)}\n",
    "\n",
    "    def hit_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
    "        return 1.0 if any(r in gt_set for r in results[:k]) else 0.0\n",
    "\n",
    "    def precision_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
    "        if k <= 0:\n",
    "            return 0.0\n",
    "        hits = sum(1 for r in results[:k] if r in gt_set)\n",
    "        return hits / k\n",
    "\n",
    "    def recall_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
    "        if not gt_set:\n",
    "            return 0.0\n",
    "        hits = sum(1 for r in results[:k] if r in gt_set)\n",
    "        return hits / len(gt_set)\n",
    "\n",
    "    def mrr(gt_set: Set[str], results: List[str]) -> float:\n",
    "        for rank, r in enumerate(results, start=1):\n",
    "            if r in gt_set:\n",
    "                return 1.0 / rank\n",
    "        return 0.0\n",
    "\n",
    "    def average_precision_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
    "        \"\"\"\n",
    "        AP@K for multi-relevant:\n",
    "          AP = (1 / min(|GT|, K)) * sum_{i=1..K} Precision@i * rel_i\n",
    "        \"\"\"\n",
    "        if not gt_set:\n",
    "            return 0.0\n",
    "\n",
    "        hits = 0\n",
    "        s = 0.0\n",
    "        for i, r in enumerate(results[:k], start=1):\n",
    "            if r in gt_set:\n",
    "                hits += 1\n",
    "                s += hits / i\n",
    "        return s / min(len(gt_set), k)\n",
    "\n",
    "    def ndcg_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
    "        # binary relevance\n",
    "        dcg = 0.0\n",
    "        for i, r in enumerate(results[:k], start=1):\n",
    "            rel = 1.0 if r in gt_set else 0.0\n",
    "            dcg += rel / math.log2(i + 1)\n",
    "\n",
    "        # ideal DCG: all relevant docs first\n",
    "        ideal_rels = [1.0] * min(len(gt_set), k)\n",
    "        idcg = 0.0\n",
    "        for i, rel in enumerate(ideal_rels, start=1):\n",
    "            idcg += rel / math.log2(i + 1)\n",
    "\n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    hits, precisions, recalls, mrrs, aps, ndcgs = [], [], [], [], [], []\n",
    "\n",
    "    for s in samples:\n",
    "        gt_set = to_set(s[\"ground_truth\"])\n",
    "        results = [str(x) for x in s[\"contexts\"]]\n",
    "\n",
    "        hits.append(hit_at_k(gt_set, results, k))\n",
    "        precisions.append(precision_at_k(gt_set, results, k))\n",
    "        recalls.append(recall_at_k(gt_set, results, k))\n",
    "        mrrs.append(mrr(gt_set, results))\n",
    "        aps.append(average_precision_at_k(gt_set, results, k))\n",
    "        ndcgs.append(ndcg_at_k(gt_set, results, k))\n",
    "\n",
    "    n = len(samples) if samples else 1\n",
    "    return {\n",
    "        f\"Hit@{k}\": sum(hits) / n,\n",
    "        f\"Precision@{k}\": sum(precisions) / n,\n",
    "        f\"Recall@{k}\": sum(recalls) / n,\n",
    "        \"MRR\": sum(mrrs) / n,\n",
    "        f\"MAP@{k}\": sum(aps) / n,\n",
    "        f\"NDCG@{k}\": sum(ndcgs) / n,\n",
    "        \"N\": float(len(samples)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe018b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_baseline_results(results_dir) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load baseline model results from JSON files\n",
    "\n",
    "    Args:\n",
    "        results_dir: Directory containing baseline result JSON files\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping model name to results\n",
    "    \"\"\"\n",
    "    results_path = Path(results_dir)\n",
    "    baseline_results = {}\n",
    "\n",
    "    if not results_path.exists():\n",
    "        logger.warning(f\"Results directory not found: {results_path}\")\n",
    "        return baseline_results\n",
    "\n",
    "    # Load all JSON files in results directory\n",
    "    for json_file in results_path.glob(\"*_results.json\"):\n",
    "        try:\n",
    "            with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                model_name = data.get(\"model_name\", json_file.stem)\n",
    "                baseline_results[model_name] = data\n",
    "                logger.info(f\"Loaded baseline results: {model_name}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load {json_file.name}: {e}\")\n",
    "\n",
    "    return baseline_results\n",
    "\n",
    "\n",
    "def display_baseline_comparison(baseline_results: Dict[str, Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Display comparison of baseline model results\n",
    "\n",
    "    Args:\n",
    "        baseline_results: Dictionary of baseline results from load_baseline_results\n",
    "    \"\"\"\n",
    "    if not baseline_results:\n",
    "        print(\n",
    "            \"No baseline results found. Please run baseline evaluation scripts first:\"\n",
    "        )\n",
    "        print(\"  - uv run src/embedding_model/zero_shot.py\")\n",
    "        print(\"  - uv run src/run_word2vec_eval.py\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BASELINE MODEL COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Collect all unique metrics\n",
    "    all_metrics = set()\n",
    "    for result in baseline_results.values():\n",
    "        all_metrics.update(result.get(\"metrics\", {}).keys())\n",
    "\n",
    "    # Remove 'N' from comparison metrics\n",
    "    all_metrics.discard(\"N\")\n",
    "    metric_list = sorted(all_metrics)\n",
    "\n",
    "    # Display results table\n",
    "    print(f\"\\n{'Model':<30}\", end=\"\")\n",
    "    for metric in metric_list:\n",
    "        print(f\"{metric:>12}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for model_name, result in baseline_results.items():\n",
    "        metrics = result.get(\"metrics\", {})\n",
    "        print(f\"{model_name:<30}\", end=\"\")\n",
    "        for metric in metric_list:\n",
    "            value = metrics.get(metric, 0.0)\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"{value:>12.4f}\", end=\"\")\n",
    "            else:\n",
    "                print(f\"{str(value):>12}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Display model details\n",
    "    print(\"\\nModel Details:\")\n",
    "    print(\"-\" * 80)\n",
    "    for model_name, result in baseline_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Type: {result.get('model_type', 'N/A')}\")\n",
    "        if \"base_model\" in result:\n",
    "            print(f\"  Base Model: {result['base_model']}\")\n",
    "        if \"parameters\" in result:\n",
    "            print(f\"  Parameters: {result['parameters']}\")\n",
    "        if \"N\" in result.get(\"metrics\", {}):\n",
    "            print(f\"  Test Samples: {int(result['metrics']['N'])}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef9fc34",
   "metadata": {},
   "source": [
    "## 6. Complete Training and Evaluation Pipeline\n",
    "\n",
    "Run the code below to execute the complete workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622083d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load raw data\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"Step 1: Loading raw data\")\n",
    "logger.info(\"=\" * 60)\n",
    "raw_df = load_scifact_raw()\n",
    "\n",
    "# Display sample data\n",
    "logger.info(\"\\nSample data:\")\n",
    "print(raw_df.head())  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6cfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocess data\n",
    "logger.info(\"\\n\" + \"=\" * 60)\n",
    "logger.info(\"Step 2: Preprocessing data\")\n",
    "logger.info(\"=\" * 60)\n",
    "beir_dir = preprocess_scifact(raw_df)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4fb4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train model\n",
    "logger.info(\"\\n\" + \"=\" * 60)\n",
    "logger.info(\"Step 3: Training model\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "# Comment out the line below if you want to skip training\n",
    "model = train_model(beir_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43704998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load pre-trained model (if already trained)\n",
    "# Comment out this section if you already trained above\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer(OUTPUT_DIR)\n",
    "# logger.info(f\"Model loaded from {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b203c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 7. Semantic Retrieval with Trained Model\n",
    "\n",
    "Demonstration of how to use the trained model for semantic retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d456291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search_demo(model, data_dir: Path, num_queries: int = 5):\n",
    "    \"\"\"\n",
    "    Demonstrate semantic search functionality\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sentence_transformers import util\n",
    "\n",
    "    # Load test data\n",
    "    queries_path = data_dir / \"queries.jsonl\"\n",
    "    corpus_path = data_dir / \"corpus.jsonl\"\n",
    "    qrels_path = data_dir / \"qrels\" / \"test.tsv\"\n",
    "\n",
    "    queries = load_queries(queries_path)\n",
    "    corpus = load_corpus(corpus_path)\n",
    "    query_to_docs = load_qrels(qrels_path)\n",
    "\n",
    "    # Encode corpus\n",
    "    logger.info(\"Encoding corpus...\")\n",
    "    corpus_ids = list(corpus.keys())\n",
    "    corpus_texts = [corpus[doc_id] for doc_id in corpus_ids]\n",
    "    corpus_embeddings = model.encode(\n",
    "        corpus_texts, convert_to_tensor=True, show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # Select test queries\n",
    "    test_queries = list(query_to_docs.keys())[:num_queries]\n",
    "\n",
    "    logger.info(f\"\\nRunning semantic search on {num_queries} test queries...\")\n",
    "\n",
    "    for query_id in test_queries:\n",
    "        query_text = queries[query_id]\n",
    "        ground_truth = query_to_docs[query_id]\n",
    "\n",
    "        # Encode query\n",
    "        query_embedding = model.encode(query_text, convert_to_tensor=True)\n",
    "\n",
    "        # Compute similarity and retrieve\n",
    "        cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "        top_results = np.argsort(-cos_scores.cpu().numpy())[:10]\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"Query: {query_text}\")\n",
    "        print(f\"Ground truth docs: {ground_truth}\")\n",
    "        print(\"\\nTop 10 retrieved documents:\")\n",
    "\n",
    "        for rank, idx in enumerate(top_results, 1):\n",
    "            doc_id = corpus_ids[idx]\n",
    "            score = cos_scores[idx].item()\n",
    "            is_relevant = \"✓\" if doc_id in ground_truth else \"✗\"\n",
    "            doc_preview = corpus[doc_id][:100] + \"...\"\n",
    "            print(f\"\\n{rank}. [{is_relevant}] Doc {doc_id} (score: {score:.4f})\")\n",
    "            print(f\"   {doc_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879f102",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Run retrieval demo\n",
    "semantic_search_demo(model, beir_dir, num_queries=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed58b19",
   "metadata": {},
   "source": [
    "## 7.3. Load and Display Baseline Results\n",
    "\n",
    "Before evaluating our fine-tuned model, let's load the baseline results for comparison.\n",
    "\n",
    "**Note**: Make sure to run the baseline evaluation scripts first:\n",
    "- `uv run src/embedding_model/zero_shot.py` - Zero-shot BGE baseline\n",
    "- `uv run src/run_word2vec_eval.py` - Word2Vec baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d843190",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load baseline results\n",
    "logger.info(\"Loading baseline model results...\")\n",
    "baseline_results = load_baseline_results(results_dir=RESULTS_DIR)\n",
    "\n",
    "# Display baseline comparison\n",
    "display_baseline_comparison(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2b58a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 7.5. Model Evaluation with traditional_eval\n",
    "Use the `traditional_eval` function to comprehensively evaluate the model's retrieval performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5941b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_retrieval(model, data_dir: Path, split: str = \"test\", k: int = 10):\n",
    "    \"\"\"\n",
    "    Evaluate model using traditional_eval function\n",
    "\n",
    "    Args:\n",
    "        model: Trained SentenceTransformer model\n",
    "        data_dir: Path to BEIR format data directory\n",
    "        split: Which split to evaluate on (\"test\", \"dev\", or \"train\")\n",
    "        k: Cutoff position for evaluation metrics\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sentence_transformers import util\n",
    "\n",
    "    # Load data\n",
    "    queries_path = data_dir / \"queries.jsonl\"\n",
    "    corpus_path = data_dir / \"corpus.jsonl\"\n",
    "    qrels_path = data_dir / \"qrels\" / f\"{split}.tsv\"\n",
    "\n",
    "    logger.info(f\"Loading {split} set for evaluation...\")\n",
    "    queries = load_queries(queries_path)\n",
    "    corpus = load_corpus(corpus_path)\n",
    "    query_to_docs = load_qrels(qrels_path)\n",
    "\n",
    "    # Encode corpus once\n",
    "    logger.info(\"Encoding corpus...\")\n",
    "    corpus_ids = list(corpus.keys())\n",
    "    corpus_texts = [corpus[doc_id] for doc_id in corpus_ids]\n",
    "    corpus_embeddings = model.encode(\n",
    "        corpus_texts, convert_to_tensor=True, show_progress_bar=True, batch_size=64\n",
    "    )\n",
    "\n",
    "    # Build evaluation samples\n",
    "    logger.info(f\"Running retrieval for {len(query_to_docs)} queries...\")\n",
    "    samples = []\n",
    "\n",
    "    for i, (query_id, gt_docs) in enumerate(query_to_docs.items(), 1):\n",
    "        # Get query text\n",
    "        query_text = queries.get(query_id)\n",
    "        if not query_text:\n",
    "            logger.warning(f\"Query {query_id} not found in queries.jsonl\")\n",
    "            continue\n",
    "\n",
    "        # Encode query\n",
    "        query_embedding = model.encode(query_text, convert_to_tensor=True)\n",
    "\n",
    "        # Retrieve top-k documents\n",
    "        cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "        top_indices = np.argsort(-cos_scores.cpu().numpy())[:k]\n",
    "        top_docs = [corpus_ids[idx] for idx in top_indices]\n",
    "\n",
    "        # Create sample for evaluation\n",
    "        samples.append(\n",
    "            {\"question\": query_text, \"contexts\": top_docs, \"ground_truth\": gt_docs}\n",
    "        )\n",
    "\n",
    "        # Progress update\n",
    "        if i % 100 == 0:\n",
    "            logger.info(f\"  Processed {i}/{len(query_to_docs)} queries\")\n",
    "\n",
    "    # Run evaluation\n",
    "    logger.info(f\"\\nEvaluating with traditional_eval (k={k})...\")\n",
    "    metrics = traditional_eval(samples, k=k)\n",
    "\n",
    "    return metrics, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d68c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation_results(\n",
    "    metrics: Dict[str, float], title: str = \"Evaluation Results\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Display evaluation results in a formatted table\n",
    "\n",
    "    Args:\n",
    "        metrics: Dictionary of metric name -> value\n",
    "        title: Title for the results display\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"{title:^70}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Group metrics by type\n",
    "    metric_groups = {\n",
    "        \"Retrieval Accuracy\": [\"Hit@10\"],\n",
    "        \"Precision & Recall\": [\"Precision@10\", \"Recall@10\"],\n",
    "        \"Ranking Quality\": [\"MRR\", \"MAP@10\", \"NDCG@10\"],\n",
    "        \"Dataset Info\": [\"N\"],\n",
    "    }\n",
    "\n",
    "    for group_name, metric_names in metric_groups.items():\n",
    "        print(f\"\\n{group_name}:\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        for metric_name in metric_names:\n",
    "            # Try different possible keys\n",
    "            possible_keys = [metric_name, metric_name.replace(\"@\", \"_at_\")]\n",
    "\n",
    "            for key in possible_keys:\n",
    "                if key in metrics:\n",
    "                    value = metrics[key]\n",
    "                    if key == \"N\":\n",
    "                        print(f\"  {metric_name:.<40} {int(value)}\")\n",
    "                    else:\n",
    "                        print(f\"  {metric_name:.<40} {value:.4f}\")\n",
    "                    break\n",
    "\n",
    "    print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_baseline(\n",
    "    finetuned_metrics: Dict[str, float],\n",
    "    baseline_metrics: Optional[Dict[str, float]] = None,\n",
    "    baseline_results: Optional[Dict[str, Dict[str, Any]]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare fine-tuned model with baseline\n",
    "\n",
    "    Args:\n",
    "        finetuned_metrics: Metrics from fine-tuned model\n",
    "        baseline_metrics: Metrics from baseline (optional, for backward compatibility)\n",
    "        baseline_results: All baseline results from load_baseline_results (optional)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Try to use loaded baseline results first\n",
    "    actual_baseline_metrics = baseline_metrics\n",
    "    if baseline_results is not None and \"Zero-shot BGE-small\" in baseline_results:\n",
    "        actual_baseline_metrics = baseline_results[\"Zero-shot BGE-small\"][\"metrics\"]\n",
    "        logger.info(\"Using loaded Zero-shot BGE-small baseline metrics\")\n",
    "    elif actual_baseline_metrics is None:\n",
    "        # Default baseline metrics (zero-shot BAAI/bge-small-en-v1.5)\n",
    "        # These are example values - replace with actual baseline results\n",
    "        actual_baseline_metrics = {\n",
    "            \"Hit@10\": 0.75,\n",
    "            \"Precision@10\": 0.15,\n",
    "            \"Recall@10\": 0.70,\n",
    "            \"MRR\": 0.65,\n",
    "            \"MAP@10\": 0.55,\n",
    "            \"NDCG@10\": 0.68,\n",
    "        }\n",
    "        logger.info(\"Using default baseline metrics (approximate)\")\n",
    "\n",
    "    # Metrics to compare\n",
    "    metric_names = [\"Hit@10\", \"Precision@10\", \"Recall@10\", \"MRR\", \"MAP@10\", \"NDCG@10\"]\n",
    "\n",
    "    # Extract values\n",
    "    baseline_values = []\n",
    "    finetuned_values = []\n",
    "    display_names = []\n",
    "\n",
    "    for metric in metric_names:\n",
    "        # Try different key formats\n",
    "        possible_keys = [metric, metric.replace(\"@\", \"_at_\")]\n",
    "\n",
    "        for key in possible_keys:\n",
    "            if key in actual_baseline_metrics and key in finetuned_metrics:\n",
    "                baseline_values.append(actual_baseline_metrics[key])\n",
    "                finetuned_values.append(finetuned_metrics[key])\n",
    "                display_names.append(metric)\n",
    "                break\n",
    "\n",
    "    if not display_names:\n",
    "        logger.warning(\"No common metrics found for comparison\")\n",
    "        print(\n",
    "            \"\\nError: No matching metrics found between baseline and fine-tuned model.\"\n",
    "        )\n",
    "        print(\n",
    "            \"Available baseline metrics:\",\n",
    "            list(actual_baseline_metrics.keys()) if actual_baseline_metrics else \"None\",\n",
    "        )\n",
    "        print(\"Available fine-tuned metrics:\", list(finetuned_metrics.keys()))\n",
    "        return\n",
    "\n",
    "    # Create comparison plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Bar chart comparison\n",
    "    x = range(len(display_names))\n",
    "    width = 0.35\n",
    "\n",
    "    ax1.bar(\n",
    "        [i - width / 2 for i in x], baseline_values, width, label=\"Baseline\", alpha=0.8\n",
    "    )\n",
    "    ax1.bar(\n",
    "        [i + width / 2 for i in x],\n",
    "        finetuned_values,\n",
    "        width,\n",
    "        label=\"Fine-tuned\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    ax1.set_xlabel(\"Metrics\")\n",
    "    ax1.set_ylabel(\"Score\")\n",
    "    ax1.set_title(\"Baseline vs Fine-tuned Model\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(display_names, rotation=45, ha=\"right\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Improvement percentage (handle division by zero)\n",
    "    improvements = []\n",
    "    for b, f in zip(baseline_values, finetuned_values):\n",
    "        if b == 0:\n",
    "            # If baseline is 0, use absolute difference instead\n",
    "            improvements.append(f * 100)\n",
    "        else:\n",
    "            improvements.append((f - b) / b * 100)\n",
    "\n",
    "    colors = [\"green\" if imp > 0 else \"red\" for imp in improvements]\n",
    "\n",
    "    ax2.bar(x, improvements, color=colors, alpha=0.7)\n",
    "    ax2.set_xlabel(\"Metrics\")\n",
    "    ax2.set_ylabel(\"Improvement (%)\")\n",
    "    ax2.set_title(\"Relative Improvement over Baseline\")\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(display_names, rotation=45, ha=\"right\")\n",
    "    ax2.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print improvement summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Improvement Summary\")\n",
    "    print(\"=\" * 70)\n",
    "    for name, baseline, finetuned, improvement in zip(\n",
    "        display_names, baseline_values, finetuned_values, improvements\n",
    "    ):\n",
    "        if baseline == 0:\n",
    "            # Special display for zero baseline\n",
    "            print(f\"{name:.<30} {baseline:.4f} → {finetuned:.4f} (baseline was 0)\")\n",
    "        else:\n",
    "            arrow = \"↑\" if improvement > 0 else \"↓\"\n",
    "            print(\n",
    "                f\"{name:.<30} {baseline:.4f} → {finetuned:.4f} ({arrow} {abs(improvement):.2f}%)\"\n",
    "            )\n",
    "    print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae516193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_models(\n",
    "    finetuned_metrics: Dict[str, float],\n",
    "    baseline_results_dict: Dict[str, Dict[str, Any]],\n",
    "    finetuned_name: str = \"Fine-tuned Model\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare fine-tuned model with all baseline models\n",
    "\n",
    "    Args:\n",
    "        finetuned_metrics: Metrics from fine-tuned model\n",
    "        baseline_results_dict: All baseline results from load_baseline_results\n",
    "        finetuned_name: Name for the fine-tuned model\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    if not baseline_results_dict:\n",
    "        logger.warning(\"No baseline results available for comparison\")\n",
    "        return\n",
    "\n",
    "    # Prepare data for comparison\n",
    "    models_data = {}\n",
    "\n",
    "    # Add baselines\n",
    "    for model_name, result in baseline_results_dict.items():\n",
    "        models_data[model_name] = result.get(\"metrics\", {})\n",
    "\n",
    "    # Add fine-tuned model\n",
    "    models_data[finetuned_name] = finetuned_metrics\n",
    "\n",
    "    # Metrics to compare\n",
    "    metric_names = [\"Hit@10\", \"Precision@10\", \"Recall@10\", \"MRR\", \"MAP@10\", \"NDCG@10\"]\n",
    "\n",
    "    # Prepare plotting data\n",
    "    model_names = list(models_data.keys())\n",
    "    n_models = len(model_names)\n",
    "    n_metrics = len(metric_names)\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_models))  # type: ignore\n",
    "\n",
    "    for idx, metric in enumerate(metric_names):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        values = []\n",
    "        labels = []\n",
    "\n",
    "        for model_name in model_names:\n",
    "            metrics = models_data[model_name]\n",
    "            # Try both formats\n",
    "            value = metrics.get(metric, metrics.get(metric.replace(\"@\", \"_at_\"), 0.0))\n",
    "            if isinstance(value, (int, float)):\n",
    "                values.append(value)\n",
    "                labels.append(model_name)\n",
    "\n",
    "        if values:\n",
    "            bars = ax.bar(\n",
    "                range(len(values)), values, color=colors[: len(values)], alpha=0.8\n",
    "            )\n",
    "            ax.set_title(metric, fontsize=12, fontweight=\"bold\")\n",
    "            ax.set_ylabel(\"Score\", fontsize=10)\n",
    "            ax.set_xticks(range(len(labels)))\n",
    "            ax.set_xticklabels(labels, rotation=45, ha=\"right\", fontsize=9)\n",
    "            ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "            # Add value labels on bars\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(\n",
    "                    bar.get_x() + bar.get_width() / 2.0,\n",
    "                    height,\n",
    "                    f\"{height:.3f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=8,\n",
    "                )\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Model Comparison: Fine-tuned vs Baselines\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        y=1.00,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed comparison table\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"DETAILED MODEL COMPARISON\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Header\n",
    "    print(f\"\\n{'Model':<25}\", end=\"\")\n",
    "    for metric in metric_names:\n",
    "        print(f\"{metric:>12}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Data rows\n",
    "    for model_name in model_names:\n",
    "        metrics = models_data[model_name]\n",
    "        print(f\"{model_name:<25}\", end=\"\")\n",
    "        for metric in metric_names:\n",
    "            value = metrics.get(metric, metrics.get(metric.replace(\"@\", \"_at_\"), 0.0))\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"{value:>12.4f}\", end=\"\")\n",
    "            else:\n",
    "                print(f\"{str(value):>12}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "    print(\"=\" * 100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191995fc",
   "metadata": {},
   "source": [
    "### Run Complete Evaluation\n",
    "\n",
    "Evaluate the fine-tuned model on the test set using `traditional_eval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0fe155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"Running complete evaluation on test set\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "test_metrics, test_samples = evaluate_model_retrieval(\n",
    "    model=model, data_dir=beir_dir, split=\"test\", k=10\n",
    ")\n",
    "\n",
    "# Display results\n",
    "display_evaluation_results(test_metrics, title=\"Test Set Evaluation Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Evaluate on dev set as well\n",
    "logger.info(\"Running evaluation on dev set for comparison...\")\n",
    "\n",
    "dev_metrics, dev_samples = evaluate_model_retrieval(\n",
    "    model=model, data_dir=beir_dir, split=\"dev\", k=10\n",
    ")\n",
    "\n",
    "display_evaluation_results(dev_metrics, title=\"Dev Set Evaluation Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958cd1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with baseline (zero-shot only)\n",
    "compare_with_baseline(test_metrics, baseline_results=baseline_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c455d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Compare with all baseline models\n",
    "logger.info(\"Generating comprehensive model comparison...\")\n",
    "compare_all_models(\n",
    "    test_metrics, baseline_results, finetuned_name=\"Fine-tuned BGE-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a640f46",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Analyze Individual Query Performance\n",
    "\n",
    "Examine specific queries to understand model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0abbc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query_performance(samples: List[Dict[str, Any]], num_examples: int = 5):\n",
    "    \"\"\"\n",
    "    Analyze performance on individual queries\n",
    "\n",
    "    Args:\n",
    "        samples: Evaluation samples from evaluate_model_retrieval\n",
    "        num_examples: Number of examples to display\n",
    "    \"\"\"\n",
    "    # Calculate per-query metrics\n",
    "    query_metrics = []\n",
    "\n",
    "    for sample in samples:\n",
    "        gt_set = set(str(x) for x in sample[\"ground_truth\"])\n",
    "        results = [str(x) for x in sample[\"contexts\"]]\n",
    "\n",
    "        # Hit@10\n",
    "        hit = 1.0 if any(r in gt_set for r in results[:10]) else 0.0\n",
    "\n",
    "        # Recall@10\n",
    "        hits_count = sum(1 for r in results[:10] if r in gt_set)\n",
    "        recall = hits_count / len(gt_set) if gt_set else 0.0\n",
    "\n",
    "        # MRR\n",
    "        mrr = 0.0\n",
    "        for rank, r in enumerate(results, start=1):\n",
    "            if r in gt_set:\n",
    "                mrr = 1.0 / rank\n",
    "                break\n",
    "\n",
    "        query_metrics.append(\n",
    "            {\n",
    "                \"query\": sample[\"question\"],\n",
    "                \"hit\": hit,\n",
    "                \"recall\": recall,\n",
    "                \"mrr\": mrr,\n",
    "                \"num_relevant\": len(gt_set),\n",
    "                \"retrieved\": results[:10],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Sort by MRR (worst first)\n",
    "    query_metrics.sort(key=lambda x: x[\"mrr\"])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Worst {num_examples} Queries (by MRR)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, qm in enumerate(query_metrics[:num_examples], 1):\n",
    "        print(f\"\\n{i}. Query: {qm['query'][:80]}...\")\n",
    "        print(\n",
    "            f\"   Hit@10: {qm['hit']:.0f} | Recall@10: {qm['recall']:.4f} | MRR: {qm['mrr']:.4f}\"\n",
    "        )\n",
    "        print(f\"   Relevant docs: {qm['num_relevant']}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Best {num_examples} Queries (by MRR)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, qm in enumerate(query_metrics[-num_examples:][::-1], 1):\n",
    "        print(f\"\\n{i}. Query: {qm['query'][:80]}...\")\n",
    "        print(\n",
    "            f\"   Hit@10: {qm['hit']:.0f} | Recall@10: {qm['recall']:.4f} | MRR: {qm['mrr']:.4f}\"\n",
    "        )\n",
    "        print(f\"   Relevant docs: {qm['num_relevant']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d7df8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Analyze query performance\n",
    "analyze_query_performance(test_samples, num_examples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f52a1",
   "metadata": {},
   "source": [
    "## 8. Results Visualization\n",
    "\n",
    "Visualize training results and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa89c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib for notebook display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# For Jupyter/IPython environments\n",
    "try:\n",
    "    from IPython.core.getipython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    if ipython is not None:\n",
    "        ipython.run_line_magic(\"matplotlib\", \"inline\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Set default style\n",
    "plt.style.use(\"default\")\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c106e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_results(results_csv_path: str):\n",
    "    \"\"\"\n",
    "    Plot evaluation results\n",
    "\n",
    "    Args:\n",
    "        results_csv_path: Path to the evaluation results CSV file\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if not Path(results_csv_path).exists():\n",
    "        logger.warning(f\"Results file not found: {results_csv_path}\")\n",
    "        print(f\"File not found: {results_csv_path}\")\n",
    "        print(\n",
    "            f\"Please make sure the model has been trained and evaluation results exist.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Read results\n",
    "    df = pd.read_csv(results_csv_path)\n",
    "    print(f\"Loaded results with {len(df)} rows\")\n",
    "    print(f\"Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Define metrics to plot (try both naming conventions)\n",
    "    # sentence-transformers uses format: \"cosine-NDCG@10\"\n",
    "    # Alternative format: \"ndcg_at_10\"\n",
    "    metric_mappings = {\n",
    "        \"NDCG@10\": [\"cosine-NDCG@10\", \"ndcg_at_10\"],\n",
    "        \"MAP@100\": [\"cosine-MAP@100\", \"map_at_100\"],\n",
    "        \"Recall@10\": [\"cosine-Recall@10\", \"recall_at_10\"],\n",
    "        \"Precision@10\": [\"cosine-Precision@10\", \"precision_at_10\"],\n",
    "    }\n",
    "\n",
    "    # Find which metrics are available\n",
    "    available_metrics = {}\n",
    "    for display_name, possible_names in metric_mappings.items():\n",
    "        for col_name in possible_names:\n",
    "            if col_name in df.columns:\n",
    "                available_metrics[display_name] = col_name\n",
    "                break\n",
    "\n",
    "    if not available_metrics:\n",
    "        print(f\"Warning: None of the expected metrics found in CSV\")\n",
    "        print(f\"Available columns: {df.columns.tolist()}\")\n",
    "        print(f\"\\nTrying to plot all numeric columns...\")\n",
    "\n",
    "        # Fallback: plot all numeric columns except 'epoch' and 'steps'\n",
    "        numeric_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "        plot_cols = [c for c in numeric_cols if c not in [\"epoch\", \"steps\"]]\n",
    "\n",
    "        if not plot_cols:\n",
    "            print(\"No plottable columns found!\")\n",
    "            return\n",
    "\n",
    "        available_metrics = {col: col for col in plot_cols[:4]}  # Max 4 plots\n",
    "\n",
    "    print(f\"\\nPlotting metrics: {list(available_metrics.keys())}\")\n",
    "\n",
    "    # Create subplots\n",
    "    n_metrics = len(available_metrics)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (display_name, col_name) in enumerate(available_metrics.items()):\n",
    "        ax = axes[i]\n",
    "        ax.plot(\n",
    "            df[\"epoch\"],\n",
    "            df[col_name],\n",
    "            marker=\"o\",\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "            color=\"steelblue\",\n",
    "        )\n",
    "        ax.set_title(display_name, fontsize=12, fontweight=\"bold\")\n",
    "        ax.set_xlabel(\"Epoch\", fontsize=10)\n",
    "        ax.set_ylabel(\"Score\", fontsize=10)\n",
    "        ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "        # Add value annotations on points (only if not too many points)\n",
    "        if len(df) <= 20:\n",
    "            for x, y in zip(df[\"epoch\"], df[col_name]):\n",
    "                ax.annotate(\n",
    "                    f\"{y:.3f}\",\n",
    "                    (x, y),\n",
    "                    textcoords=\"offset points\",\n",
    "                    xytext=(0, 5),\n",
    "                    ha=\"center\",\n",
    "                    fontsize=7,\n",
    "                    alpha=0.7,\n",
    "                )\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for i in range(n_metrics, 4):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.suptitle(\"Training Evaluation Results\", fontsize=14, fontweight=\"bold\", y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Evaluation Results Summary\")\n",
    "    print(\"=\" * 70)\n",
    "    for display_name, col_name in available_metrics.items():\n",
    "        best_idx = df[col_name].idxmax()\n",
    "        best_epoch = df.loc[best_idx, \"epoch\"]\n",
    "        best_score = df[col_name].max()\n",
    "        final_score = df[col_name].iloc[-1]\n",
    "        print(f\"{display_name}:\")\n",
    "        print(f\"  Best:  {best_score:.4f} (epoch {int(best_epoch)})\")\n",
    "        print(f\"  Final: {final_score:.4f}\")\n",
    "        print(\n",
    "            f\"  Improvement: {((final_score - df[col_name].iloc[0]) / df[col_name].iloc[0] * 100):.2f}%\"\n",
    "        )\n",
    "    print(\"=\" * 70)\n",
    "    for metric in available_metrics:\n",
    "        best_epoch = df.loc[df[metric].idxmax(), \"epoch\"]\n",
    "        best_score = df[metric].max()\n",
    "        final_score = df[metric].iloc[-1]\n",
    "        print(f\"{metric}:\")\n",
    "        print(f\"  Best: {best_score:.4f} (epoch {int(best_epoch)})\")\n",
    "        print(f\"  Final: {final_score:.4f}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cad6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results (if training logs exist)\n",
    "# Uncomment the lines below after training is complete\n",
    "results_path = f\"{OUTPUT_DIR}/eval/Information-Retrieval_evaluation_dev_results.csv\"\n",
    "plot_evaluation_results(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561db4e",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "This notebook implements a complete semantic retrieval system, including:\n",
    "\n",
    "1. **Data Loading and Preprocessing**: Load SciFact dataset from HuggingFace and convert to BEIR format\n",
    "2. **Model Training**: Fine-tune sentence-transformers model using MultipleNegativesRankingLoss\n",
    "3. **Evaluation**: Evaluate model performance on test set with multiple retrieval metrics\n",
    "4. **Inference**: Perform semantic retrieval using the trained model\n",
    "\n",
    "### Custom Configuration\n",
    "\n",
    "You can modify the following parameters to customize training:\n",
    "\n",
    "- `MODEL_NAME`: Base model (default: \"BAAI/bge-small-en-v1.5\")\n",
    "- `BATCH_SIZE`: Batch size (default: 32)\n",
    "- `EPOCHS`: Number of epochs (default: 6)\n",
    "- `LEARNING_RATE`: Learning rate (default: 3e-5)\n",
    "- `OUTPUT_DIR`: Model save path"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
