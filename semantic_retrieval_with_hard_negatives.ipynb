{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6d109dd6",
      "metadata": {
        "id": "6d109dd6"
      },
      "source": [
        "# Semantic Retrieval for Scientific Documents\n",
        "\n",
        "This notebook implements a deep learning-based semantic retrieval system, trained and evaluated on the SciFact dataset.\n",
        "\n",
        "## Project Overview\n",
        "- Fine-tune embedding models using sentence-transformers\n",
        "- Use MultipleNegativesRankingLoss loss function\n",
        "- Train and evaluate on BEIR-format datasets\n",
        "- Compare with traditional methods like BM25"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "803b3a73",
      "metadata": {
        "id": "803b3a73"
      },
      "source": [
        "## 1. Environment Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7071be04",
      "metadata": {
        "id": "7071be04",
        "outputId": "33f32721-3160-498d-e5d9-2bad27859543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Collecting beir\n",
            "  Downloading beir-2.2.0-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Collecting pytrec-eval-terrier (from beir)\n",
            "  Downloading pytrec_eval_terrier-0.5.10-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Downloading beir-2.2.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytrec_eval_terrier-0.5.10-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.8/304.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytrec-eval-terrier, beir\n",
            "Successfully installed beir-2.2.0 pytrec-eval-terrier-0.5.10\n"
          ]
        }
      ],
      "source": [
        "# If running in Colab, uncomment the line below to install dependencies\n",
        "!pip install sentence-transformers datasets pandas scikit-learn torch accelerate beir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "263573d9",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "263573d9"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import json\n",
        "import math\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from typing import Dict, Set, List, Tuple, Optional, Any, Union\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60a9d3c0",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "60a9d3c0"
      },
      "source": [
        "## 2. Data Loading and Preprocessing\n",
        "\n",
        "We use the BeIR/scifact-generated-queries dataset, a scientific literature retrieval dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "38811eda",
      "metadata": {
        "id": "38811eda"
      },
      "outputs": [],
      "source": [
        "def load_scifact_raw(output_dir: str = \"./data/raw\"):\n",
        "    \"\"\"\n",
        "    Load BeIR/scifact-generated-queries dataset from HuggingFace\n",
        "    \"\"\"\n",
        "    from datasets import load_dataset\n",
        "\n",
        "    raw_dir = Path(output_dir)\n",
        "    raw_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Load dataset\n",
        "    logger.info(\"Loading SciFact dataset from HuggingFace...\")\n",
        "    ds = load_dataset(\"BeIR/scifact-generated-queries\")\n",
        "    df = ds[\"train\"].to_pandas()  # type: ignore\n",
        "\n",
        "    # Save raw data\n",
        "    raw_path = raw_dir / \"scifact_raw.csv\"\n",
        "    df.to_csv(raw_path, index=False)  # type: ignore\n",
        "    logger.info(f\"Raw data saved to {raw_path}\")\n",
        "    logger.info(f\"Dataset size: {len(df)} rows\")  # type: ignore\n",
        "\n",
        "    return df  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "54b6766b",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "54b6766b"
      },
      "outputs": [],
      "source": [
        "def preprocess_scifact(raw_df: pd.DataFrame, output_dir: str = \"./data/processed\"):\n",
        "    \"\"\"\n",
        "    Preprocess SciFact dataset and generate BEIR-format training data\n",
        "\n",
        "    Output files:\n",
        "    - scifact_pairs.csv: Query-document pairs\n",
        "    - scifact_corpus.csv: Deduplicated document corpus\n",
        "    - beir_format/corpus.jsonl: BEIR-format corpus\n",
        "    - beir_format/queries.jsonl: BEIR-format queries\n",
        "    - beir_format/qrels/train.tsv: Training set relevance labels (70%)\n",
        "    - beir_format/qrels/dev.tsv: Validation set relevance labels (10%)\n",
        "    - beir_format/qrels/test.tsv: Test set relevance labels (20%)\n",
        "    \"\"\"\n",
        "    processed_dir = Path(output_dir)\n",
        "    processed_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Process data\n",
        "    df = raw_df[[\"_id\", \"title\", \"text\", \"query\"]].copy()\n",
        "    df = df.dropna(subset=[\"text\", \"query\"])  # type: ignore\n",
        "    df = df.rename(columns={\"_id\": \"doc_id\"})\n",
        "\n",
        "    # Data cleaning\n",
        "    df[\"title\"] = df[\"title\"].fillna(\"\").astype(str).str.strip()\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\").astype(str).str.strip()\n",
        "    df[\"query\"] = df[\"query\"].astype(str).str.strip()\n",
        "\n",
        "    # Combine title and text\n",
        "    df[\"content\"] = df[\"title\"] + \". \" + df[\"text\"]\n",
        "\n",
        "    # Assign ID to each unique query\n",
        "    df = df.reset_index(drop=True)\n",
        "    df[\"query_id\"] = pd.factorize(df[\"query\"])[0]\n",
        "\n",
        "    # Save query-document pairs\n",
        "    pairs_path = processed_dir / \"scifact_pairs.csv\"\n",
        "    df.to_csv(pairs_path, index=False)\n",
        "    logger.info(f\"Query-doc pairs saved to {pairs_path}\")\n",
        "\n",
        "    # Save deduplicated corpus\n",
        "    corpus = (\n",
        "        df[[\"doc_id\", \"content\"]]\n",
        "        .drop_duplicates(subset=[\"doc_id\"])  # type: ignore\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    corpus_path = processed_dir / \"scifact_corpus.csv\"\n",
        "    corpus.to_csv(corpus_path, index=False)\n",
        "    logger.info(f\"Corpus saved to {corpus_path} ({len(corpus)} documents)\")\n",
        "\n",
        "    # Create BEIR-format data\n",
        "    beir_dir = processed_dir / \"beir_format\"\n",
        "    beir_dir.mkdir(parents=True, exist_ok=True)\n",
        "    qrels_dir = beir_dir / \"qrels\"\n",
        "    qrels_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Write corpus.jsonl\n",
        "    corpus_jsonl_path = beir_dir / \"corpus.jsonl\"\n",
        "    with open(corpus_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in corpus.iterrows():\n",
        "            doc = {\n",
        "                \"_id\": str(row[\"doc_id\"]),\n",
        "                \"text\": row[\"content\"],\n",
        "                \"title\": \"\",\n",
        "            }\n",
        "            f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
        "    logger.info(f\"BEIR corpus saved to {corpus_jsonl_path}\")\n",
        "\n",
        "    # Write queries.jsonl\n",
        "    queries_jsonl_path = beir_dir / \"queries.jsonl\"\n",
        "    unique_queries = df[[\"query_id\", \"query\"]].drop_duplicates(subset=[\"query_id\"])  # type: ignore\n",
        "    with open(queries_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in unique_queries.iterrows():\n",
        "            query = {\"_id\": str(row[\"query_id\"]), \"text\": row[\"query\"]}\n",
        "            f.write(json.dumps(query, ensure_ascii=False) + \"\\n\")\n",
        "    logger.info(f\"BEIR queries saved to {queries_jsonl_path}\")\n",
        "\n",
        "    # Split into train/dev/test sets (70/10/20)\n",
        "    unique_query_ids = df[\"query_id\"].unique()\n",
        "    train_dev_query_ids, test_query_ids = train_test_split(\n",
        "        unique_query_ids, test_size=0.2, random_state=42\n",
        "    )\n",
        "    train_query_ids, dev_query_ids = train_test_split(\n",
        "        train_dev_query_ids, test_size=0.125, random_state=42\n",
        "    )\n",
        "\n",
        "    train_df = df[df[\"query_id\"].isin(train_query_ids)]\n",
        "    dev_df = df[df[\"query_id\"].isin(dev_query_ids)]\n",
        "    test_df = df[df[\"query_id\"].isin(test_query_ids)]\n",
        "\n",
        "    # Write qrels files\n",
        "    for split_name, split_df in [\n",
        "        (\"train\", train_df),\n",
        "        (\"dev\", dev_df),\n",
        "        (\"test\", test_df),\n",
        "    ]:\n",
        "        qrels_path = qrels_dir / f\"{split_name}.tsv\"\n",
        "        with open(qrels_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            for _, row in split_df.iterrows():\n",
        "                f.write(f\"{row['query_id']}\\t{row['doc_id']}\\t1\\n\")\n",
        "        logger.info(f\"{split_name} qrels saved to {qrels_path} ({len(split_df)} pairs)\")\n",
        "\n",
        "    logger.info(f\"\\nData preprocessing completed!\")\n",
        "    logger.info(f\"Train: {len(train_query_ids)} queries, {len(train_df)} pairs\")\n",
        "    logger.info(f\"Dev: {len(dev_query_ids)} queries, {len(dev_df)} pairs\")\n",
        "    logger.info(f\"Test: {len(test_query_ids)} queries, {len(test_df)} pairs\")\n",
        "\n",
        "    return beir_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5157a20e",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "5157a20e"
      },
      "source": [
        "## 3. Data Loading Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ffcf81fc",
      "metadata": {
        "id": "ffcf81fc"
      },
      "outputs": [],
      "source": [
        "def load_qrels(qrels_path: Path) -> Dict[str, Set[str]]:\n",
        "    \"\"\"Load qrels file and return query_id -> {doc_ids} mapping\"\"\"\n",
        "    query_to_docs = defaultdict(set)\n",
        "\n",
        "    with open(qrels_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) >= 2:\n",
        "                query_id = parts[0]\n",
        "                doc_id = parts[1]\n",
        "                query_to_docs[query_id].add(doc_id)\n",
        "\n",
        "    return dict(query_to_docs)\n",
        "\n",
        "\n",
        "def load_queries(queries_path: Path) -> Dict[str, str]:\n",
        "    \"\"\"Load queries.jsonl file and return query_id -> query_text mapping\"\"\"\n",
        "    queries = {}\n",
        "\n",
        "    with open(queries_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            query = json.loads(line)\n",
        "            queries[query[\"_id\"]] = query[\"text\"]\n",
        "\n",
        "    return queries\n",
        "\n",
        "\n",
        "def load_corpus(corpus_path: Path) -> Dict[str, str]:\n",
        "    \"\"\"Load corpus.jsonl file and return doc_id -> doc_text mapping\"\"\"\n",
        "    corpus = {}\n",
        "\n",
        "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            doc = json.loads(line)\n",
        "            corpus[doc[\"_id\"]] = doc[\"text\"]\n",
        "\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70251626",
      "metadata": {
        "id": "70251626"
      },
      "source": [
        "## 4. Model Training\n",
        "\n",
        "Fine-tune embedding models using sentence-transformers library with MultipleNegativesRankingLoss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "953e02e3",
      "metadata": {
        "id": "953e02e3",
        "outputId": "b7f52be3-f53f-474f-fbc4-7199982075c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Training configuration\n",
        "MODEL_NAME = \"BAAI/bge-small-en-v1.5\"  # Base model\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "WARMUP_RATIO = 0.1\n",
        "LEARNING_RATE = 3e-5\n",
        "OUTPUT_DIR = \"/content/drive/semantic-retrieval/models/finetuned-mnrl\"\n",
        "\n",
        "IN_COLAB = \"google.colab\" in str(get_ipython())\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "    BASE_DIR = Path(\"/content/drive/MyDrive/semantic-retrieval\")\n",
        "    OUTPUT_DIR = str(BASE_DIR / \"models/finetuned-mnrl\")\n",
        "    DATA_DIR = str(BASE_DIR / \"data\")\n",
        "    RESULTS_DIR = str(BASE_DIR / \"results\")\n",
        "else:\n",
        "    BASE_DIR = Path(\".\")\n",
        "    OUTPUT_DIR = \"./models/finetuned-mnrl\"\n",
        "    DATA_DIR = \"./data\"\n",
        "    RESULTS_DIR = \"./results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "956d7cc4",
      "metadata": {
        "id": "956d7cc4"
      },
      "outputs": [],
      "source": [
        "def load_training_data(data_dir: Path) -> list:\n",
        "    \"\"\"\n",
        "    Load training data and convert to InputExample format\n",
        "\n",
        "    For MultipleNegativesRankingLoss, we only need (query, positive_doc) pairs.\n",
        "    Negative samples are drawn from other examples in the same batch.\n",
        "    \"\"\"\n",
        "    from sentence_transformers import InputExample\n",
        "\n",
        "    qrels_path = data_dir / \"qrels\" / \"train.tsv\"\n",
        "    queries_path = data_dir / \"queries.jsonl\"\n",
        "    corpus_path = data_dir / \"corpus.jsonl\"\n",
        "\n",
        "    logger.info(\"Loading training data...\")\n",
        "    query_to_docs = load_qrels(qrels_path)\n",
        "    queries = load_queries(queries_path)\n",
        "    corpus = load_corpus(corpus_path)\n",
        "\n",
        "    # Create training examples\n",
        "    examples = []\n",
        "    for query_id, doc_ids in query_to_docs.items():\n",
        "        if query_id not in queries:\n",
        "            continue\n",
        "\n",
        "        query_text = queries[query_id]\n",
        "\n",
        "        for doc_id in doc_ids:\n",
        "            if doc_id not in corpus:\n",
        "                continue\n",
        "\n",
        "            doc_text = corpus[doc_id]\n",
        "            examples.append(InputExample(texts=[query_text, doc_text]))\n",
        "\n",
        "    logger.info(f\"Created {len(examples)} training examples\")\n",
        "    return examples\n",
        "\n",
        "\n",
        "def create_evaluator(data_dir: Path, split: str = \"dev\"):\n",
        "    \"\"\"\n",
        "    Create InformationRetrievalEvaluator for validation during training\n",
        "    \"\"\"\n",
        "    from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "\n",
        "    qrels_path = data_dir / \"qrels\" / f\"{split}.tsv\"\n",
        "    queries_path = data_dir / \"queries.jsonl\"\n",
        "    corpus_path = data_dir / \"corpus.jsonl\"\n",
        "\n",
        "    logger.info(f\"Loading {split} data for evaluation...\")\n",
        "    query_to_docs = load_qrels(qrels_path)\n",
        "    queries = load_queries(queries_path)\n",
        "    corpus = load_corpus(corpus_path)\n",
        "\n",
        "    # Filter queries and documents\n",
        "    eval_queries = {qid: queries[qid] for qid in query_to_docs if qid in queries}\n",
        "    eval_corpus = corpus\n",
        "\n",
        "    # Convert qrels format\n",
        "    eval_qrels: Dict[str, Set[str]] = {\n",
        "        qid: set(doc_ids)\n",
        "        for qid, doc_ids in query_to_docs.items()\n",
        "        if qid in eval_queries\n",
        "    }\n",
        "\n",
        "    logger.info(f\"Evaluator: {len(eval_queries)} queries, {len(eval_corpus)} documents\")\n",
        "\n",
        "    return InformationRetrievalEvaluator(\n",
        "        queries=eval_queries,\n",
        "        corpus=eval_corpus,\n",
        "        relevant_docs=eval_qrels,\n",
        "        name=split,\n",
        "        ndcg_at_k=[10, 100],\n",
        "        precision_recall_at_k=[10, 100],\n",
        "        map_at_k=[100],\n",
        "        mrr_at_k=[10],\n",
        "        show_progress_bar=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9529bf41",
      "metadata": {
        "id": "9529bf41"
      },
      "outputs": [],
      "source": [
        "def train_model(data_dir: Path, output_dir: str = OUTPUT_DIR):\n",
        "    \"\"\"\n",
        "    Train the embedding model\n",
        "    \"\"\"\n",
        "    from sentence_transformers import SentenceTransformer, losses\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Load model\n",
        "    logger.info(f\"Loading model: {MODEL_NAME}\")\n",
        "    model = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "    # Load training data\n",
        "    train_examples = load_training_data(data_dir)\n",
        "    train_dataloader = DataLoader(\n",
        "        train_examples,  # type: ignore\n",
        "        shuffle=True,\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "\n",
        "    # Setup loss function\n",
        "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
        "\n",
        "    # Create validation evaluator\n",
        "    dev_evaluator = create_evaluator(data_dir, split=\"dev\")\n",
        "\n",
        "    # Calculate training steps\n",
        "    total_steps = len(train_dataloader) * EPOCHS\n",
        "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
        "\n",
        "    logger.info(f\"\\nTraining configuration:\")\n",
        "    logger.info(f\"  Model: {MODEL_NAME}\")\n",
        "    logger.info(f\"  Batch size: {BATCH_SIZE}\")\n",
        "    logger.info(f\"  Epochs: {EPOCHS}\")\n",
        "    logger.info(f\"  Total steps: {total_steps}\")\n",
        "    logger.info(f\"  Warmup steps: {warmup_steps}\")\n",
        "    logger.info(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "    logger.info(f\"  Output directory: {output_path}\")\n",
        "\n",
        "    # Start training\n",
        "    logger.info(\"\\nStarting training...\")\n",
        "    model.fit(\n",
        "        train_objectives=[(train_dataloader, train_loss)],\n",
        "        evaluator=dev_evaluator,\n",
        "        epochs=EPOCHS,\n",
        "        warmup_steps=warmup_steps,\n",
        "        optimizer_params={\"lr\": LEARNING_RATE},\n",
        "        output_path=str(output_path),\n",
        "        evaluation_steps=len(train_dataloader) // 2,  # Evaluate twice per epoch\n",
        "        save_best_model=True,\n",
        "        show_progress_bar=True,\n",
        "    )\n",
        "\n",
        "    logger.info(f\"\\nTraining completed! Model saved to: {output_path}\")\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    logger.info(\"\\nRunning final evaluation on test set...\")\n",
        "    test_evaluator = create_evaluator(data_dir, split=\"test\")\n",
        "    test_results = test_evaluator(model, output_path=str(output_path))\n",
        "\n",
        "    logger.info(\"\\n\" + \"=\" * 50)\n",
        "    logger.info(\"Final Test Results:\")\n",
        "    for metric, value in test_results.items():\n",
        "        logger.info(f\"  {metric}: {value:.4f}\")\n",
        "    logger.info(\"=\" * 50)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abc11328",
      "metadata": {
        "id": "abc11328"
      },
      "source": [
        "## 5. Evaluation Metrics\n",
        "\n",
        "Implement standard information retrieval evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "84e9eace",
      "metadata": {
        "id": "84e9eace"
      },
      "outputs": [],
      "source": [
        "# Type alias for ground truth\n",
        "GroundTruth = Union[str, List[str], Set[str]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "72174abf",
      "metadata": {
        "id": "72174abf"
      },
      "outputs": [],
      "source": [
        "def traditional_eval(samples: List[Dict[str, Any]], k: int = 10) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluation for retrieval with single or multi-ground-truth.\n",
        "\n",
        "    Args:\n",
        "        samples: List of samples in the format:\n",
        "            [{\n",
        "                \"question\": str,\n",
        "                \"contexts\": List[str],  # ranked top-K doc_ids\n",
        "                \"ground_truth\": str | list | set  # gold doc_id(s)\n",
        "            }]\n",
        "        k: Cutoff position for evaluation\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing various metrics\n",
        "    \"\"\"\n",
        "\n",
        "    def to_set(gt: GroundTruth) -> Set[str]:\n",
        "        if isinstance(gt, set):\n",
        "            return set(str(x) for x in gt)\n",
        "        if isinstance(gt, list):\n",
        "            return set(str(x) for x in gt)\n",
        "        # single string / single id\n",
        "        return {str(gt)}\n",
        "\n",
        "    def hit_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
        "        return 1.0 if any(r in gt_set for r in results[:k]) else 0.0\n",
        "\n",
        "    def precision_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
        "        if k <= 0:\n",
        "            return 0.0\n",
        "        hits = sum(1 for r in results[:k] if r in gt_set)\n",
        "        return hits / k\n",
        "\n",
        "    def recall_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
        "        if not gt_set:\n",
        "            return 0.0\n",
        "        hits = sum(1 for r in results[:k] if r in gt_set)\n",
        "        return hits / len(gt_set)\n",
        "\n",
        "    def mrr(gt_set: Set[str], results: List[str]) -> float:\n",
        "        for rank, r in enumerate(results, start=1):\n",
        "            if r in gt_set:\n",
        "                return 1.0 / rank\n",
        "        return 0.0\n",
        "\n",
        "    def average_precision_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
        "        \"\"\"\n",
        "        AP@K for multi-relevant:\n",
        "          AP = (1 / min(|GT|, K)) * sum_{i=1..K} Precision@i * rel_i\n",
        "        \"\"\"\n",
        "        if not gt_set:\n",
        "            return 0.0\n",
        "\n",
        "        hits = 0\n",
        "        s = 0.0\n",
        "        for i, r in enumerate(results[:k], start=1):\n",
        "            if r in gt_set:\n",
        "                hits += 1\n",
        "                s += hits / i\n",
        "        return s / min(len(gt_set), k)\n",
        "\n",
        "    def ndcg_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
        "        # binary relevance\n",
        "        dcg = 0.0\n",
        "        for i, r in enumerate(results[:k], start=1):\n",
        "            rel = 1.0 if r in gt_set else 0.0\n",
        "            dcg += rel / math.log2(i + 1)\n",
        "\n",
        "        # ideal DCG: all relevant docs first\n",
        "        ideal_rels = [1.0] * min(len(gt_set), k)\n",
        "        idcg = 0.0\n",
        "        for i, rel in enumerate(ideal_rels, start=1):\n",
        "            idcg += rel / math.log2(i + 1)\n",
        "\n",
        "        return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "    hits, precisions, recalls, mrrs, aps, ndcgs = [], [], [], [], [], []\n",
        "\n",
        "    for s in samples:\n",
        "        gt_set = to_set(s[\"ground_truth\"])\n",
        "        results = [str(x) for x in s[\"contexts\"]]\n",
        "\n",
        "        hits.append(hit_at_k(gt_set, results, k))\n",
        "        precisions.append(precision_at_k(gt_set, results, k))\n",
        "        recalls.append(recall_at_k(gt_set, results, k))\n",
        "        mrrs.append(mrr(gt_set, results))\n",
        "        aps.append(average_precision_at_k(gt_set, results, k))\n",
        "        ndcgs.append(ndcg_at_k(gt_set, results, k))\n",
        "\n",
        "    n = len(samples) if samples else 1\n",
        "    return {\n",
        "        f\"Hit@{k}\": sum(hits) / n,\n",
        "        f\"Precision@{k}\": sum(precisions) / n,\n",
        "        f\"Recall@{k}\": sum(recalls) / n,\n",
        "        \"MRR\": sum(mrrs) / n,\n",
        "        f\"MAP@{k}\": sum(aps) / n,\n",
        "        f\"NDCG@{k}\": sum(ndcgs) / n,\n",
        "        \"N\": float(len(samples)),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "408bb88c",
      "metadata": {
        "id": "408bb88c"
      },
      "outputs": [],
      "source": [
        "def load_baseline_results(results_dir) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Load baseline model results from JSON files\n",
        "\n",
        "    Args:\n",
        "        results_dir: Directory containing baseline result JSON files\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping model name to results\n",
        "    \"\"\"\n",
        "    results_path = Path(results_dir)\n",
        "    baseline_results = {}\n",
        "\n",
        "    if not results_path.exists():\n",
        "        logger.warning(f\"Results directory not found: {results_path}\")\n",
        "        return baseline_results\n",
        "\n",
        "    # Load all JSON files in results directory\n",
        "    for json_file in results_path.glob(\"*_results.json\"):\n",
        "        try:\n",
        "            with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "                model_name = data.get(\"model_name\", json_file.stem)\n",
        "                baseline_results[model_name] = data\n",
        "                logger.info(f\"Loaded baseline results: {model_name}\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to load {json_file.name}: {e}\")\n",
        "\n",
        "    return baseline_results\n",
        "\n",
        "\n",
        "def display_baseline_comparison(baseline_results: Dict[str, Dict[str, Any]]):\n",
        "    \"\"\"\n",
        "    Display comparison of baseline model results\n",
        "\n",
        "    Args:\n",
        "        baseline_results: Dictionary of baseline results from load_baseline_results\n",
        "    \"\"\"\n",
        "    if not baseline_results:\n",
        "        print(\n",
        "            \"No baseline results found. Please run baseline evaluation scripts first:\"\n",
        "        )\n",
        "        print(\"  - uv run src/embedding_model/zero_shot.py\")\n",
        "        print(\"  - uv run src/run_word2vec_eval.py\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"BASELINE MODEL COMPARISON\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Collect all unique metrics\n",
        "    all_metrics = set()\n",
        "    for result in baseline_results.values():\n",
        "        all_metrics.update(result.get(\"metrics\", {}).keys())\n",
        "\n",
        "    # Remove 'N' from comparison metrics\n",
        "    all_metrics.discard(\"N\")\n",
        "    metric_list = sorted(all_metrics)\n",
        "\n",
        "    # Display results table\n",
        "    print(f\"\\n{'Model':<30}\", end=\"\")\n",
        "    for metric in metric_list:\n",
        "        print(f\"{metric:>12}\", end=\"\")\n",
        "    print()\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for model_name, result in baseline_results.items():\n",
        "        metrics = result.get(\"metrics\", {})\n",
        "        print(f\"{model_name:<30}\", end=\"\")\n",
        "        for metric in metric_list:\n",
        "            value = metrics.get(metric, 0.0)\n",
        "            if isinstance(value, (int, float)):\n",
        "                print(f\"{value:>12.4f}\", end=\"\")\n",
        "            else:\n",
        "                print(f\"{str(value):>12}\", end=\"\")\n",
        "        print()\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Display model details\n",
        "    print(\"\\nModel Details:\")\n",
        "    print(\"-\" * 80)\n",
        "    for model_name, result in baseline_results.items():\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  Type: {result.get('model_type', 'N/A')}\")\n",
        "        if \"base_model\" in result:\n",
        "            print(f\"  Base Model: {result['base_model']}\")\n",
        "        if \"parameters\" in result:\n",
        "            print(f\"  Parameters: {result['parameters']}\")\n",
        "        if \"N\" in result.get(\"metrics\", {}):\n",
        "            print(f\"  Test Samples: {int(result['metrics']['N'])}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3adc359c",
      "metadata": {
        "id": "3adc359c"
      },
      "source": [
        "## 6. Complete Training and Evaluation Pipeline\n",
        "\n",
        "Run the code below to execute the complete workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "46e68d4f",
      "metadata": {
        "id": "46e68d4f",
        "outputId": "b2ddb6fd-4567-4e3d-8f04-21a84969e689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575,
          "referenced_widgets": [
            "4a354efa0d444ba59925b1a6e8df20cd",
            "bc607c6d5b5d4e55a4d72a6b64288d02",
            "6ad8066348a54d1ba6e57cf093d69e7f",
            "a3ec46ea51a24a128ccd8125ef714ab1",
            "4ae3a4a6ce99404498005695f00e3504",
            "7b4506da2b2f4eb084c72f3f6c1cd30d",
            "c03e3b9da543442c9c022c9b480b8037",
            "ad5c72d46e2a434ab75937139dcb1f34",
            "96a15e07e5ee44a0babf599e803aa429",
            "b82b4ab0d3d4423cbaf541dd3a59be1d",
            "6e89f3f652f44b309188ddd52b45485a",
            "7a1ce9dae057497bba86148bff76d490",
            "a8a5b2851e534663b9a1bec696a80762",
            "cff35b502e6344e5a9c1dfdcb6d6dc49",
            "9fe08c7437374f87bc1f1907fe200fe4",
            "491798cd6f924be6933fdfee6a2f2b6b",
            "e4add5223fd9405f8ee1f72aeadbc5b7",
            "254934befb7c4ad58da9e2bc4d660cfd",
            "3b7adc3323f84a5988453c9bbb055057",
            "2e2d9b9105854699a81ddfdab3dbbd9b",
            "462be654cc7c4635b33c07853705a842",
            "8f820c23ced24d3abc317cf7bbe25dd6",
            "a3bda4dc84164db28c5c3755a9f07b86",
            "6305bb1cb8224994bca18ccdf65fdfb0",
            "9d9543ba25054ff79ee2ee772b72b59a",
            "df5be92b44894b428700aef9fd217bed",
            "e02d3fedd82f4fb4bdfed1b324f4285a",
            "c37916a87e874a17aee555037d8077f8",
            "0b2d2dcabc834fb39641e28420b28c3d",
            "ee7b5e0a252c4283bbdd97e0224997b4",
            "abaac28452d342808ea96511e31b4a4c",
            "316188e232774a0a93e173249a84fe7b",
            "097b8ba05bc24ee19322fd7a098e6184"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a354efa0d444ba59925b1a6e8df20cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train.jsonl.gz:   0%|          | 0.00/3.43M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a1ce9dae057497bba86148bff76d490"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/15422 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3bda4dc84164db28c5c3755a9f07b86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    _id                                              title  \\\n",
            "0  4983  Microstructural development of human newborn c...   \n",
            "1  4983  Microstructural development of human newborn c...   \n",
            "2  4983  Microstructural development of human newborn c...   \n",
            "3  5836  Induction of myelodysplasia by myeloid-derived...   \n",
            "4  5836  Induction of myelodysplasia by myeloid-derived...   \n",
            "\n",
            "                                                text  \\\n",
            "0  Alterations of the architecture of cerebral wh...   \n",
            "1  Alterations of the architecture of cerebral wh...   \n",
            "2  Alterations of the architecture of cerebral wh...   \n",
            "3  Myelodysplastic syndromes (MDS) are age-depend...   \n",
            "4  Myelodysplastic syndromes (MDS) are age-depend...   \n",
            "\n",
            "                                               query  \n",
            "0  what is the diffusion coefficient of cerebral ...  \n",
            "1                           what is diffusion tensor  \n",
            "2  what is the diffusion coefficient of the cereb...  \n",
            "3  which type of hematopoiesis is characterized b...  \n",
            "4                which cell types have hematopoiesis  \n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load raw data\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"Step 1: Loading raw data\")\n",
        "logger.info(\"=\" * 60)\n",
        "raw_df = load_scifact_raw()\n",
        "\n",
        "# Display sample data\n",
        "logger.info(\"\\nSample data:\")\n",
        "print(raw_df.head())  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8a615c5",
      "metadata": {
        "id": "a8a615c5"
      },
      "outputs": [],
      "source": [
        "# Step 2: Preprocess data\n",
        "logger.info(\"\\n\" + \"=\" * 60)\n",
        "logger.info(\"Step 2: Preprocessing data\")\n",
        "logger.info(\"=\" * 60)\n",
        "beir_dir = preprocess_scifact(raw_df)  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c0e25ac",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "6c0e25ac"
      },
      "outputs": [],
      "source": [
        "# Step 3: Train model\n",
        "logger.info(\"\\n\" + \"=\" * 60)\n",
        "logger.info(\"Step 3: Training model\")\n",
        "logger.info(\"=\" * 60)\n",
        "\n",
        "# Comment out the line below if you want to skip training\n",
        "model = train_model(beir_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b7e0840",
      "metadata": {
        "id": "8b7e0840"
      },
      "outputs": [],
      "source": [
        "# Step 4: Load pre-trained model (if already trained)\n",
        "# Comment out this section if you already trained above\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# model = SentenceTransformer(OUTPUT_DIR)\n",
        "# logger.info(f\"Model loaded from {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef5f5d09",
      "metadata": {
        "id": "ef5f5d09"
      },
      "source": [
        "## 6.5. Fine-tuning with Hard Negatives (Enhanced Training)\n",
        "\n",
        "In this section, we enhance the training process by using **hard negatives**.\n",
        "Hard negatives are documents that are semantically similar to the query but not relevant,\n",
        "making them more challenging examples for the model to learn from.\n",
        "\n",
        "### Process:\n",
        "1. **Mine hard negatives**: Use the base model to retrieve top-k similar but irrelevant documents\n",
        "2. **Create training data**: Include explicit hard negatives in training examples\n",
        "3. **Train with CachedMultipleNegativesRankingLoss**: More efficient for larger batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6178802e",
      "metadata": {
        "id": "6178802e"
      },
      "outputs": [],
      "source": [
        "# Hard negatives training configuration\n",
        "MODEL_NAME_HARDNEG = \"BAAI/bge-small-en-v1.5\"\n",
        "BATCH_SIZE_HARDNEG = 64\n",
        "MINI_BATCH_SIZE_HARDNEG = 16  # For CachedMultipleNegativesRankingLoss\n",
        "EPOCHS_HARDNEG = 4\n",
        "WARMUP_RATIO_HARDNEG = 0.1\n",
        "LEARNING_RATE_HARDNEG = 3e-5\n",
        "NEGATIVES_PER_QUERY = 2\n",
        "\n",
        "if IN_COLAB:\n",
        "    OUTPUT_DIR_HARDNEG = str(BASE_DIR / \"models/finetuned-mnrl-hardneg\")\n",
        "    HARD_NEGATIVES_PATH = str(\n",
        "        BASE_DIR / \"data/processed/beir_format/hard_negatives_train.jsonl\"\n",
        "    )\n",
        "else:\n",
        "    OUTPUT_DIR_HARDNEG = \"./models/finetuned-mnrl-hardneg\"\n",
        "    HARD_NEGATIVES_PATH = \"./data/processed/beir_format/hard_negatives_train.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea7a3ad5",
      "metadata": {
        "id": "ea7a3ad5"
      },
      "outputs": [],
      "source": [
        "# InputExampleDataset for hard negatives training\n",
        "class InputExampleDataset:\n",
        "    \"\"\"A thin Dataset wrapper around a list of InputExample objects.\"\"\"\n",
        "\n",
        "    def __init__(self, examples: list):\n",
        "        self.examples = list(examples)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return self.examples[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07495185",
      "metadata": {
        "id": "07495185"
      },
      "outputs": [],
      "source": [
        "def mine_hard_negatives(\n",
        "    model,\n",
        "    data_dir: Path,\n",
        "    output_path: Path,\n",
        "    split: str = \"train\",\n",
        "    negatives_per_query: int = 2,\n",
        "    top_k: int = 50,\n",
        "    batch_size: int = 64,\n",
        "):\n",
        "    \"\"\"\n",
        "    Mine hard negatives using semantic search.\n",
        "\n",
        "    Args:\n",
        "        model: SentenceTransformer model to use for encoding\n",
        "        data_dir: Path to BEIR format data directory\n",
        "        output_path: Path to save hard negatives JSONL file\n",
        "        split: Which split to mine from (\"train\", \"dev\", or \"test\")\n",
        "        negatives_per_query: Number of hard negatives to keep per query\n",
        "        top_k: Top-k retrieved documents (before filtering positives)\n",
        "        batch_size: Batch size for encoding\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from sentence_transformers import util\n",
        "\n",
        "    # Load data\n",
        "    qrels_path = data_dir / \"qrels\" / f\"{split}.tsv\"\n",
        "    queries_path = data_dir / \"queries.jsonl\"\n",
        "    corpus_path = data_dir / \"corpus.jsonl\"\n",
        "\n",
        "    logger.info(f\"Mining hard negatives for {split} split...\")\n",
        "    query_to_docs = load_qrels(qrels_path)\n",
        "    queries = load_queries(queries_path)\n",
        "    corpus = load_corpus(corpus_path)\n",
        "\n",
        "    # Encode corpus\n",
        "    logger.info(\"Encoding corpus for hard negative mining...\")\n",
        "    doc_ids = list(corpus.keys())\n",
        "    corpus_texts = [corpus[doc_id] for doc_id in doc_ids]\n",
        "    corpus_embeddings = model.encode(\n",
        "        corpus_texts,\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_tensor=True,\n",
        "    )\n",
        "\n",
        "    # Encode queries\n",
        "    logger.info(\"Encoding queries for hard negative mining...\")\n",
        "    query_ids = [qid for qid in query_to_docs.keys() if qid in queries]\n",
        "    query_texts = [queries[qid] for qid in query_ids]\n",
        "    query_embeddings = model.encode(\n",
        "        query_texts,\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_tensor=True,\n",
        "    )\n",
        "\n",
        "    # Semantic search\n",
        "    logger.info(f\"Running semantic search with top_k={top_k}...\")\n",
        "    search_results = util.semantic_search(\n",
        "        query_embeddings,\n",
        "        corpus_embeddings,\n",
        "        top_k=max(top_k, negatives_per_query * 5),\n",
        "        score_function=util.cos_sim,\n",
        "    )\n",
        "\n",
        "    # Save hard negatives\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    total_queries = 0\n",
        "    queries_with_negatives = 0\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for idx, hits in enumerate(search_results):\n",
        "            query_id = query_ids[idx]\n",
        "            positives = query_to_docs.get(query_id, set())\n",
        "\n",
        "            # Filter out positive documents\n",
        "            hard_neg_ids = []\n",
        "            for hit in hits:\n",
        "                corpus_idx = hit[\"corpus_id\"]\n",
        "                doc_id = doc_ids[corpus_idx]\n",
        "\n",
        "                if doc_id in positives or doc_id in hard_neg_ids:\n",
        "                    continue\n",
        "\n",
        "                hard_neg_ids.append(doc_id)\n",
        "                if len(hard_neg_ids) >= negatives_per_query:\n",
        "                    break\n",
        "\n",
        "            total_queries += 1\n",
        "            if len(hard_neg_ids) >= negatives_per_query:\n",
        "                queries_with_negatives += 1\n",
        "\n",
        "            record = {\n",
        "                \"query_id\": query_id,\n",
        "                \"positive_doc_ids\": sorted(list(positives)),\n",
        "                \"hard_negative_doc_ids\": hard_neg_ids,\n",
        "                \"query_text\": queries[query_id],\n",
        "            }\n",
        "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    logger.info(\n",
        "        f\"Finished mining: {queries_with_negatives}/{total_queries} queries \"\n",
        "        f\"have >= {negatives_per_query} hard negatives\"\n",
        "    )\n",
        "    logger.info(f\"Saved results to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bee806a",
      "metadata": {
        "id": "3bee806a"
      },
      "outputs": [],
      "source": [
        "def load_hard_negatives(path: Path) -> dict:\n",
        "    \"\"\"Load mined hard negatives from JSONL file\"\"\"\n",
        "    hard_negatives = {}\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            record = json.loads(line)\n",
        "            hard_negatives[record[\"query_id\"]] = record.get(\"hard_negative_doc_ids\", [])\n",
        "    return hard_negatives\n",
        "\n",
        "\n",
        "def load_training_data_with_hard_negatives(\n",
        "    data_dir: Path,\n",
        "    hard_negatives_path: Path,\n",
        "    negatives_per_query: int = 2,\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Load training data with hard negatives.\n",
        "\n",
        "    For CachedMultipleNegativesRankingLoss, each example contains:\n",
        "    - Query text\n",
        "    - Positive document text\n",
        "    - Multiple hard negative document texts\n",
        "    \"\"\"\n",
        "    from sentence_transformers import InputExample\n",
        "\n",
        "    qrels_path = data_dir / \"qrels\" / \"train.tsv\"\n",
        "    queries_path = data_dir / \"queries.jsonl\"\n",
        "    corpus_path = data_dir / \"corpus.jsonl\"\n",
        "\n",
        "    logger.info(\"Loading training data with hard negatives...\")\n",
        "    query_to_docs = load_qrels(qrels_path)\n",
        "    queries = load_queries(queries_path)\n",
        "    corpus = load_corpus(corpus_path)\n",
        "    hard_negatives = load_hard_negatives(hard_negatives_path)\n",
        "\n",
        "    examples = []\n",
        "    skipped = 0\n",
        "\n",
        "    for query_id, doc_ids in query_to_docs.items():\n",
        "        if query_id not in queries:\n",
        "            continue\n",
        "\n",
        "        query_text = queries[query_id]\n",
        "        hard_negs = hard_negatives.get(query_id, [])\n",
        "\n",
        "        # Skip if not enough hard negatives\n",
        "        if len(hard_negs) < negatives_per_query:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        for doc_id in doc_ids:\n",
        "            if doc_id not in corpus:\n",
        "                continue\n",
        "\n",
        "            doc_text = corpus[doc_id]\n",
        "\n",
        "            # Get hard negative texts\n",
        "            neg_texts = [corpus[neg_id] for neg_id in hard_negs if neg_id in corpus][\n",
        "                :negatives_per_query\n",
        "            ]\n",
        "\n",
        "            if len(neg_texts) < negatives_per_query:\n",
        "                continue\n",
        "\n",
        "            # Create example with query, positive, and hard negatives\n",
        "            texts = [query_text, doc_text, *neg_texts]\n",
        "            examples.append(InputExample(texts=texts))\n",
        "\n",
        "    logger.info(f\"Created {len(examples)} training examples with hard negatives\")\n",
        "    if skipped > 0:\n",
        "        logger.info(f\"Skipped {skipped} queries without enough hard negatives\")\n",
        "\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "371642d8",
      "metadata": {
        "id": "371642d8"
      },
      "outputs": [],
      "source": [
        "def train_model_with_hard_negatives(\n",
        "    data_dir: Path,\n",
        "    hard_negatives_path: Path,\n",
        "    output_dir: str = OUTPUT_DIR_HARDNEG,\n",
        "):\n",
        "    \"\"\"\n",
        "    Train embedding model with hard negatives using CachedMultipleNegativesRankingLoss.\n",
        "\n",
        "    This loss function is more efficient for training with explicit hard negatives\n",
        "    because it uses mini-batches within each batch for gradient accumulation.\n",
        "    \"\"\"\n",
        "    from sentence_transformers import SentenceTransformer, losses\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Load model\n",
        "    logger.info(f\"Loading model: {MODEL_NAME_HARDNEG}\")\n",
        "    model = SentenceTransformer(MODEL_NAME_HARDNEG)\n",
        "\n",
        "    # Load training data with hard negatives\n",
        "    train_examples = load_training_data_with_hard_negatives(\n",
        "        data_dir,\n",
        "        hard_negatives_path,\n",
        "        negatives_per_query=NEGATIVES_PER_QUERY,\n",
        "    )\n",
        "    train_dataset = InputExampleDataset(train_examples)\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        shuffle=True,\n",
        "        batch_size=BATCH_SIZE_HARDNEG,\n",
        "        drop_last=True,  # Important for CachedMultipleNegativesRankingLoss\n",
        "    )\n",
        "\n",
        "    # Setup CachedMultipleNegativesRankingLoss\n",
        "    # This loss is optimized for training with explicit hard negatives\n",
        "    train_loss = losses.CachedMultipleNegativesRankingLoss(\n",
        "        model,\n",
        "        mini_batch_size=MINI_BATCH_SIZE_HARDNEG,\n",
        "    )\n",
        "\n",
        "    # Create validation evaluator\n",
        "    dev_evaluator = create_evaluator(data_dir, split=\"dev\")\n",
        "\n",
        "    # Calculate training steps\n",
        "    total_steps = len(train_dataloader) * EPOCHS_HARDNEG\n",
        "    warmup_steps = int(total_steps * WARMUP_RATIO_HARDNEG)\n",
        "\n",
        "    logger.info(f\"\\nTraining configuration (with hard negatives):\")\n",
        "    logger.info(f\"  Model: {MODEL_NAME_HARDNEG}\")\n",
        "    logger.info(f\"  Batch size: {BATCH_SIZE_HARDNEG}\")\n",
        "    logger.info(f\"  Mini-batch size: {MINI_BATCH_SIZE_HARDNEG}\")\n",
        "    logger.info(f\"  Hard negatives per query: {NEGATIVES_PER_QUERY}\")\n",
        "    logger.info(f\"  Epochs: {EPOCHS_HARDNEG}\")\n",
        "    logger.info(f\"  Total steps: {total_steps}\")\n",
        "    logger.info(f\"  Warmup steps: {warmup_steps}\")\n",
        "    logger.info(f\"  Learning rate: {LEARNING_RATE_HARDNEG}\")\n",
        "    logger.info(f\"  Output directory: {output_path}\")\n",
        "\n",
        "    # Start training\n",
        "    logger.info(\"\\nStarting training with hard negatives...\")\n",
        "    model.fit(\n",
        "        train_objectives=[(train_dataloader, train_loss)],\n",
        "        evaluator=dev_evaluator,\n",
        "        epochs=EPOCHS_HARDNEG,\n",
        "        warmup_steps=warmup_steps,\n",
        "        optimizer_params={\"lr\": LEARNING_RATE_HARDNEG},\n",
        "        output_path=str(output_path),\n",
        "        evaluation_steps=max(1, len(train_dataloader) // 2),\n",
        "        save_best_model=True,\n",
        "        show_progress_bar=True,\n",
        "    )\n",
        "\n",
        "    logger.info(f\"\\nTraining completed! Model saved to: {output_path}\")\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    logger.info(\"\\nRunning final evaluation on test set...\")\n",
        "    test_evaluator = create_evaluator(data_dir, split=\"test\")\n",
        "    test_results = test_evaluator(model, output_path=str(output_path))\n",
        "\n",
        "    logger.info(\"\\n\" + \"=\" * 50)\n",
        "    logger.info(\"Final Test Results (with hard negatives):\")\n",
        "    for metric, value in test_results.items():\n",
        "        logger.info(f\"  {metric}: {value:.4f}\")\n",
        "    logger.info(\"=\" * 50)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0ce7f81",
      "metadata": {
        "id": "b0ce7f81"
      },
      "source": [
        "### Step 1: Mine Hard Negatives\n",
        "\n",
        "Use the base model (or previously fine-tuned model) to find hard negatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4622aa4",
      "metadata": {
        "id": "e4622aa4"
      },
      "outputs": [],
      "source": [
        "# Mine hard negatives using the base model\n",
        "hard_negatives_path = Path(HARD_NEGATIVES_PATH)\n",
        "\n",
        "# Check if hard negatives already exist\n",
        "if hard_negatives_path.exists():\n",
        "    logger.info(f\"Hard negatives file already exists: {hard_negatives_path}\")\n",
        "    logger.info(\"Skipping mining step. Delete the file to re-mine.\")\n",
        "else:\n",
        "    logger.info(\"=\" * 70)\n",
        "    logger.info(\"Mining hard negatives for training...\")\n",
        "    logger.info(\"=\" * 70)\n",
        "\n",
        "    # Load base model for mining\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "    base_model = SentenceTransformer(MODEL_NAME_HARDNEG)\n",
        "\n",
        "    # Mine hard negatives\n",
        "    mine_hard_negatives(\n",
        "        model=base_model,\n",
        "        data_dir=beir_dir,\n",
        "        output_path=hard_negatives_path,\n",
        "        split=\"train\",\n",
        "        negatives_per_query=NEGATIVES_PER_QUERY,\n",
        "        top_k=50,\n",
        "        batch_size=64,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99719a35",
      "metadata": {
        "id": "99719a35"
      },
      "source": [
        "### Step 2: Train with Hard Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d51662f",
      "metadata": {
        "id": "9d51662f"
      },
      "outputs": [],
      "source": [
        "# Train model with hard negatives\n",
        "logger.info(\"\\n\" + \"=\" * 70)\n",
        "logger.info(\"Step 6.5: Training model with hard negatives\")\n",
        "logger.info(\"=\" * 70)\n",
        "\n",
        "# Comment out the line below if you want to skip this training\n",
        "model_hardneg = train_model_with_hard_negatives(beir_dir, hard_negatives_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e4dd0f",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "c1e4dd0f"
      },
      "outputs": [],
      "source": [
        "# Step: Load pre-trained model with hard negatives (if already trained)\n",
        "# Uncomment this section if you already trained above\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# model_hardneg = SentenceTransformer(OUTPUT_DIR_HARDNEG)\n",
        "# logger.info(f\"Model loaded from {OUTPUT_DIR_HARDNEG}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fe700df",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "6fe700df"
      },
      "source": [
        "## 7. Semantic Retrieval with Trained Model\n",
        "\n",
        "Demonstration of how to use the trained model for semantic retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eda6f77e",
      "metadata": {
        "id": "eda6f77e"
      },
      "outputs": [],
      "source": [
        "def semantic_search_demo(model, data_dir: Path, num_queries: int = 5):\n",
        "    \"\"\"\n",
        "    Demonstrate semantic search functionality\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from sentence_transformers import util\n",
        "\n",
        "    # Load test data\n",
        "    queries_path = data_dir / \"queries.jsonl\"\n",
        "    corpus_path = data_dir / \"corpus.jsonl\"\n",
        "    qrels_path = data_dir / \"qrels\" / \"test.tsv\"\n",
        "\n",
        "    queries = load_queries(queries_path)\n",
        "    corpus = load_corpus(corpus_path)\n",
        "    query_to_docs = load_qrels(qrels_path)\n",
        "\n",
        "    # Encode corpus\n",
        "    logger.info(\"Encoding corpus...\")\n",
        "    corpus_ids = list(corpus.keys())\n",
        "    corpus_texts = [corpus[doc_id] for doc_id in corpus_ids]\n",
        "    corpus_embeddings = model.encode(\n",
        "        corpus_texts, convert_to_tensor=True, show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    # Select test queries\n",
        "    test_queries = list(query_to_docs.keys())[:num_queries]\n",
        "\n",
        "    logger.info(f\"\\nRunning semantic search on {num_queries} test queries...\")\n",
        "\n",
        "    for query_id in test_queries:\n",
        "        query_text = queries[query_id]\n",
        "        ground_truth = query_to_docs[query_id]\n",
        "\n",
        "        # Encode query\n",
        "        query_embedding = model.encode(query_text, convert_to_tensor=True)\n",
        "\n",
        "        # Compute similarity and retrieve\n",
        "        cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "        top_results = np.argsort(-cos_scores.cpu().numpy())[:10]\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(f\"Query: {query_text}\")\n",
        "        print(f\"Ground truth docs: {ground_truth}\")\n",
        "        print(\"\\nTop 10 retrieved documents:\")\n",
        "\n",
        "        for rank, idx in enumerate(top_results, 1):\n",
        "            doc_id = corpus_ids[idx]\n",
        "            score = cos_scores[idx].item()\n",
        "            is_relevant = \"✓\" if doc_id in ground_truth else \"✗\"\n",
        "            doc_preview = corpus[doc_id][:100] + \"...\"\n",
        "            print(f\"\\n{rank}. [{is_relevant}] Doc {doc_id} (score: {score:.4f})\")\n",
        "            print(f\"   {doc_preview}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb798960",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "fb798960"
      },
      "outputs": [],
      "source": [
        "# Run retrieval demo\n",
        "semantic_search_demo(model, beir_dir, num_queries=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42cba3ff",
      "metadata": {
        "id": "42cba3ff"
      },
      "source": [
        "## 7.3. Load and Display Baseline Results\n",
        "\n",
        "Before evaluating our fine-tuned model, let's load the baseline results for comparison.\n",
        "\n",
        "**Note**: Make sure to run the baseline evaluation scripts first:\n",
        "- `uv run src/embedding_model/zero_shot.py` - Zero-shot BGE baseline\n",
        "- `uv run src/run_word2vec_eval.py` - Word2Vec baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f797e80c",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "f797e80c"
      },
      "outputs": [],
      "source": [
        "# Load baseline results\n",
        "logger.info(\"Loading baseline model results...\")\n",
        "baseline_results = load_baseline_results(results_dir=RESULTS_DIR)\n",
        "\n",
        "# Display baseline comparison\n",
        "display_baseline_comparison(baseline_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8cf291a",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "b8cf291a"
      },
      "source": [
        "## 7.5. Model Evaluation with traditional_eval\n",
        "Use the `traditional_eval` function to comprehensively evaluate the model's retrieval performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f199c9",
      "metadata": {
        "id": "90f199c9"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_retrieval(model, data_dir: Path, split: str = \"test\", k: int = 10):\n",
        "    \"\"\"\n",
        "    Evaluate model using traditional_eval function\n",
        "\n",
        "    Args:\n",
        "        model: Trained SentenceTransformer model\n",
        "        data_dir: Path to BEIR format data directory\n",
        "        split: Which split to evaluate on (\"test\", \"dev\", or \"train\")\n",
        "        k: Cutoff position for evaluation metrics\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing evaluation metrics\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from sentence_transformers import util\n",
        "\n",
        "    # Load data\n",
        "    queries_path = data_dir / \"queries.jsonl\"\n",
        "    corpus_path = data_dir / \"corpus.jsonl\"\n",
        "    qrels_path = data_dir / \"qrels\" / f\"{split}.tsv\"\n",
        "\n",
        "    logger.info(f\"Loading {split} set for evaluation...\")\n",
        "    queries = load_queries(queries_path)\n",
        "    corpus = load_corpus(corpus_path)\n",
        "    query_to_docs = load_qrels(qrels_path)\n",
        "\n",
        "    # Encode corpus once\n",
        "    logger.info(\"Encoding corpus...\")\n",
        "    corpus_ids = list(corpus.keys())\n",
        "    corpus_texts = [corpus[doc_id] for doc_id in corpus_ids]\n",
        "    corpus_embeddings = model.encode(\n",
        "        corpus_texts, convert_to_tensor=True, show_progress_bar=True, batch_size=64\n",
        "    )\n",
        "\n",
        "    # Build evaluation samples\n",
        "    logger.info(f\"Running retrieval for {len(query_to_docs)} queries...\")\n",
        "    samples = []\n",
        "\n",
        "    for i, (query_id, gt_docs) in enumerate(query_to_docs.items(), 1):\n",
        "        # Get query text\n",
        "        query_text = queries.get(query_id)\n",
        "        if not query_text:\n",
        "            logger.warning(f\"Query {query_id} not found in queries.jsonl\")\n",
        "            continue\n",
        "\n",
        "        # Encode query\n",
        "        query_embedding = model.encode(query_text, convert_to_tensor=True)\n",
        "\n",
        "        # Retrieve top-k documents\n",
        "        cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "        top_indices = np.argsort(-cos_scores.cpu().numpy())[:k]\n",
        "        top_docs = [corpus_ids[idx] for idx in top_indices]\n",
        "\n",
        "        # Create sample for evaluation\n",
        "        samples.append(\n",
        "            {\"question\": query_text, \"contexts\": top_docs, \"ground_truth\": gt_docs}\n",
        "        )\n",
        "\n",
        "        # Progress update\n",
        "        if i % 100 == 0:\n",
        "            logger.info(f\"  Processed {i}/{len(query_to_docs)} queries\")\n",
        "\n",
        "    # Run evaluation\n",
        "    logger.info(f\"\\nEvaluating with traditional_eval (k={k})...\")\n",
        "    metrics = traditional_eval(samples, k=k)\n",
        "\n",
        "    return metrics, samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "255bbdae",
      "metadata": {
        "id": "255bbdae"
      },
      "outputs": [],
      "source": [
        "def display_evaluation_results(\n",
        "    metrics: Dict[str, float], title: str = \"Evaluation Results\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Display evaluation results in a formatted table\n",
        "\n",
        "    Args:\n",
        "        metrics: Dictionary of metric name -> value\n",
        "        title: Title for the results display\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"{title:^70}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Group metrics by type\n",
        "    metric_groups = {\n",
        "        \"Retrieval Accuracy\": [\"Hit@10\"],\n",
        "        \"Precision & Recall\": [\"Precision@10\", \"Recall@10\"],\n",
        "        \"Ranking Quality\": [\"MRR\", \"MAP@10\", \"NDCG@10\"],\n",
        "        \"Dataset Info\": [\"N\"],\n",
        "    }\n",
        "\n",
        "    for group_name, metric_names in metric_groups.items():\n",
        "        print(f\"\\n{group_name}:\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        for metric_name in metric_names:\n",
        "            # Try different possible keys\n",
        "            possible_keys = [metric_name, metric_name.replace(\"@\", \"_at_\")]\n",
        "\n",
        "            for key in possible_keys:\n",
        "                if key in metrics:\n",
        "                    value = metrics[key]\n",
        "                    if key == \"N\":\n",
        "                        print(f\"  {metric_name:.<40} {int(value)}\")\n",
        "                    else:\n",
        "                        print(f\"  {metric_name:.<40} {value:.4f}\")\n",
        "                    break\n",
        "\n",
        "    print(\"=\" * 70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "277aee25",
      "metadata": {
        "id": "277aee25"
      },
      "outputs": [],
      "source": [
        "def compare_with_baseline(\n",
        "    finetuned_metrics: Dict[str, float],\n",
        "    baseline_metrics: Optional[Dict[str, float]] = None,\n",
        "    baseline_results: Optional[Dict[str, Dict[str, Any]]] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Compare fine-tuned model with baseline\n",
        "\n",
        "    Args:\n",
        "        finetuned_metrics: Metrics from fine-tuned model\n",
        "        baseline_metrics: Metrics from baseline (optional, for backward compatibility)\n",
        "        baseline_results: All baseline results from load_baseline_results (optional)\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Try to use loaded baseline results first\n",
        "    actual_baseline_metrics = baseline_metrics\n",
        "    if baseline_results is not None and \"Zero-shot BGE-small\" in baseline_results:\n",
        "        actual_baseline_metrics = baseline_results[\"Zero-shot BGE-small\"][\"metrics\"]\n",
        "        logger.info(\"Using loaded Zero-shot BGE-small baseline metrics\")\n",
        "    elif actual_baseline_metrics is None:\n",
        "        # Default baseline metrics (zero-shot BAAI/bge-small-en-v1.5)\n",
        "        # These are example values - replace with actual baseline results\n",
        "        actual_baseline_metrics = {\n",
        "            \"Hit@10\": 0.75,\n",
        "            \"Precision@10\": 0.15,\n",
        "            \"Recall@10\": 0.70,\n",
        "            \"MRR\": 0.65,\n",
        "            \"MAP@10\": 0.55,\n",
        "            \"NDCG@10\": 0.68,\n",
        "        }\n",
        "        logger.info(\"Using default baseline metrics (approximate)\")\n",
        "\n",
        "    # Metrics to compare\n",
        "    metric_names = [\"Hit@10\", \"Precision@10\", \"Recall@10\", \"MRR\", \"MAP@10\", \"NDCG@10\"]\n",
        "\n",
        "    # Extract values\n",
        "    baseline_values = []\n",
        "    finetuned_values = []\n",
        "    display_names = []\n",
        "\n",
        "    for metric in metric_names:\n",
        "        # Try different key formats\n",
        "        possible_keys = [metric, metric.replace(\"@\", \"_at_\")]\n",
        "\n",
        "        for key in possible_keys:\n",
        "            if key in actual_baseline_metrics and key in finetuned_metrics:\n",
        "                baseline_values.append(actual_baseline_metrics[key])\n",
        "                finetuned_values.append(finetuned_metrics[key])\n",
        "                display_names.append(metric)\n",
        "                break\n",
        "\n",
        "    if not display_names:\n",
        "        logger.warning(\"No common metrics found for comparison\")\n",
        "        print(\n",
        "            \"\\nError: No matching metrics found between baseline and fine-tuned model.\"\n",
        "        )\n",
        "        print(\n",
        "            \"Available baseline metrics:\",\n",
        "            list(actual_baseline_metrics.keys()) if actual_baseline_metrics else \"None\",\n",
        "        )\n",
        "        print(\"Available fine-tuned metrics:\", list(finetuned_metrics.keys()))\n",
        "        return\n",
        "\n",
        "    # Create comparison plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Bar chart comparison\n",
        "    x = range(len(display_names))\n",
        "    width = 0.35\n",
        "\n",
        "    ax1.bar(\n",
        "        [i - width / 2 for i in x], baseline_values, width, label=\"Baseline\", alpha=0.8\n",
        "    )\n",
        "    ax1.bar(\n",
        "        [i + width / 2 for i in x],\n",
        "        finetuned_values,\n",
        "        width,\n",
        "        label=\"Fine-tuned\",\n",
        "        alpha=0.8,\n",
        "    )\n",
        "    ax1.set_xlabel(\"Metrics\")\n",
        "    ax1.set_ylabel(\"Score\")\n",
        "    ax1.set_title(\"Baseline vs Fine-tuned Model\")\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(display_names, rotation=45, ha=\"right\")\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Improvement percentage (handle division by zero)\n",
        "    improvements = []\n",
        "    for b, f in zip(baseline_values, finetuned_values):\n",
        "        if b == 0:\n",
        "            # If baseline is 0, use absolute difference instead\n",
        "            improvements.append(f * 100)\n",
        "        else:\n",
        "            improvements.append((f - b) / b * 100)\n",
        "\n",
        "    colors = [\"green\" if imp > 0 else \"red\" for imp in improvements]\n",
        "\n",
        "    ax2.bar(x, improvements, color=colors, alpha=0.7)\n",
        "    ax2.set_xlabel(\"Metrics\")\n",
        "    ax2.set_ylabel(\"Improvement (%)\")\n",
        "    ax2.set_title(\"Relative Improvement over Baseline\")\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels(display_names, rotation=45, ha=\"right\")\n",
        "    ax2.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print improvement summary\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Improvement Summary\")\n",
        "    print(\"=\" * 70)\n",
        "    for name, baseline, finetuned, improvement in zip(\n",
        "        display_names, baseline_values, finetuned_values, improvements\n",
        "    ):\n",
        "        if baseline == 0:\n",
        "            # Special display for zero baseline\n",
        "            print(f\"{name:.<30} {baseline:.4f} → {finetuned:.4f} (baseline was 0)\")\n",
        "        else:\n",
        "            arrow = \"↑\" if improvement > 0 else \"↓\"\n",
        "            print(\n",
        "                f\"{name:.<30} {baseline:.4f} → {finetuned:.4f} ({arrow} {abs(improvement):.2f}%)\"\n",
        "            )\n",
        "    print(\"=\" * 70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b26d626",
      "metadata": {
        "id": "0b26d626"
      },
      "outputs": [],
      "source": [
        "def compare_all_models(\n",
        "    finetuned_metrics: Dict[str, float],\n",
        "    baseline_results_dict: Dict[str, Dict[str, Any]],\n",
        "    finetuned_name: str = \"Fine-tuned Model\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Compare fine-tuned model with all baseline models\n",
        "\n",
        "    Args:\n",
        "        finetuned_metrics: Metrics from fine-tuned model\n",
        "        baseline_results_dict: All baseline results from load_baseline_results\n",
        "        finetuned_name: Name for the fine-tuned model\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    if not baseline_results_dict:\n",
        "        logger.warning(\"No baseline results available for comparison\")\n",
        "        return\n",
        "\n",
        "    # Prepare data for comparison\n",
        "    models_data = {}\n",
        "\n",
        "    # Add baselines\n",
        "    for model_name, result in baseline_results_dict.items():\n",
        "        models_data[model_name] = result.get(\"metrics\", {})\n",
        "\n",
        "    # Add fine-tuned model\n",
        "    models_data[finetuned_name] = finetuned_metrics\n",
        "\n",
        "    # Metrics to compare\n",
        "    metric_names = [\"Hit@10\", \"Precision@10\", \"Recall@10\", \"MRR\", \"MAP@10\", \"NDCG@10\"]\n",
        "\n",
        "    # Prepare plotting data\n",
        "    model_names = list(models_data.keys())\n",
        "    n_models = len(model_names)\n",
        "    n_metrics = len(metric_names)\n",
        "\n",
        "    # Create figure with subplots\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, n_models))  # type: ignore\n",
        "\n",
        "    for idx, metric in enumerate(metric_names):\n",
        "        ax = axes[idx]\n",
        "\n",
        "        values = []\n",
        "        labels = []\n",
        "\n",
        "        for model_name in model_names:\n",
        "            metrics = models_data[model_name]\n",
        "            # Try both formats\n",
        "            value = metrics.get(metric, metrics.get(metric.replace(\"@\", \"_at_\"), 0.0))\n",
        "            if isinstance(value, (int, float)):\n",
        "                values.append(value)\n",
        "                labels.append(model_name)\n",
        "\n",
        "        if values:\n",
        "            bars = ax.bar(\n",
        "                range(len(values)), values, color=colors[: len(values)], alpha=0.8\n",
        "            )\n",
        "            ax.set_title(metric, fontsize=12, fontweight=\"bold\")\n",
        "            ax.set_ylabel(\"Score\", fontsize=10)\n",
        "            ax.set_xticks(range(len(labels)))\n",
        "            ax.set_xticklabels(labels, rotation=45, ha=\"right\", fontsize=9)\n",
        "            ax.grid(True, alpha=0.3, axis=\"y\")\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for bar in bars:\n",
        "                height = bar.get_height()\n",
        "                ax.text(\n",
        "                    bar.get_x() + bar.get_width() / 2.0,\n",
        "                    height,\n",
        "                    f\"{height:.3f}\",\n",
        "                    ha=\"center\",\n",
        "                    va=\"bottom\",\n",
        "                    fontsize=8,\n",
        "                )\n",
        "\n",
        "    plt.suptitle(\n",
        "        \"Model Comparison: Fine-tuned vs Baselines\",\n",
        "        fontsize=14,\n",
        "        fontweight=\"bold\",\n",
        "        y=1.00,\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed comparison table\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(\"DETAILED MODEL COMPARISON\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "    # Header\n",
        "    print(f\"\\n{'Model':<25}\", end=\"\")\n",
        "    for metric in metric_names:\n",
        "        print(f\"{metric:>12}\", end=\"\")\n",
        "    print()\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    # Data rows\n",
        "    for model_name in model_names:\n",
        "        metrics = models_data[model_name]\n",
        "        print(f\"{model_name:<25}\", end=\"\")\n",
        "        for metric in metric_names:\n",
        "            value = metrics.get(metric, metrics.get(metric.replace(\"@\", \"_at_\"), 0.0))\n",
        "            if isinstance(value, (int, float)):\n",
        "                print(f\"{value:>12.4f}\", end=\"\")\n",
        "            else:\n",
        "                print(f\"{str(value):>12}\", end=\"\")\n",
        "        print()\n",
        "\n",
        "    print(\"=\" * 100 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8abb2683",
      "metadata": {
        "id": "8abb2683"
      },
      "source": [
        "### Run Complete Evaluation\n",
        "\n",
        "Evaluate the fine-tuned model on the test set using `traditional_eval`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "588e9e3d",
      "metadata": {
        "id": "588e9e3d"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "logger.info(\"=\" * 70)\n",
        "logger.info(\"Running complete evaluation on test set\")\n",
        "logger.info(\"=\" * 70)\n",
        "\n",
        "test_metrics, test_samples = evaluate_model_retrieval(\n",
        "    model=model, data_dir=beir_dir, split=\"test\", k=10\n",
        ")\n",
        "\n",
        "# Display results\n",
        "display_evaluation_results(test_metrics, title=\"Test Set Evaluation Results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5114cb7",
      "metadata": {
        "id": "f5114cb7"
      },
      "outputs": [],
      "source": [
        "# Optional: Evaluate on dev set as well\n",
        "logger.info(\"Running evaluation on dev set for comparison...\")\n",
        "\n",
        "dev_metrics, dev_samples = evaluate_model_retrieval(\n",
        "    model=model, data_dir=beir_dir, split=\"dev\", k=10\n",
        ")\n",
        "\n",
        "display_evaluation_results(dev_metrics, title=\"Dev Set Evaluation Results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f815965",
      "metadata": {
        "id": "0f815965"
      },
      "outputs": [],
      "source": [
        "# Compare with baseline (zero-shot only)\n",
        "compare_with_baseline(test_metrics, baseline_results=baseline_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3666de0b",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "3666de0b"
      },
      "outputs": [],
      "source": [
        "# Compare with all baseline models\n",
        "logger.info(\"Generating comprehensive model comparison...\")\n",
        "compare_all_models(\n",
        "    test_metrics, baseline_results, finetuned_name=\"Fine-tuned BGE-small\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8534e227",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "8534e227"
      },
      "source": [
        "### Analyze Individual Query Performance\n",
        "\n",
        "Examine specific queries to understand model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "399d7ca0",
      "metadata": {
        "id": "399d7ca0"
      },
      "outputs": [],
      "source": [
        "def analyze_query_performance(samples: List[Dict[str, Any]], num_examples: int = 5):\n",
        "    \"\"\"\n",
        "    Analyze performance on individual queries\n",
        "\n",
        "    Args:\n",
        "        samples: Evaluation samples from evaluate_model_retrieval\n",
        "        num_examples: Number of examples to display\n",
        "    \"\"\"\n",
        "    # Calculate per-query metrics\n",
        "    query_metrics = []\n",
        "\n",
        "    for sample in samples:\n",
        "        gt_set = set(str(x) for x in sample[\"ground_truth\"])\n",
        "        results = [str(x) for x in sample[\"contexts\"]]\n",
        "\n",
        "        # Hit@10\n",
        "        hit = 1.0 if any(r in gt_set for r in results[:10]) else 0.0\n",
        "\n",
        "        # Recall@10\n",
        "        hits_count = sum(1 for r in results[:10] if r in gt_set)\n",
        "        recall = hits_count / len(gt_set) if gt_set else 0.0\n",
        "\n",
        "        # MRR\n",
        "        mrr = 0.0\n",
        "        for rank, r in enumerate(results, start=1):\n",
        "            if r in gt_set:\n",
        "                mrr = 1.0 / rank\n",
        "                break\n",
        "\n",
        "        query_metrics.append(\n",
        "            {\n",
        "                \"query\": sample[\"question\"],\n",
        "                \"hit\": hit,\n",
        "                \"recall\": recall,\n",
        "                \"mrr\": mrr,\n",
        "                \"num_relevant\": len(gt_set),\n",
        "                \"retrieved\": results[:10],\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # Sort by MRR (worst first)\n",
        "    query_metrics.sort(key=lambda x: x[\"mrr\"])\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Worst {num_examples} Queries (by MRR)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    for i, qm in enumerate(query_metrics[:num_examples], 1):\n",
        "        print(f\"\\n{i}. Query: {qm['query'][:80]}...\")\n",
        "        print(\n",
        "            f\"   Hit@10: {qm['hit']:.0f} | Recall@10: {qm['recall']:.4f} | MRR: {qm['mrr']:.4f}\"\n",
        "        )\n",
        "        print(f\"   Relevant docs: {qm['num_relevant']}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Best {num_examples} Queries (by MRR)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    for i, qm in enumerate(query_metrics[-num_examples:][::-1], 1):\n",
        "        print(f\"\\n{i}. Query: {qm['query'][:80]}...\")\n",
        "        print(\n",
        "            f\"   Hit@10: {qm['hit']:.0f} | Recall@10: {qm['recall']:.4f} | MRR: {qm['mrr']:.4f}\"\n",
        "        )\n",
        "        print(f\"   Relevant docs: {qm['num_relevant']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02e33a33",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "02e33a33"
      },
      "outputs": [],
      "source": [
        "# Analyze query performance\n",
        "analyze_query_performance(test_samples, num_examples=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02611723"
      },
      "source": [
        "### Step 3: Evaluate Hard Negatives Model"
      ],
      "id": "02611723"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "6c8a14b8"
      },
      "outputs": [],
      "source": [
        "# Evaluate the hard negatives model on test set\n",
        "logger.info(\"=\" * 70)\n",
        "logger.info(\"Evaluating hard negatives model on test set\")\n",
        "logger.info(\"=\" * 70)\n",
        "\n",
        "test_metrics_hardneg, test_samples_hardneg = evaluate_model_retrieval(\n",
        "    model=model_hardneg, data_dir=beir_dir, split=\"test\", k=10\n",
        ")\n",
        "\n",
        "# Display results\n",
        "display_evaluation_results(\n",
        "    test_metrics_hardneg, title=\"Hard Negatives Model - Test Set Results\"\n",
        ")"
      ],
      "id": "6c8a14b8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7df71067"
      },
      "source": [
        "### Step 4: Compare All Models (Baseline + Standard Fine-tuning + Hard Negatives)"
      ],
      "id": "7df71067"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "c42a9f2b"
      },
      "outputs": [],
      "source": [
        "# Compare all three approaches\n",
        "logger.info(\n",
        "    \"Generating comprehensive comparison: Baseline vs Fine-tuned vs Hard Negatives...\"\n",
        ")\n",
        "\n",
        "# Prepare data for comparison\n",
        "all_models_results = baseline_results.copy()\n",
        "\n",
        "# Add standard fine-tuned model\n",
        "all_models_results[\"Fine-tuned (Standard MNRL)\"] = {\n",
        "    \"metrics\": test_metrics,\n",
        "    \"model_type\": \"Fine-tuned\",\n",
        "    \"base_model\": MODEL_NAME,\n",
        "}\n",
        "\n",
        "# Add hard negatives model\n",
        "all_models_results[\"Fine-tuned (Hard Negatives)\"] = {\n",
        "    \"metrics\": test_metrics_hardneg,\n",
        "    \"model_type\": \"Fine-tuned with Hard Negatives\",\n",
        "    \"base_model\": MODEL_NAME_HARDNEG,\n",
        "}\n",
        "\n",
        "# Display comprehensive comparison\n",
        "compare_all_models(\n",
        "    test_metrics_hardneg,\n",
        "    all_models_results,\n",
        "    finetuned_name=\"Fine-tuned (Hard Negatives)\",\n",
        ")"
      ],
      "id": "c42a9f2b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "490c49a0"
      },
      "source": [
        "### Detailed Comparison: Standard vs Hard Negatives Fine-tuning"
      ],
      "id": "490c49a0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b6f8783"
      },
      "outputs": [],
      "source": [
        "def compare_finetuning_methods(\n",
        "    standard_metrics: Dict[str, float],\n",
        "    hardneg_metrics: Dict[str, float],\n",
        "):\n",
        "    \"\"\"\n",
        "    Compare standard fine-tuning vs hard negatives fine-tuning\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    metric_names = [\"Hit@10\", \"Precision@10\", \"Recall@10\", \"MRR\", \"MAP@10\", \"NDCG@10\"]\n",
        "\n",
        "    standard_values = []\n",
        "    hardneg_values = []\n",
        "    display_names = []\n",
        "\n",
        "    for metric in metric_names:\n",
        "        if metric in standard_metrics and metric in hardneg_metrics:\n",
        "            standard_values.append(standard_metrics[metric])\n",
        "            hardneg_values.append(hardneg_metrics[metric])\n",
        "            display_names.append(metric)\n",
        "\n",
        "    if not display_names:\n",
        "        logger.warning(\"No common metrics found for comparison\")\n",
        "        return\n",
        "\n",
        "    # Create comparison plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Bar chart comparison\n",
        "    x = range(len(display_names))\n",
        "    width = 0.35\n",
        "\n",
        "    ax1.bar(\n",
        "        [i - width / 2 for i in x],\n",
        "        standard_values,\n",
        "        width,\n",
        "        label=\"Standard MNRL\",\n",
        "        alpha=0.8,\n",
        "        color=\"steelblue\",\n",
        "    )\n",
        "    ax1.bar(\n",
        "        [i + width / 2 for i in x],\n",
        "        hardneg_values,\n",
        "        width,\n",
        "        label=\"Hard Negatives\",\n",
        "        alpha=0.8,\n",
        "        color=\"orange\",\n",
        "    )\n",
        "    ax1.set_xlabel(\"Metrics\")\n",
        "    ax1.set_ylabel(\"Score\")\n",
        "    ax1.set_title(\"Standard Fine-tuning vs Hard Negatives Fine-tuning\")\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(display_names, rotation=45, ha=\"right\")\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Improvement percentage\n",
        "    improvements = [\n",
        "        (h - s) / s * 100 if s > 0 else 0\n",
        "        for s, h in zip(standard_values, hardneg_values)\n",
        "    ]\n",
        "\n",
        "    colors = [\"green\" if imp > 0 else \"red\" for imp in improvements]\n",
        "\n",
        "    ax2.bar(x, improvements, color=colors, alpha=0.7)\n",
        "    ax2.set_xlabel(\"Metrics\")\n",
        "    ax2.set_ylabel(\"Improvement (%)\")\n",
        "    ax2.set_title(\"Hard Negatives Improvement over Standard Fine-tuning\")\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels(display_names, rotation=45, ha=\"right\")\n",
        "    ax2.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print comparison summary\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Fine-tuning Methods Comparison\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\n",
        "        f\"\\n{'Metric':<20} {'Standard MNRL':>15} {'Hard Negatives':>15} {'Improvement':>15}\"\n",
        "    )\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for name, std_val, hn_val, imp in zip(\n",
        "        display_names, standard_values, hardneg_values, improvements\n",
        "    ):\n",
        "        arrow = \"↑\" if imp > 0 else \"↓\"\n",
        "        print(\n",
        "            f\"{name:<20} {std_val:>15.4f} {hn_val:>15.4f} {arrow:>2} {abs(imp):>12.2f}%\"\n",
        "        )\n",
        "\n",
        "    print(\"=\" * 80 + \"\\n\")"
      ],
      "id": "2b6f8783"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "e211139a"
      },
      "outputs": [],
      "source": [
        "# Compare the two fine-tuning approaches\n",
        "compare_finetuning_methods(test_metrics, test_metrics_hardneg)"
      ],
      "id": "e211139a"
    },
    {
      "cell_type": "markdown",
      "id": "2716e861",
      "metadata": {
        "id": "2716e861"
      },
      "source": [
        "## 8. Results Visualization\n",
        "\n",
        "Visualize training results and evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41d6a61",
      "metadata": {
        "id": "e41d6a61"
      },
      "outputs": [],
      "source": [
        "# Configure matplotlib for notebook display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "# For Jupyter/IPython environments\n",
        "try:\n",
        "    from IPython.core.getipython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    if ipython is not None:\n",
        "        ipython.run_line_magic(\"matplotlib\", \"inline\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Set default style\n",
        "plt.style.use(\"default\")\n",
        "matplotlib.rcParams[\"figure.dpi\"] = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35b364e5",
      "metadata": {
        "id": "35b364e5"
      },
      "outputs": [],
      "source": [
        "def plot_evaluation_results(results_csv_path: str):\n",
        "    \"\"\"\n",
        "    Plot evaluation results\n",
        "\n",
        "    Args:\n",
        "        results_csv_path: Path to the evaluation results CSV file\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    if not Path(results_csv_path).exists():\n",
        "        logger.warning(f\"Results file not found: {results_csv_path}\")\n",
        "        print(f\"File not found: {results_csv_path}\")\n",
        "        print(\n",
        "            f\"Please make sure the model has been trained and evaluation results exist.\"\n",
        "        )\n",
        "        return\n",
        "\n",
        "    # Read results\n",
        "    df = pd.read_csv(results_csv_path)\n",
        "    # take the last evaluation result of this epoch\n",
        "    df[\"epoch\"] = pd.to_numeric(df[\"epoch\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"epoch\"])\n",
        "\n",
        "    if df[\"epoch\"].duplicated().any():\n",
        "        if \"steps\" in df.columns:\n",
        "            df[\"steps\"] = pd.to_numeric(df[\"steps\"], errors=\"coerce\").fillna(-1)\n",
        "            df = (\n",
        "                df.sort_values([\"epoch\", \"steps\"])\n",
        "                .groupby(\"epoch\", as_index=False)\n",
        "                .last()\n",
        "            )\n",
        "        else:\n",
        "            df = df.groupby(\"epoch\", as_index=False).last()\n",
        "\n",
        "    print(f\"Loaded results with {len(df)} rows\")\n",
        "    print(f\"Available columns: {df.columns.tolist()}\")\n",
        "\n",
        "    # Define metrics to plot (try both naming conventions)\n",
        "    # sentence-transformers uses format: \"cosine-NDCG@10\"\n",
        "    # Alternative format: \"ndcg_at_10\"\n",
        "    # metric_mappings = {\n",
        "    #     \"NDCG@10\": [\"cosine-NDCG@10\", \"ndcg_at_10\"],\n",
        "    #     \"MAP@100\": [\"cosine-MAP@100\", \"map_at_100\"],\n",
        "    #     \"Recall@10\": [\"cosine-Recall@10\", \"recall_at_10\"],\n",
        "    #     \"Precision@10\": [\"cosine-Precision@10\", \"precision_at_10\"],\n",
        "    # }\n",
        "    metric_mappings = {\n",
        "        \"NDCG@10\": \"cosine-NDCG@10\",\n",
        "        \"MAP@100\": \"cosine-MAP@100\",\n",
        "        \"Recall@10\": \"cosine-Recall@10\",\n",
        "        \"Precision@10\": \"cosine-Precision@10\",\n",
        "    }\n",
        "\n",
        "    # Find which metrics are available\n",
        "    available_metrics = {k: v for k, v in metric_mappings.items() if v in df.columns}\n",
        "\n",
        "    if not available_metrics:\n",
        "        print(\"Warning: cosine metrics not found in CSV\")\n",
        "        print(f\"Available columns: {df.columns.tolist()}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nPlotting metrics: {list(available_metrics.keys())}\")\n",
        "\n",
        "    # Create subplots\n",
        "    n_metrics = len(available_metrics)\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, (display_name, col_name) in enumerate(available_metrics.items()):\n",
        "        ax = axes[i]\n",
        "        ax.plot(\n",
        "            df[\"epoch\"],\n",
        "            df[col_name],\n",
        "            marker=\"o\",\n",
        "            linewidth=2,\n",
        "            markersize=6,\n",
        "            color=\"steelblue\",\n",
        "        )\n",
        "        ax.set_title(display_name, fontsize=12, fontweight=\"bold\")\n",
        "        ax.set_xlabel(\"Epoch\", fontsize=10)\n",
        "        ax.set_ylabel(\"Score\", fontsize=10)\n",
        "        ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
        "\n",
        "        # Add value annotations on points (only if not too many points)\n",
        "        if len(df) <= 20:\n",
        "            for x, y in zip(df[\"epoch\"], df[col_name]):\n",
        "                ax.annotate(\n",
        "                    f\"{y:.3f}\",\n",
        "                    (x, y),\n",
        "                    textcoords=\"offset points\",\n",
        "                    xytext=(0, 5),\n",
        "                    ha=\"center\",\n",
        "                    fontsize=7,\n",
        "                    alpha=0.7,\n",
        "                )\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for i in range(n_metrics, 4):\n",
        "        axes[i].set_visible(False)\n",
        "\n",
        "    plt.suptitle(\"Training Evaluation Results\", fontsize=14, fontweight=\"bold\", y=1.00)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    df = df.sort_values(\"epoch\").reset_index(drop=True)\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Evaluation Results Summary\")\n",
        "    print(\"=\" * 70)\n",
        "    for display_name, col_name in available_metrics.items():\n",
        "        best_idx = df[col_name].idxmax()\n",
        "        best_epoch = df.loc[best_idx, \"epoch\"]\n",
        "        best_score = df[col_name].max()\n",
        "        final_score = df[col_name].iloc[-1]\n",
        "        print(f\"{display_name}:\")\n",
        "        print(f\"  Best:  {best_score:.4f} (epoch {int(best_epoch)})\")\n",
        "        print(f\"  Final: {final_score:.4f}\")\n",
        "        base = df[col_name].iloc[0]\n",
        "        improve = ((final_score - base) / base * 100) if base != 0 else float(\"nan\")\n",
        "        print(f\"  Improvement: {improve:.2f}%\")\n",
        "        print(\"=\" * 70)\n",
        "    print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c57ce6c4",
      "metadata": {
        "id": "c57ce6c4"
      },
      "outputs": [],
      "source": [
        "# Visualize results (if training logs exist)\n",
        "# Uncomment the lines below after training is complete\n",
        "results_path = f\"{OUTPUT_DIR}/eval/Information-Retrieval_evaluation_dev_results.csv\"\n",
        "plot_evaluation_results(results_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a3630f1"
      },
      "source": [
        "### Visualize Hard Negatives Training Results"
      ],
      "id": "2a3630f1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "59e48edc"
      },
      "outputs": [],
      "source": [
        "# Visualize hard negatives training results\n",
        "results_path_hardneg = (\n",
        "    f\"{OUTPUT_DIR_HARDNEG}/eval/Information-Retrieval_evaluation_dev_results.csv\"\n",
        ")\n",
        "plot_evaluation_results(results_path_hardneg)"
      ],
      "id": "59e48edc"
    },
    {
      "cell_type": "markdown",
      "id": "41ba41de",
      "metadata": {
        "id": "41ba41de"
      },
      "source": [
        "## 9. Summary\n",
        "\n",
        "This notebook implements a complete semantic retrieval system, including:\n",
        "\n",
        "1. **Data Loading and Preprocessing**: Load SciFact dataset from HuggingFace and convert to BEIR format\n",
        "2. **Model Training**: Fine-tune sentence-transformers model using MultipleNegativesRankingLoss\n",
        "3. **Evaluation**: Evaluate model performance on test set with multiple retrieval metrics\n",
        "4. **Inference**: Perform semantic retrieval using the trained model"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4a354efa0d444ba59925b1a6e8df20cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc607c6d5b5d4e55a4d72a6b64288d02",
              "IPY_MODEL_6ad8066348a54d1ba6e57cf093d69e7f",
              "IPY_MODEL_a3ec46ea51a24a128ccd8125ef714ab1"
            ],
            "layout": "IPY_MODEL_4ae3a4a6ce99404498005695f00e3504"
          }
        },
        "bc607c6d5b5d4e55a4d72a6b64288d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b4506da2b2f4eb084c72f3f6c1cd30d",
            "placeholder": "​",
            "style": "IPY_MODEL_c03e3b9da543442c9c022c9b480b8037",
            "value": "README.md: "
          }
        },
        "6ad8066348a54d1ba6e57cf093d69e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad5c72d46e2a434ab75937139dcb1f34",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96a15e07e5ee44a0babf599e803aa429",
            "value": 1
          }
        },
        "a3ec46ea51a24a128ccd8125ef714ab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b82b4ab0d3d4423cbaf541dd3a59be1d",
            "placeholder": "​",
            "style": "IPY_MODEL_6e89f3f652f44b309188ddd52b45485a",
            "value": " 14.0k/? [00:00&lt;00:00, 1.58MB/s]"
          }
        },
        "4ae3a4a6ce99404498005695f00e3504": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b4506da2b2f4eb084c72f3f6c1cd30d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c03e3b9da543442c9c022c9b480b8037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad5c72d46e2a434ab75937139dcb1f34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "96a15e07e5ee44a0babf599e803aa429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b82b4ab0d3d4423cbaf541dd3a59be1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e89f3f652f44b309188ddd52b45485a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a1ce9dae057497bba86148bff76d490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8a5b2851e534663b9a1bec696a80762",
              "IPY_MODEL_cff35b502e6344e5a9c1dfdcb6d6dc49",
              "IPY_MODEL_9fe08c7437374f87bc1f1907fe200fe4"
            ],
            "layout": "IPY_MODEL_491798cd6f924be6933fdfee6a2f2b6b"
          }
        },
        "a8a5b2851e534663b9a1bec696a80762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4add5223fd9405f8ee1f72aeadbc5b7",
            "placeholder": "​",
            "style": "IPY_MODEL_254934befb7c4ad58da9e2bc4d660cfd",
            "value": "train.jsonl.gz: 100%"
          }
        },
        "cff35b502e6344e5a9c1dfdcb6d6dc49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b7adc3323f84a5988453c9bbb055057",
            "max": 3434010,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e2d9b9105854699a81ddfdab3dbbd9b",
            "value": 3434010
          }
        },
        "9fe08c7437374f87bc1f1907fe200fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_462be654cc7c4635b33c07853705a842",
            "placeholder": "​",
            "style": "IPY_MODEL_8f820c23ced24d3abc317cf7bbe25dd6",
            "value": " 3.43M/3.43M [00:01&lt;00:00, 3.13MB/s]"
          }
        },
        "491798cd6f924be6933fdfee6a2f2b6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4add5223fd9405f8ee1f72aeadbc5b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "254934befb7c4ad58da9e2bc4d660cfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b7adc3323f84a5988453c9bbb055057": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e2d9b9105854699a81ddfdab3dbbd9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "462be654cc7c4635b33c07853705a842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f820c23ced24d3abc317cf7bbe25dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3bda4dc84164db28c5c3755a9f07b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6305bb1cb8224994bca18ccdf65fdfb0",
              "IPY_MODEL_9d9543ba25054ff79ee2ee772b72b59a",
              "IPY_MODEL_df5be92b44894b428700aef9fd217bed"
            ],
            "layout": "IPY_MODEL_e02d3fedd82f4fb4bdfed1b324f4285a"
          }
        },
        "6305bb1cb8224994bca18ccdf65fdfb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c37916a87e874a17aee555037d8077f8",
            "placeholder": "​",
            "style": "IPY_MODEL_0b2d2dcabc834fb39641e28420b28c3d",
            "value": "Generating train split: 100%"
          }
        },
        "9d9543ba25054ff79ee2ee772b72b59a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee7b5e0a252c4283bbdd97e0224997b4",
            "max": 15422,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_abaac28452d342808ea96511e31b4a4c",
            "value": 15422
          }
        },
        "df5be92b44894b428700aef9fd217bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_316188e232774a0a93e173249a84fe7b",
            "placeholder": "​",
            "style": "IPY_MODEL_097b8ba05bc24ee19322fd7a098e6184",
            "value": " 15422/15422 [00:00&lt;00:00, 85046.87 examples/s]"
          }
        },
        "e02d3fedd82f4fb4bdfed1b324f4285a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c37916a87e874a17aee555037d8077f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b2d2dcabc834fb39641e28420b28c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee7b5e0a252c4283bbdd97e0224997b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abaac28452d342808ea96511e31b4a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "316188e232774a0a93e173249a84fe7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "097b8ba05bc24ee19322fd7a098e6184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}