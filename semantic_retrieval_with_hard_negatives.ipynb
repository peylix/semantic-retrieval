{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54db718e",
   "metadata": {},
   "source": [
    "# Semantic Retrieval for Scientific Documents\n",
    "\n",
    "This notebook implements a deep learning-based semantic retrieval system, trained and evaluated on the SciFact dataset.\n",
    "\n",
    "## Project Overview\n",
    "- Fine-tune embedding models using sentence-transformers\n",
    "- Use MultipleNegativesRankingLoss loss function\n",
    "- Train and evaluate on BEIR-format datasets\n",
    "- Compare with traditional methods like BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541d0db3",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff22829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Colab, uncomment the line below to install dependencies\n",
    "!pip install sentence-transformers datasets pandas scikit-learn torch accelerate beir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26491a24",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Set, List, Tuple, Optional, Any, Union\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    from IPython import get_ipython  # type: ignore\n",
    "except Exception:  # pragma: no cover\n",
    "\n",
    "    def get_ipython():\n",
    "        return None\n",
    "\n",
    "\n",
    "try:\n",
    "    from IPython.core.getipython import get_ipython\n",
    "except Exception:  # pragma: no cover\n",
    "\n",
    "    def get_ipython():\n",
    "        return None\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca6c28f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "We use the BeIR/scifact-generated-queries dataset, a scientific literature retrieval dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222faa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scifact_raw(output_dir: str = \"./data/raw\"):\n",
    "    \"\"\"\n",
    "    Load BeIR/scifact-generated-queries dataset from HuggingFace\n",
    "    \"\"\"\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    raw_dir = Path(output_dir)\n",
    "    raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load dataset\n",
    "    logger.info(\"Loading SciFact dataset from HuggingFace...\")\n",
    "    ds = load_dataset(\"BeIR/scifact-generated-queries\")\n",
    "    df = ds[\"train\"].to_pandas()  # type: ignore\n",
    "\n",
    "    # Save raw data\n",
    "    raw_path = raw_dir / \"scifact_raw.csv\"\n",
    "    df.to_csv(raw_path, index=False)  # type: ignore\n",
    "    logger.info(f\"Raw data saved to {raw_path}\")\n",
    "    logger.info(f\"Dataset size: {len(df)} rows\")  # type: ignore\n",
    "\n",
    "    return df  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f296a1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def preprocess_scifact(raw_df: pd.DataFrame, output_dir: str = \"./data/processed\"):\n",
    "    \"\"\"\n",
    "    Preprocess SciFact dataset and generate BEIR-format training data\n",
    "\n",
    "    Output files:\n",
    "    - scifact_pairs.csv: Query-document pairs\n",
    "    - scifact_corpus.csv: Deduplicated document corpus\n",
    "    - beir_format/corpus.jsonl: BEIR-format corpus\n",
    "    - beir_format/queries.jsonl: BEIR-format queries\n",
    "    - beir_format/qrels/train.tsv: Training set relevance labels (70%)\n",
    "    - beir_format/qrels/dev.tsv: Validation set relevance labels (10%)\n",
    "    - beir_format/qrels/test.tsv: Test set relevance labels (20%)\n",
    "    \"\"\"\n",
    "    processed_dir = Path(output_dir)\n",
    "    processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Process data\n",
    "    df = raw_df[[\"_id\", \"title\", \"text\", \"query\"]].copy()\n",
    "    df = df.dropna(subset=[\"text\", \"query\"])  # type: ignore\n",
    "    df = df.rename(columns={\"_id\": \"doc_id\"})\n",
    "\n",
    "    # Data cleaning\n",
    "    df[\"title\"] = df[\"title\"].fillna(\"\").astype(str).str.strip()\n",
    "    df[\"text\"] = df[\"text\"].fillna(\"\").astype(str).str.strip()\n",
    "    df[\"query\"] = df[\"query\"].astype(str).str.strip()\n",
    "\n",
    "    # Combine title and text\n",
    "    df[\"content\"] = df[\"title\"] + \". \" + df[\"text\"]\n",
    "\n",
    "    # Assign ID to each unique query\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"query_id\"] = pd.factorize(df[\"query\"])[0]\n",
    "\n",
    "    # Save query-document pairs\n",
    "    pairs_path = processed_dir / \"scifact_pairs.csv\"\n",
    "    df.to_csv(pairs_path, index=False)\n",
    "    logger.info(f\"Query-doc pairs saved to {pairs_path}\")\n",
    "\n",
    "    # Save deduplicated corpus\n",
    "    corpus = (\n",
    "        df[[\"doc_id\", \"content\"]]\n",
    "        .drop_duplicates(subset=[\"doc_id\"])  # type: ignore\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    corpus_path = processed_dir / \"scifact_corpus.csv\"\n",
    "    corpus.to_csv(corpus_path, index=False)\n",
    "    logger.info(f\"Corpus saved to {corpus_path} ({len(corpus)} documents)\")\n",
    "\n",
    "    # Create BEIR-format data\n",
    "    beir_dir = processed_dir / \"beir_format\"\n",
    "    beir_dir.mkdir(parents=True, exist_ok=True)\n",
    "    qrels_dir = beir_dir / \"qrels\"\n",
    "    qrels_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Write corpus.jsonl\n",
    "    corpus_jsonl_path = beir_dir / \"corpus.jsonl\"\n",
    "    with open(corpus_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in corpus.iterrows():\n",
    "            doc = {\n",
    "                \"_id\": str(row[\"doc_id\"]),\n",
    "                \"text\": row[\"content\"],\n",
    "                \"title\": \"\",\n",
    "            }\n",
    "            f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "    logger.info(f\"BEIR corpus saved to {corpus_jsonl_path}\")\n",
    "\n",
    "    # Write queries.jsonl\n",
    "    queries_jsonl_path = beir_dir / \"queries.jsonl\"\n",
    "    unique_queries = df[[\"query_id\", \"query\"]].drop_duplicates(subset=[\"query_id\"])  # type: ignore\n",
    "    with open(queries_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in unique_queries.iterrows():\n",
    "            query = {\"_id\": str(row[\"query_id\"]), \"text\": row[\"query\"]}\n",
    "            f.write(json.dumps(query, ensure_ascii=False) + \"\\n\")\n",
    "    logger.info(f\"BEIR queries saved to {queries_jsonl_path}\")\n",
    "\n",
    "    # Split into train/dev/test sets (70/10/20)\n",
    "    unique_query_ids = df[\"query_id\"].unique()\n",
    "    train_dev_query_ids, test_query_ids = train_test_split(\n",
    "        unique_query_ids, test_size=0.2, random_state=42\n",
    "    )\n",
    "    train_query_ids, dev_query_ids = train_test_split(\n",
    "        train_dev_query_ids, test_size=0.125, random_state=42\n",
    "    )\n",
    "\n",
    "    train_df = df[df[\"query_id\"].isin(train_query_ids)]\n",
    "    dev_df = df[df[\"query_id\"].isin(dev_query_ids)]\n",
    "    test_df = df[df[\"query_id\"].isin(test_query_ids)]\n",
    "\n",
    "    # Write qrels files\n",
    "    for split_name, split_df in [\n",
    "        (\"train\", train_df),\n",
    "        (\"dev\", dev_df),\n",
    "        (\"test\", test_df),\n",
    "    ]:\n",
    "        qrels_path = qrels_dir / f\"{split_name}.tsv\"\n",
    "        with open(qrels_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for _, row in split_df.iterrows():\n",
    "                f.write(f\"{row['query_id']}\\t{row['doc_id']}\\t1\\n\")\n",
    "        logger.info(f\"{split_name} qrels saved to {qrels_path} ({len(split_df)} pairs)\")\n",
    "\n",
    "    logger.info(f\"\\nData preprocessing completed!\")\n",
    "    logger.info(f\"Train: {len(train_query_ids)} queries, {len(train_df)} pairs\")\n",
    "    logger.info(f\"Dev: {len(dev_query_ids)} queries, {len(dev_df)} pairs\")\n",
    "    logger.info(f\"Test: {len(test_query_ids)} queries, {len(test_df)} pairs\")\n",
    "\n",
    "    return beir_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a20f3ca",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 3. Data Loading Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qrels(qrels_path: Path) -> Dict[str, Set[str]]:\n",
    "    \"\"\"Load qrels file and return query_id -> {doc_ids} mapping\"\"\"\n",
    "    query_to_docs = defaultdict(set)\n",
    "\n",
    "    with open(qrels_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) >= 2:\n",
    "                query_id = parts[0]\n",
    "                doc_id = parts[1]\n",
    "                query_to_docs[query_id].add(doc_id)\n",
    "\n",
    "    return dict(query_to_docs)\n",
    "\n",
    "\n",
    "def load_queries(queries_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Load queries.jsonl file and return query_id -> query_text mapping\"\"\"\n",
    "    queries = {}\n",
    "\n",
    "    with open(queries_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            query = json.loads(line)\n",
    "            queries[query[\"_id\"]] = query[\"text\"]\n",
    "\n",
    "    return queries\n",
    "\n",
    "\n",
    "def load_corpus(corpus_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Load corpus.jsonl file and return doc_id -> doc_text mapping\"\"\"\n",
    "    corpus = {}\n",
    "\n",
    "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            corpus[doc[\"_id\"]] = doc[\"text\"]\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c17b5",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Fine-tune embedding models using sentence-transformers library with MultipleNegativesRankingLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732affca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "# Use MiniLM for zero-shot / fine-tuning baseline in this notebook\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "WARMUP_RATIO = 0.1\n",
    "LEARNING_RATE = 3e-5\n",
    "OUTPUT_DIR = \"/content/drive/semantic-retrieval/models/finetuned-mnrl\"\n",
    "\n",
    "IN_COLAB = \"google.colab\" in str(get_ipython())\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    BASE_DIR = Path(\"/content/drive/MyDrive/semantic-retrieval\")\n",
    "    OUTPUT_DIR = str(BASE_DIR / \"models/finetuned-mnrl\")\n",
    "    DATA_DIR = str(BASE_DIR / \"data\")\n",
    "    RESULTS_DIR = str(BASE_DIR / \"results\")\n",
    "else:\n",
    "    BASE_DIR = Path(\".\")\n",
    "    OUTPUT_DIR = \"./models/finetuned-mnrl\"\n",
    "    DATA_DIR = \"./data\"\n",
    "    RESULTS_DIR = \"./results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225708e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(data_dir: Path) -> list:\n",
    "    \"\"\"\n",
    "    Load training data and convert to InputExample format\n",
    "\n",
    "    For MultipleNegativesRankingLoss, we only need (query, positive_doc) pairs.\n",
    "    Negative samples are drawn from other examples in the same batch.\n",
    "    \"\"\"\n",
    "    from sentence_transformers import InputExample\n",
    "\n",
    "    qrels_path = data_dir / \"qrels\" / \"train.tsv\"\n",
    "    queries_path = data_dir / \"queries.jsonl\"\n",
    "    corpus_path = data_dir / \"corpus.jsonl\"\n",
    "\n",
    "    logger.info(\"Loading training data...\")\n",
    "    query_to_docs = load_qrels(qrels_path)\n",
    "    queries = load_queries(queries_path)\n",
    "    corpus = load_corpus(corpus_path)\n",
    "\n",
    "    # Create training examples\n",
    "    examples = []\n",
    "    for query_id, doc_ids in query_to_docs.items():\n",
    "        if query_id not in queries:\n",
    "            continue\n",
    "\n",
    "        query_text = queries[query_id]\n",
    "\n",
    "        for doc_id in doc_ids:\n",
    "            if doc_id not in corpus:\n",
    "                continue\n",
    "\n",
    "            doc_text = corpus[doc_id]\n",
    "            examples.append(InputExample(texts=[query_text, doc_text]))\n",
    "\n",
    "    logger.info(f\"Created {len(examples)} training examples\")\n",
    "    return examples\n",
    "\n",
    "\n",
    "def create_evaluator(data_dir: Path, split: str = \"dev\"):\n",
    "    \"\"\"\n",
    "    Create InformationRetrievalEvaluator for validation during training\n",
    "    \"\"\"\n",
    "    from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "    qrels_path = data_dir / \"qrels\" / f\"{split}.tsv\"\n",
    "    queries_path = data_dir / \"queries.jsonl\"\n",
    "    corpus_path = data_dir / \"corpus.jsonl\"\n",
    "\n",
    "    logger.info(f\"Loading {split} data for evaluation...\")\n",
    "    query_to_docs = load_qrels(qrels_path)\n",
    "    queries = load_queries(queries_path)\n",
    "    corpus = load_corpus(corpus_path)\n",
    "\n",
    "    # Filter queries and documents\n",
    "    eval_queries = {qid: queries[qid] for qid in query_to_docs if qid in queries}\n",
    "    eval_corpus = corpus\n",
    "\n",
    "    # Convert qrels format\n",
    "    eval_qrels: Dict[str, Set[str]] = {\n",
    "        qid: set(doc_ids)\n",
    "        for qid, doc_ids in query_to_docs.items()\n",
    "        if qid in eval_queries\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Evaluator: {len(eval_queries)} queries, {len(eval_corpus)} documents\")\n",
    "\n",
    "    return InformationRetrievalEvaluator(\n",
    "        queries=eval_queries,\n",
    "        corpus=eval_corpus,\n",
    "        relevant_docs=eval_qrels,\n",
    "        name=split,\n",
    "        ndcg_at_k=[10, 100],\n",
    "        precision_recall_at_k=[10, 100],\n",
    "        map_at_k=[100],\n",
    "        mrr_at_k=[10],\n",
    "        show_progress_bar=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851cd830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir: Path, output_dir: str = OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Train the embedding model\n",
    "    \"\"\"\n",
    "    from sentence_transformers import SentenceTransformer, losses\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load model\n",
    "    logger.info(f\"Loading model: {MODEL_NAME}\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "    # Load training data\n",
    "    train_examples = load_training_data(data_dir)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_examples,  # type: ignore\n",
    "        shuffle=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    # Setup loss function\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "    # Create validation evaluator\n",
    "    dev_evaluator = create_evaluator(data_dir, split=\"dev\")\n",
    "\n",
    "    # Calculate training steps\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "    logger.info(f\"\\nTraining configuration:\")\n",
    "    logger.info(f\"  Model: {MODEL_NAME}\")\n",
    "    logger.info(f\"  Batch size: {BATCH_SIZE}\")\n",
    "    logger.info(f\"  Epochs: {EPOCHS}\")\n",
    "    logger.info(f\"  Total steps: {total_steps}\")\n",
    "    logger.info(f\"  Warmup steps: {warmup_steps}\")\n",
    "    logger.info(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "    logger.info(f\"  Output directory: {output_path}\")\n",
    "\n",
    "    # Start training\n",
    "    logger.info(\"\\nStarting training...\")\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        evaluator=dev_evaluator,\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=warmup_steps,\n",
    "        optimizer_params={\"lr\": LEARNING_RATE},\n",
    "        output_path=str(output_path),\n",
    "        evaluation_steps=len(train_dataloader) // 2,  # Evaluate twice per epoch\n",
    "        save_best_model=True,\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "\n",
    "    logger.info(f\"\\nTraining completed! Model saved to: {output_path}\")\n",
    "\n",
    "    # Final evaluation on test set\n",
    "    logger.info(\"\\nRunning final evaluation on test set...\")\n",
    "    test_evaluator = create_evaluator(data_dir, split=\"test\")\n",
    "    test_results = test_evaluator(model, output_path=str(output_path))\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\" * 50)\n",
    "    logger.info(\"Final Test Results:\")\n",
    "    for metric, value in test_results.items():\n",
    "        logger.info(f\"  {metric}: {value:.4f}\")\n",
    "    logger.info(\"=\" * 50)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e8443",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics\n",
    "\n",
    "Implement standard information retrieval evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4dae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type alias for ground truth\n",
    "GroundTruth = Union[str, List[str], Set[str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6216e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_eval(samples: List[Dict[str, Any]], k: int = 10) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluation for retrieval with single or multi-ground-truth.\n",
    "\n",
    "    Args:\n",
    "        samples: List of samples in the format:\n",
    "            [{\n",
    "                \"question\": str,\n",
    "                \"contexts\": List[str],  # ranked top-K doc_ids\n",
    "                \"ground_truth\": str | list | set  # gold doc_id(s)\n",
    "            }]\n",
    "        k: Cutoff position for evaluation\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing various metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def to_set(gt: GroundTruth) -> Set[str]:\n",
    "        if isinstance(gt, set):\n",
    "            return set(str(x) for x in gt)\n",
    "        if isinstance(gt, list):\n",
    "            return set(str(x) for x in gt)\n",
    "        # single string / single id\n",
    "        return {str(gt)}\n",
    "\n",
    "    def hit_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
    "        return 1.0 if any(r in gt_set for r in results[:k]) else 0.0\n",
    "\n",
    "    def precision_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
    "        if k <= 0:\n",
    "            return 0.0\n",
    "        hits = sum(1 for r in results[:k] if r in gt_set)\n",
    "        return hits / k\n",
    "\n",
    "    def recall_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
    "        if not gt_set:\n",
    "            return 0.0\n",
    "        hits = sum(1 for r in results[:k] if r in gt_set)\n",
    "        return hits / len(gt_set)\n",
    "\n",
    "    def mrr(gt_set: Set[str], results: List[str]) -> float:\n",
    "        for rank, r in enumerate(results, start=1):\n",
    "            if r in gt_set:\n",
    "                return 1.0 / rank\n",
    "        return 0.0\n",
    "\n",
    "    def average_precision_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
    "        \"\"\"\n",
    "        AP@K for multi-relevant:\n",
    "          AP = (1 / min(|GT|, K)) * sum_{i=1..K} Precision@i * rel_i\n",
    "        \"\"\"\n",
    "        if not gt_set:\n",
    "            return 0.0\n",
    "\n",
    "        hits = 0\n",
    "        s = 0.0\n",
    "        for i, r in enumerate(results[:k], start=1):\n",
    "            if r in gt_set:\n",
    "                hits += 1\n",
    "                s += hits / i\n",
    "        return s / min(len(gt_set), k)\n",
    "\n",
    "    def ndcg_at_k(gt_set: Set[str], results: List[str], k: int) -> float:\n",
    "        # binary relevance\n",
    "        dcg = 0.0\n",
    "        for i, r in enumerate(results[:k], start=1):\n",
    "            rel = 1.0 if r in gt_set else 0.0\n",
    "            dcg += rel / math.log2(i + 1)\n",
    "\n",
    "        # ideal DCG: all relevant docs first\n",
    "        ideal_rels = [1.0] * min(len(gt_set), k)\n",
    "        idcg = 0.0\n",
    "        for i, rel in enumerate(ideal_rels, start=1):\n",
    "            idcg += rel / math.log2(i + 1)\n",
    "\n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    hits, precisions, recalls, mrrs, aps, ndcgs = [], [], [], [], [], []\n",
    "\n",
    "    for s in samples:\n",
    "        gt_set = to_set(s[\"ground_truth\"])\n",
    "        results = [str(x) for x in s[\"contexts\"]]\n",
    "\n",
    "        hits.append(hit_at_k(gt_set, results, k))\n",
    "        precisions.append(precision_at_k(gt_set, results, k))\n",
    "        recalls.append(recall_at_k(gt_set, results, k))\n",
    "        mrrs.append(mrr(gt_set, results))\n",
    "        aps.append(average_precision_at_k(gt_set, results, k))\n",
    "        ndcgs.append(ndcg_at_k(gt_set, results, k))\n",
    "\n",
    "    n = len(samples) if samples else 1\n",
    "    return {\n",
    "        f\"Hit@{k}\": sum(hits) / n,\n",
    "        f\"Precision@{k}\": sum(precisions) / n,\n",
    "        f\"Recall@{k}\": sum(recalls) / n,\n",
    "        \"MRR\": sum(mrrs) / n,\n",
    "        f\"MAP@{k}\": sum(aps) / n,\n",
    "        f\"NDCG@{k}\": sum(ndcgs) / n,\n",
    "        \"N\": float(len(samples)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baccad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_baseline_results(results_dir) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load baseline model results from JSON files\n",
    "\n",
    "    Args:\n",
    "        results_dir: Directory containing baseline result JSON files\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping model name to results\n",
    "    \"\"\"\n",
    "    results_path = Path(results_dir)\n",
    "    baseline_results = {}\n",
    "\n",
    "    if not results_path.exists():\n",
    "        logger.warning(f\"Results directory not found: {results_path}\")\n",
    "        return baseline_results\n",
    "\n",
    "    # Load all JSON files in results directory\n",
    "    for json_file in results_path.glob(\"*_results.json\"):\n",
    "        try:\n",
    "            with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                model_name = data.get(\"model_name\", json_file.stem)\n",
    "                baseline_results[model_name] = data\n",
    "                logger.info(f\"Loaded baseline results: {model_name}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load {json_file.name}: {e}\")\n",
    "\n",
    "    return baseline_results\n",
    "\n",
    "\n",
    "def display_baseline_comparison(baseline_results: Dict[str, Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Display comparison of baseline model results\n",
    "\n",
    "    Args:\n",
    "        baseline_results: Dictionary of baseline results from load_baseline_results\n",
    "    \"\"\"\n",
    "    if not baseline_results:\n",
    "        print(\n",
    "            \"No baseline results found. Please run baseline evaluation scripts first:\"\n",
    "        )\n",
    "        print(\"  - uv run src/embedding_model/zero_shot.py\")\n",
    "        print(\"  - uv run src/run_word2vec_eval.py\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BASELINE MODEL COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Collect all unique metrics\n",
    "    all_metrics = set()\n",
    "    for result in baseline_results.values():\n",
    "        all_metrics.update(result.get(\"metrics\", {}).keys())\n",
    "\n",
    "    # Remove 'N' from comparison metrics\n",
    "    all_metrics.discard(\"N\")\n",
    "    metric_list = sorted(all_metrics)\n",
    "\n",
    "    # Display results table\n",
    "    print(f\"\\n{'Model':<30}\", end=\"\")\n",
    "    for metric in metric_list:\n",
    "        print(f\"{metric:>12}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for model_name, result in baseline_results.items():\n",
    "        metrics = result.get(\"metrics\", {})\n",
    "        print(f\"{model_name:<30}\", end=\"\")\n",
    "        for metric in metric_list:\n",
    "            value = metrics.get(metric, 0.0)\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"{value:>12.4f}\", end=\"\")\n",
    "            else:\n",
    "                print(f\"{str(value):>12}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Display model details\n",
    "    print(\"\\nModel Details:\")\n",
    "    print(\"-\" * 80)\n",
    "    for model_name, result in baseline_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Type: {result.get('model_type', 'N/A')}\")\n",
    "        if \"base_model\" in result:\n",
    "            print(f\"  Base Model: {result['base_model']}\")\n",
    "        if \"parameters\" in result:\n",
    "            print(f\"  Parameters: {result['parameters']}\")\n",
    "        if \"N\" in result.get(\"metrics\", {}):\n",
    "            print(f\"  Test Samples: {int(result['metrics']['N'])}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be04e12f",
   "metadata": {},
   "source": [
    "## 6. Complete Training and Evaluation Pipeline\n",
    "\n",
    "Run the code below to execute the complete workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7accb471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load raw data\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"Step 1: Loading raw data\")\n",
    "logger.info(\"=\" * 60)\n",
    "raw_df = load_scifact_raw()\n",
    "\n",
    "# Display sample data\n",
    "logger.info(\"\\nSample data:\")\n",
    "print(raw_df.head())  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61095e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocess data\n",
    "logger.info(\"\\n\" + \"=\" * 60)\n",
    "logger.info(\"Step 2: Preprocessing data\")\n",
    "logger.info(\"=\" * 60)\n",
    "beir_dir = preprocess_scifact(raw_df)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa40aba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Step 3: Train model\n",
    "logger.info(\"\\n\" + \"=\" * 60)\n",
    "logger.info(\"Step 3: Training model\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "# Comment out the line below if you want to skip training\n",
    "model = train_model(beir_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd81699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load pre-trained model (if already trained)\n",
    "# Comment out this section if you already trained above\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer(OUTPUT_DIR)\n",
    "# logger.info(f\"Model loaded from {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3aa71",
   "metadata": {},
   "source": [
    "## 6.5. Fine-tuning with Hard Negatives (Enhanced Training)\n",
    "\n",
    "In this section, we enhance the training process by using **hard negatives**.\n",
    "Hard negatives are documents that are semantically similar to the query but not relevant,\n",
    "making them more challenging examples for the model to learn from.\n",
    "\n",
    "### Process:\n",
    "1. **Mine hard negatives**: Use the base model to retrieve top-k similar but irrelevant documents\n",
    "2. **Create training data**: Include explicit hard negatives in training examples\n",
    "3. **Train with CachedMultipleNegativesRankingLoss**: More efficient for larger batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f01ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard negatives training configuration (MiniLM for consistency)\n",
    "MODEL_NAME_HARDNEG = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "BATCH_SIZE_HARDNEG = 64\n",
    "MINI_BATCH_SIZE_HARDNEG = 16  # For CachedMultipleNegativesRankingLoss\n",
    "EPOCHS_HARDNEG = 4\n",
    "WARMUP_RATIO_HARDNEG = 0.1\n",
    "LEARNING_RATE_HARDNEG = 3e-5\n",
    "NEGATIVES_PER_QUERY = 2\n",
    "\n",
    "if IN_COLAB:\n",
    "    OUTPUT_DIR_HARDNEG = str(BASE_DIR / \"models/finetuned-mnrl-hardneg\")\n",
    "    HARD_NEGATIVES_PATH = str(\n",
    "        BASE_DIR / \"data/processed/beir_format/hard_negatives_train.jsonl\"\n",
    "    )\n",
    "else:\n",
    "    OUTPUT_DIR_HARDNEG = \"./models/finetuned-mnrl-hardneg\"\n",
    "    HARD_NEGATIVES_PATH = \"./data/processed/beir_format/hard_negatives_train.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InputExampleDataset for hard negatives training\n",
    "class InputExampleDataset:\n",
    "    \"\"\"A thin Dataset wrapper around a list of InputExample objects.\"\"\"\n",
    "\n",
    "    def __init__(self, examples: list):\n",
    "        self.examples = list(examples)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.examples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3d0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_hard_negatives(\n",
    "    model,\n",
    "    data_dir: Path,\n",
    "    output_path: Path,\n",
    "    split: str = \"train\",\n",
    "    negatives_per_query: int = 2,\n",
    "    top_k: int = 50,\n",
    "    batch_size: int = 64,\n",
    "):\n",
    "    \"\"\"\n",
    "    Mine hard negatives using semantic search.\n",
    "\n",
    "    Args:\n",
    "        model: SentenceTransformer model to use for encoding\n",
    "        data_dir: Path to BEIR format data directory\n",
    "        output_path: Path to save hard negatives JSONL file\n",
    "        split: Which split to mine from (\"train\", \"dev\", or \"test\")\n",
    "        negatives_per_query: Number of hard negatives to keep per query\n",
    "        top_k: Top-k retrieved documents (before filtering positives)\n",
    "        batch_size: Batch size for encoding\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sentence_transformers import util\n",
    "\n",
    "    # Load data\n",
    "    qrels_path = data_dir / \"qrels\" / f\"{split}.tsv\"\n",
    "    queries_path = data_dir / \"queries.jsonl\"\n",
    "    corpus_path = data_dir / \"corpus.jsonl\"\n",
    "\n",
    "    logger.info(f\"Mining hard negatives for {split} split...\")\n",
    "    query_to_docs = load_qrels(qrels_path)\n",
    "    queries = load_queries(queries_path)\n",
    "    corpus = load_corpus(corpus_path)\n",
    "\n",
    "    # Encode corpus\n",
    "    logger.info(\"Encoding corpus for hard negative mining...\")\n",
    "    doc_ids = list(corpus.keys())\n",
    "    corpus_texts = [corpus[doc_id] for doc_id in doc_ids]\n",
    "    corpus_embeddings = model.encode(\n",
    "        corpus_texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True,\n",
    "    )\n",
    "\n",
    "    # Encode queries\n",
    "    logger.info(\"Encoding queries for hard negative mining...\")\n",
    "    query_ids = [qid for qid in query_to_docs.keys() if qid in queries]\n",
    "    query_texts = [queries[qid] for qid in query_ids]\n",
    "    query_embeddings = model.encode(\n",
    "        query_texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True,\n",
    "    )\n",
    "\n",
    "    # Semantic search\n",
    "    logger.info(f\"Running semantic search with top_k={top_k}...\")\n",
    "    search_results = util.semantic_search(\n",
    "        query_embeddings,\n",
    "        corpus_embeddings,\n",
    "        top_k=max(top_k, negatives_per_query * 5),\n",
    "        score_function=util.cos_sim,\n",
    "    )\n",
    "\n",
    "    # Save hard negatives\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    total_queries = 0\n",
    "    queries_with_negatives = 0\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, hits in enumerate(search_results):\n",
    "            query_id = query_ids[idx]\n",
    "            positives = query_to_docs.get(query_id, set())\n",
    "\n",
    "            # Filter out positive documents\n",
    "            hard_neg_ids = []\n",
    "            for hit in hits:\n",
    "                corpus_idx = hit[\"corpus_id\"]\n",
    "                doc_id = doc_ids[corpus_idx]\n",
    "\n",
    "                if doc_id in positives or doc_id in hard_neg_ids:\n",
    "                    continue\n",
    "\n",
    "                hard_neg_ids.append(doc_id)\n",
    "                if len(hard_neg_ids) >= negatives_per_query:\n",
    "                    break\n",
    "\n",
    "            total_queries += 1\n",
    "            if len(hard_neg_ids) >= negatives_per_query:\n",
    "                queries_with_negatives += 1\n",
    "\n",
    "            record = {\n",
    "                \"query_id\": query_id,\n",
    "                \"positive_doc_ids\": sorted(list(positives)),\n",
    "                \"hard_negative_doc_ids\": hard_neg_ids,\n",
    "                \"query_text\": queries[query_id],\n",
    "            }\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    logger.info(\n",
    "        f\"Finished mining: {queries_with_negatives}/{total_queries} queries \"\n",
    "        f\"have >= {negatives_per_query} hard negatives\"\n",
    "    )\n",
    "    logger.info(f\"Saved results to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2955445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hard_negatives(path: Path) -> dict:\n",
    "    \"\"\"Load mined hard negatives from JSONL file\"\"\"\n",
    "    hard_negatives = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line)\n",
    "            hard_negatives[record[\"query_id\"]] = record.get(\"hard_negative_doc_ids\", [])\n",
    "    return hard_negatives\n",
    "\n",
    "\n",
    "def load_training_data_with_hard_negatives(\n",
    "    data_dir: Path,\n",
    "    hard_negatives_path: Path,\n",
    "    negatives_per_query: int = 2,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Load training data with hard negatives.\n",
    "\n",
    "    For CachedMultipleNegativesRankingLoss, each example contains:\n",
    "    - Query text\n",
    "    - Positive document text\n",
    "    - Multiple hard negative document texts\n",
    "    \"\"\"\n",
    "    from sentence_transformers import InputExample\n",
    "\n",
    "    qrels_path = data_dir / \"qrels\" / \"train.tsv\"\n",
    "    queries_path = data_dir / \"queries.jsonl\"\n",
    "    corpus_path = data_dir / \"corpus.jsonl\"\n",
    "\n",
    "    logger.info(\"Loading training data with hard negatives...\")\n",
    "    query_to_docs = load_qrels(qrels_path)\n",
    "    queries = load_queries(queries_path)\n",
    "    corpus = load_corpus(corpus_path)\n",
    "    hard_negatives = load_hard_negatives(hard_negatives_path)\n",
    "\n",
    "    examples = []\n",
    "    skipped = 0\n",
    "\n",
    "    for query_id, doc_ids in query_to_docs.items():\n",
    "        if query_id not in queries:\n",
    "            continue\n",
    "\n",
    "        query_text = queries[query_id]\n",
    "        hard_negs = hard_negatives.get(query_id, [])\n",
    "\n",
    "        # Skip if not enough hard negatives\n",
    "        if len(hard_negs) < negatives_per_query:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        for doc_id in doc_ids:\n",
    "            if doc_id not in corpus:\n",
    "                continue\n",
    "\n",
    "            doc_text = corpus[doc_id]\n",
    "\n",
    "            # Get hard negative texts\n",
    "            neg_texts = [corpus[neg_id] for neg_id in hard_negs if neg_id in corpus][\n",
    "                :negatives_per_query\n",
    "            ]\n",
    "\n",
    "            if len(neg_texts) < negatives_per_query:\n",
    "                continue\n",
    "\n",
    "            # Create example with query, positive, and hard negatives\n",
    "            texts = [query_text, doc_text, *neg_texts]\n",
    "            examples.append(InputExample(texts=texts))\n",
    "\n",
    "    logger.info(f\"Created {len(examples)} training examples with hard negatives\")\n",
    "    if skipped > 0:\n",
    "        logger.info(f\"Skipped {skipped} queries without enough hard negatives\")\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_hard_negatives(\n",
    "    data_dir: Path,\n",
    "    hard_negatives_path: Path,\n",
    "    output_dir: str = OUTPUT_DIR_HARDNEG,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train embedding model with hard negatives using CachedMultipleNegativesRankingLoss.\n",
    "\n",
    "    This loss function is more efficient for training with explicit hard negatives\n",
    "    because it uses mini-batches within each batch for gradient accumulation.\n",
    "    \"\"\"\n",
    "    from sentence_transformers import SentenceTransformer, losses\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load model\n",
    "    logger.info(f\"Loading model: {MODEL_NAME_HARDNEG}\")\n",
    "    model = SentenceTransformer(MODEL_NAME_HARDNEG)\n",
    "\n",
    "    # Load training data with hard negatives\n",
    "    train_examples = load_training_data_with_hard_negatives(\n",
    "        data_dir,\n",
    "        hard_negatives_path,\n",
    "        negatives_per_query=NEGATIVES_PER_QUERY,\n",
    "    )\n",
    "    train_dataset = InputExampleDataset(train_examples)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=BATCH_SIZE_HARDNEG,\n",
    "        drop_last=True,  # Important for CachedMultipleNegativesRankingLoss\n",
    "    )\n",
    "\n",
    "    # Setup CachedMultipleNegativesRankingLoss\n",
    "    # This loss is optimized for training with explicit hard negatives\n",
    "    train_loss = losses.CachedMultipleNegativesRankingLoss(\n",
    "        model,\n",
    "        mini_batch_size=MINI_BATCH_SIZE_HARDNEG,\n",
    "    )\n",
    "\n",
    "    # Create validation evaluator\n",
    "    dev_evaluator = create_evaluator(data_dir, split=\"dev\")\n",
    "\n",
    "    # Calculate training steps\n",
    "    total_steps = len(train_dataloader) * EPOCHS_HARDNEG\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO_HARDNEG)\n",
    "\n",
    "    logger.info(f\"\\nTraining configuration (with hard negatives):\")\n",
    "    logger.info(f\"  Model: {MODEL_NAME_HARDNEG}\")\n",
    "    logger.info(f\"  Batch size: {BATCH_SIZE_HARDNEG}\")\n",
    "    logger.info(f\"  Mini-batch size: {MINI_BATCH_SIZE_HARDNEG}\")\n",
    "    logger.info(f\"  Hard negatives per query: {NEGATIVES_PER_QUERY}\")\n",
    "    logger.info(f\"  Epochs: {EPOCHS_HARDNEG}\")\n",
    "    logger.info(f\"  Total steps: {total_steps}\")\n",
    "    logger.info(f\"  Warmup steps: {warmup_steps}\")\n",
    "    logger.info(f\"  Learning rate: {LEARNING_RATE_HARDNEG}\")\n",
    "    logger.info(f\"  Output directory: {output_path}\")\n",
    "\n",
    "    # Start training\n",
    "    logger.info(\"\\nStarting training with hard negatives...\")\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        evaluator=dev_evaluator,\n",
    "        epochs=EPOCHS_HARDNEG,\n",
    "        warmup_steps=warmup_steps,\n",
    "        optimizer_params={\"lr\": LEARNING_RATE_HARDNEG},\n",
    "        output_path=str(output_path),\n",
    "        evaluation_steps=max(1, len(train_dataloader) // 2),\n",
    "        save_best_model=True,\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "\n",
    "    logger.info(f\"\\nTraining completed! Model saved to: {output_path}\")\n",
    "\n",
    "    # Final evaluation on test set\n",
    "    logger.info(\"\\nRunning final evaluation on test set...\")\n",
    "    test_evaluator = create_evaluator(data_dir, split=\"test\")\n",
    "    test_results = test_evaluator(model, output_path=str(output_path))\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\" * 50)\n",
    "    logger.info(\"Final Test Results (with hard negatives):\")\n",
    "    for metric, value in test_results.items():\n",
    "        logger.info(f\"  {metric}: {value:.4f}\")\n",
    "    logger.info(\"=\" * 50)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936fcc6",
   "metadata": {},
   "source": [
    "### Step 1: Mine Hard Negatives\n",
    "\n",
    "Use the base model (or previously fine-tuned model) to find hard negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb218c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mine hard negatives using the base model\n",
    "hard_negatives_path = Path(HARD_NEGATIVES_PATH)\n",
    "\n",
    "# Check if hard negatives already exist\n",
    "if hard_negatives_path.exists():\n",
    "    logger.info(f\"Hard negatives file already exists: {hard_negatives_path}\")\n",
    "    logger.info(\"Skipping mining step. Delete the file to re-mine.\")\n",
    "else:\n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(\"Mining hard negatives for training...\")\n",
    "    logger.info(\"=\" * 70)\n",
    "\n",
    "    # Load base model for mining\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    base_model = SentenceTransformer(MODEL_NAME_HARDNEG)\n",
    "\n",
    "    # Mine hard negatives\n",
    "    mine_hard_negatives(\n",
    "        model=base_model,\n",
    "        data_dir=beir_dir,\n",
    "        output_path=hard_negatives_path,\n",
    "        split=\"train\",\n",
    "        negatives_per_query=NEGATIVES_PER_QUERY,\n",
    "        top_k=50,\n",
    "        batch_size=64,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe9a3a",
   "metadata": {},
   "source": [
    "### Step 2: Train with Hard Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb98097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with hard negatives\n",
    "logger.info(\"\\n\" + \"=\" * 70)\n",
    "logger.info(\"Step 6.5: Training model with hard negatives\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "# Comment out the line below if you want to skip this training\n",
    "model_hardneg = train_model_with_hard_negatives(beir_dir, hard_negatives_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f173b743",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Step: Load pre-trained model with hard negatives (if already trained)\n",
    "# Uncomment this section if you already trained above\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model_hardneg = SentenceTransformer(OUTPUT_DIR_HARDNEG)\n",
    "# logger.info(f\"Model loaded from {OUTPUT_DIR_HARDNEG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226b743",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 7. Semantic Retrieval with Trained Model\n",
    "\n",
    "Demonstration of how to use the trained model for semantic retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2cca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search_demo(model, data_dir: Path, num_queries: int = 5):\n",
    "    \"\"\"\n",
    "    Demonstrate semantic search functionality\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sentence_transformers import util\n",
    "\n",
    "    # Load test data\n",
    "    queries_path = data_dir / \"queries.jsonl\"\n",
    "    corpus_path = data_dir / \"corpus.jsonl\"\n",
    "    qrels_path = data_dir / \"qrels\" / \"test.tsv\"\n",
    "\n",
    "    queries = load_queries(queries_path)\n",
    "    corpus = load_corpus(corpus_path)\n",
    "    query_to_docs = load_qrels(qrels_path)\n",
    "\n",
    "    # Encode corpus\n",
    "    logger.info(\"Encoding corpus...\")\n",
    "    corpus_ids = list(corpus.keys())\n",
    "    corpus_texts = [corpus[doc_id] for doc_id in corpus_ids]\n",
    "    corpus_embeddings = model.encode(\n",
    "        corpus_texts, convert_to_tensor=True, show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # Select test queries\n",
    "    test_queries = list(query_to_docs.keys())[:num_queries]\n",
    "\n",
    "    logger.info(f\"\\nRunning semantic search on {num_queries} test queries...\")\n",
    "\n",
    "    for query_id in test_queries:\n",
    "        query_text = queries[query_id]\n",
    "        ground_truth = query_to_docs[query_id]\n",
    "\n",
    "        # Encode query\n",
    "        query_embedding = model.encode(query_text, convert_to_tensor=True)\n",
    "\n",
    "        # Compute similarity and retrieve\n",
    "        cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "        top_results = np.argsort(-cos_scores.cpu().numpy())[:10]\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"Query: {query_text}\")\n",
    "        print(f\"Ground truth docs: {ground_truth}\")\n",
    "        print(\"\\nTop 10 retrieved documents:\")\n",
    "\n",
    "        for rank, idx in enumerate(top_results, 1):\n",
    "            doc_id = corpus_ids[idx]\n",
    "            score = cos_scores[idx].item()\n",
    "            is_relevant = \"✓\" if doc_id in ground_truth else \"✗\"\n",
    "            doc_preview = corpus[doc_id][:100] + \"...\"\n",
    "            print(f\"\\n{rank}. [{is_relevant}] Doc {doc_id} (score: {score:.4f})\")\n",
    "            print(f\"   {doc_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8664709",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Run retrieval demo\n",
    "semantic_search_demo(model, beir_dir, num_queries=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cdcb97",
   "metadata": {},
   "source": [
    "## 7.3. Load and Display Baseline Results\n",
    "\n",
    "Before evaluating our fine-tuned model, let's load the baseline results for comparison.\n",
    "\n",
    "**Note**: Make sure to run the baseline evaluation scripts first:\n",
    "- `uv run src/embedding_model/zero_shot.py` - Zero-shot MiniLM baseline\n",
    "- `uv run src/run_word2vec_eval.py` - Word2Vec baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99590fdb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load baseline results\n",
    "logger.info(\"Loading baseline model results...\")\n",
    "baseline_results = load_baseline_results(results_dir=RESULTS_DIR)\n",
    "\n",
    "# Display baseline comparison\n",
    "display_baseline_comparison(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810deef1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 7.5. Model Evaluation with traditional_eval\n",
    "Use the `traditional_eval` function to comprehensively evaluate the model's retrieval performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_retrieval(\n",
    "    model: Union[str, Any],\n",
    "    data_dir: Path,\n",
    "    split: str = \"test\",\n",
    "    k: int = 10,\n",
    "    batch_size: int = 64,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a SentenceTransformer model with BEIR's EvaluateRetrieval (cosine).\n",
    "\n",
    "    Args:\n",
    "        model: SentenceTransformer instance or model path string\n",
    "        data_dir: Path to BEIR format data directory\n",
    "        split: Which split to evaluate on (\"test\", \"dev\", or \"train\")\n",
    "        k: Top-k cutoff for evaluation metrics (used for Hit@k and MRR@k)\n",
    "        batch_size: Batch size for encoding\n",
    "\n",
    "    Returns:\n",
    "        metrics: Flat dict of metrics (BEIR-style keys + Hit/MRR)\n",
    "        samples: List of per-query results for downstream analysis\n",
    "    \"\"\"\n",
    "\n",
    "    from beir.datasets.data_loader import GenericDataLoader\n",
    "    from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "    from beir.retrieval.search.dense import DenseRetrievalExactSearch\n",
    "\n",
    "    class _SentenceTransformerWrapper:\n",
    "        \"\"\"Minimal wrapper exposing encode_corpus/encode_queries for BEIR.\"\"\"\n",
    "\n",
    "        def __init__(self, st_model: Any):\n",
    "            self.model = st_model\n",
    "\n",
    "        def encode_corpus(self, corpus, batch_size, **kwargs):\n",
    "            texts = [\n",
    "                doc.get(\"text\", \"\") if isinstance(doc, dict) else str(doc)\n",
    "                for doc in corpus\n",
    "            ]\n",
    "            return self.model.encode(\n",
    "                texts,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_numpy=True,\n",
    "            )\n",
    "\n",
    "        def encode_queries(self, queries, batch_size, **kwargs):\n",
    "            texts = [\n",
    "                query.get(\"text\", \"\") if isinstance(query, dict) else str(query)\n",
    "                for query in queries\n",
    "            ]\n",
    "            return self.model.encode(\n",
    "                texts,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_numpy=True,\n",
    "            )\n",
    "\n",
    "    # Load BEIR-formatted data\n",
    "    corpus, queries, qrels = GenericDataLoader(str(data_dir)).load(split=split)\n",
    "\n",
    "    # Prepare model wrapper (handle path vs instance)\n",
    "    if isinstance(model, str):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "\n",
    "        logger.info(f\"Loading model from path: {model}\")\n",
    "        st_model = SentenceTransformer(model)\n",
    "    else:\n",
    "        st_model = model\n",
    "\n",
    "    dres = DenseRetrievalExactSearch(\n",
    "        _SentenceTransformerWrapper(st_model), batch_size=batch_size\n",
    "    )\n",
    "    retriever = EvaluateRetrieval(dres, score_function=\"cos_sim\")\n",
    "\n",
    "    logger.info(f\"Running BEIR retrieval on split={split} ...\")\n",
    "    results = retriever.retrieve(corpus, queries)\n",
    "\n",
    "    logger.info(f\"Evaluating with BEIR metrics (k_values=[10, 100]) ...\")\n",
    "    ndcg, _map, recall, precision = retriever.evaluate(\n",
    "        qrels, results, k_values=[10, 100]\n",
    "    )\n",
    "\n",
    "    # Compute Hit@k and MRR@k manually from BEIR results\n",
    "    def hit_and_mrr_at_k(k_val: int):\n",
    "        hits = []\n",
    "        mrrs = []\n",
    "        for qid, doc_scores in results.items():\n",
    "            top_docs = [\n",
    "                doc_id\n",
    "                for doc_id, _ in sorted(\n",
    "                    doc_scores.items(), key=lambda x: x[1], reverse=True\n",
    "                )[:k_val]\n",
    "            ]\n",
    "            rels = qrels.get(qid, {})\n",
    "            rel_set = set(rels.keys()) if isinstance(rels, dict) else set(rels)\n",
    "\n",
    "            # Hit@k\n",
    "            hits.append(1.0 if any(d in rel_set for d in top_docs) else 0.0)\n",
    "\n",
    "            # MRR@k\n",
    "            mrr_val = 0.0\n",
    "            for rank, doc_id in enumerate(top_docs, start=1):\n",
    "                if doc_id in rel_set:\n",
    "                    mrr_val = 1.0 / rank\n",
    "                    break\n",
    "            mrrs.append(mrr_val)\n",
    "        n = len(hits) if hits else 1\n",
    "        return sum(hits) / n, sum(mrrs) / n\n",
    "\n",
    "    hit_k, mrr_k = hit_and_mrr_at_k(k)\n",
    "\n",
    "    metrics = {\n",
    "        \"NDCG@10\": float(ndcg.get(\"NDCG@10\", 0.0)),\n",
    "        \"NDCG@100\": float(ndcg.get(\"NDCG@100\", 0.0)),\n",
    "        \"MAP@10\": float(_map.get(\"MAP@10\", 0.0)),\n",
    "        \"MAP@100\": float(_map.get(\"MAP@100\", 0.0)),\n",
    "        \"Recall@10\": float(recall.get(\"Recall@10\", 0.0)),\n",
    "        \"Recall@100\": float(recall.get(\"Recall@100\", 0.0)),\n",
    "        \"Precision@10\": float(precision.get(\"P@10\", 0.0)),\n",
    "        \"Precision@100\": float(precision.get(\"P@100\", 0.0)),\n",
    "        f\"Hit@{k}\": float(hit_k),\n",
    "        \"MRR\": float(mrr_k),\n",
    "        \"N\": float(len(queries)),\n",
    "    }\n",
    "\n",
    "    # Build per-query samples for downstream analysis (e.g., analyze_query_performance)\n",
    "    samples = []\n",
    "    for qid, doc_scores in results.items():\n",
    "        sorted_docs = [\n",
    "            doc_id\n",
    "            for doc_id, _ in sorted(\n",
    "                doc_scores.items(), key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "        ]\n",
    "        samples.append(\n",
    "            {\n",
    "                \"question\": queries.get(qid, \"\"),\n",
    "                \"contexts\": sorted_docs,\n",
    "                \"ground_truth\": list(qrels.get(qid, {}).keys()),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return metrics, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation_results(\n",
    "    metrics: Dict[str, float], title: str = \"Evaluation Results\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Display evaluation results in a formatted table\n",
    "\n",
    "    Args:\n",
    "        metrics: Dictionary of metric name -> value\n",
    "        title: Title for the results display\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"{title:^70}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Group metrics by type\n",
    "    metric_groups = {\n",
    "        \"Retrieval Accuracy\": [\"Hit@10\"],\n",
    "        \"Precision & Recall\": [\"Precision@10\", \"Recall@10\"],\n",
    "        \"Ranking Quality\": [\"MRR\", \"MAP@10\", \"NDCG@10\"],\n",
    "        \"Dataset Info\": [\"N\"],\n",
    "    }\n",
    "\n",
    "    for group_name, metric_names in metric_groups.items():\n",
    "        print(f\"\\n{group_name}:\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        for metric_name in metric_names:\n",
    "            # Try different possible keys\n",
    "            possible_keys = [metric_name, metric_name.replace(\"@\", \"_at_\")]\n",
    "\n",
    "            for key in possible_keys:\n",
    "                if key in metrics:\n",
    "                    value = metrics[key]\n",
    "                    if key == \"N\":\n",
    "                        print(f\"  {metric_name:.<40} {int(value)}\")\n",
    "                    else:\n",
    "                        print(f\"  {metric_name:.<40} {value:.4f}\")\n",
    "                    break\n",
    "\n",
    "    print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_baseline(\n",
    "    finetuned_metrics: Dict[str, float],\n",
    "    baseline_metrics: Optional[Dict[str, float]] = None,\n",
    "    baseline_results: Optional[Dict[str, Dict[str, Any]]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare fine-tuned model with baseline\n",
    "\n",
    "    Args:\n",
    "        finetuned_metrics: Metrics from fine-tuned model\n",
    "        baseline_metrics: Metrics from baseline (optional, for backward compatibility)\n",
    "        baseline_results: All baseline results from load_baseline_results (optional)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Try to use loaded baseline results first\n",
    "    actual_baseline_metrics = baseline_metrics\n",
    "    if baseline_results is not None and \"Zero-shot MiniLM\" in baseline_results:\n",
    "        actual_baseline_metrics = baseline_results[\"Zero-shot MiniLM\"][\"metrics\"]\n",
    "        logger.info(\"Using loaded Zero-shot MiniLM baseline metrics\")\n",
    "    elif actual_baseline_metrics is None:\n",
    "        # Default baseline metrics (zero-shot MiniLM)\n",
    "        # These are example values - replace with actual baseline results\n",
    "        actual_baseline_metrics = {\n",
    "            \"Hit@10\": 0.75,\n",
    "            \"Precision@10\": 0.15,\n",
    "            \"Recall@10\": 0.70,\n",
    "            \"MRR\": 0.65,\n",
    "            \"MAP@10\": 0.55,\n",
    "            \"NDCG@10\": 0.68,\n",
    "        }\n",
    "        logger.info(\"Using default baseline metrics (approximate)\")\n",
    "\n",
    "    # Metrics to compare\n",
    "    metric_names = [\"Hit@10\", \"Precision@10\", \"Recall@10\", \"MRR\", \"MAP@10\", \"NDCG@10\"]\n",
    "\n",
    "    # Extract values\n",
    "    baseline_values = []\n",
    "    finetuned_values = []\n",
    "    display_names = []\n",
    "\n",
    "    for metric in metric_names:\n",
    "        # Try different key formats\n",
    "        possible_keys = [metric, metric.replace(\"@\", \"_at_\")]\n",
    "\n",
    "        for key in possible_keys:\n",
    "            if key in actual_baseline_metrics and key in finetuned_metrics:\n",
    "                baseline_values.append(actual_baseline_metrics[key])\n",
    "                finetuned_values.append(finetuned_metrics[key])\n",
    "                display_names.append(metric)\n",
    "                break\n",
    "\n",
    "    if not display_names:\n",
    "        logger.warning(\"No common metrics found for comparison\")\n",
    "        print(\n",
    "            \"\\nError: No matching metrics found between baseline and fine-tuned model.\"\n",
    "        )\n",
    "        print(\n",
    "            \"Available baseline metrics:\",\n",
    "            list(actual_baseline_metrics.keys()) if actual_baseline_metrics else \"None\",\n",
    "        )\n",
    "        print(\"Available fine-tuned metrics:\", list(finetuned_metrics.keys()))\n",
    "        return\n",
    "\n",
    "    # Create comparison plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Bar chart comparison\n",
    "    x = range(len(display_names))\n",
    "    width = 0.35\n",
    "\n",
    "    ax1.bar(\n",
    "        [i - width / 2 for i in x], baseline_values, width, label=\"Baseline\", alpha=0.8\n",
    "    )\n",
    "    ax1.bar(\n",
    "        [i + width / 2 for i in x],\n",
    "        finetuned_values,\n",
    "        width,\n",
    "        label=\"Fine-tuned\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    ax1.set_xlabel(\"Metrics\")\n",
    "    ax1.set_ylabel(\"Score\")\n",
    "    ax1.set_title(\"Baseline vs Fine-tuned Model\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(display_names, rotation=45, ha=\"right\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Improvement percentage (handle division by zero)\n",
    "    improvements = []\n",
    "    for b, f in zip(baseline_values, finetuned_values):\n",
    "        if b == 0:\n",
    "            # If baseline is 0, use absolute difference instead\n",
    "            improvements.append(f * 100)\n",
    "        else:\n",
    "            improvements.append((f - b) / b * 100)\n",
    "\n",
    "    colors = [\"green\" if imp > 0 else \"red\" for imp in improvements]\n",
    "\n",
    "    ax2.bar(x, improvements, color=colors, alpha=0.7)\n",
    "    ax2.set_xlabel(\"Metrics\")\n",
    "    ax2.set_ylabel(\"Improvement (%)\")\n",
    "    ax2.set_title(\"Relative Improvement over Baseline\")\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(display_names, rotation=45, ha=\"right\")\n",
    "    ax2.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print improvement summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Improvement Summary\")\n",
    "    print(\"=\" * 70)\n",
    "    for name, baseline, finetuned, improvement in zip(\n",
    "        display_names, baseline_values, finetuned_values, improvements\n",
    "    ):\n",
    "        if baseline == 0:\n",
    "            # Special display for zero baseline\n",
    "            print(f\"{name:.<30} {baseline:.4f} → {finetuned:.4f} (baseline was 0)\")\n",
    "        else:\n",
    "            arrow = \"↑\" if improvement > 0 else \"↓\"\n",
    "            print(\n",
    "                f\"{name:.<30} {baseline:.4f} → {finetuned:.4f} ({arrow} {abs(improvement):.2f}%)\"\n",
    "            )\n",
    "    print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c9cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_models(\n",
    "    finetuned_metrics: Dict[str, float],\n",
    "    baseline_results_dict: Dict[str, Dict[str, Any]],\n",
    "    finetuned_name: str = \"Fine-tuned Model\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare fine-tuned model with all baseline models\n",
    "\n",
    "    Args:\n",
    "        finetuned_metrics: Metrics from fine-tuned model\n",
    "        baseline_results_dict: All baseline results from load_baseline_results\n",
    "        finetuned_name: Name for the fine-tuned model\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    if not baseline_results_dict:\n",
    "        logger.warning(\"No baseline results available for comparison\")\n",
    "        return\n",
    "\n",
    "    # Prepare data for comparison\n",
    "    models_data = {}\n",
    "\n",
    "    # Add baselines\n",
    "    for model_name, result in baseline_results_dict.items():\n",
    "        models_data[model_name] = result.get(\"metrics\", {})\n",
    "\n",
    "    # Add fine-tuned model\n",
    "    models_data[finetuned_name] = finetuned_metrics\n",
    "\n",
    "    # Metrics to compare\n",
    "    metric_names = [\"Hit@10\", \"Precision@10\", \"Recall@10\", \"MRR\", \"MAP@10\", \"NDCG@10\"]\n",
    "\n",
    "    # Prepare plotting data\n",
    "    model_names = list(models_data.keys())\n",
    "    n_models = len(model_names)\n",
    "    n_metrics = len(metric_names)\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_models))  # type: ignore\n",
    "\n",
    "    for idx, metric in enumerate(metric_names):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        values = []\n",
    "        labels = []\n",
    "\n",
    "        for model_name in model_names:\n",
    "            metrics = models_data[model_name]\n",
    "            # Try both formats\n",
    "            value = metrics.get(metric, metrics.get(metric.replace(\"@\", \"_at_\"), 0.0))\n",
    "            if isinstance(value, (int, float)):\n",
    "                values.append(value)\n",
    "                labels.append(model_name)\n",
    "\n",
    "        if values:\n",
    "            bars = ax.bar(\n",
    "                range(len(values)), values, color=colors[: len(values)], alpha=0.8\n",
    "            )\n",
    "            ax.set_title(metric, fontsize=12, fontweight=\"bold\")\n",
    "            ax.set_ylabel(\"Score\", fontsize=10)\n",
    "            ax.set_xticks(range(len(labels)))\n",
    "            ax.set_xticklabels(labels, rotation=45, ha=\"right\", fontsize=9)\n",
    "            ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "            # Add value labels on bars\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(\n",
    "                    bar.get_x() + bar.get_width() / 2.0,\n",
    "                    height,\n",
    "                    f\"{height:.3f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=8,\n",
    "                )\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Model Comparison: Fine-tuned vs Baselines\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        y=1.00,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed comparison table\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"DETAILED MODEL COMPARISON\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Header\n",
    "    print(f\"\\n{'Model':<25}\", end=\"\")\n",
    "    for metric in metric_names:\n",
    "        print(f\"{metric:>12}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Data rows\n",
    "    for model_name in model_names:\n",
    "        metrics = models_data[model_name]\n",
    "        print(f\"{model_name:<25}\", end=\"\")\n",
    "        for metric in metric_names:\n",
    "            value = metrics.get(metric, metrics.get(metric.replace(\"@\", \"_at_\"), 0.0))\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"{value:>12.4f}\", end=\"\")\n",
    "            else:\n",
    "                print(f\"{str(value):>12}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "    print(\"=\" * 100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ecb78d",
   "metadata": {},
   "source": [
    "### Run Complete Evaluation\n",
    "\n",
    "Evaluate the fine-tuned model on the test set using `traditional_eval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"Running complete evaluation on test set\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "test_metrics, test_samples = evaluate_model_retrieval(\n",
    "    model=model, data_dir=beir_dir, split=\"test\", k=10\n",
    ")\n",
    "\n",
    "# Display results\n",
    "display_evaluation_results(test_metrics, title=\"Test Set Evaluation Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Evaluate on dev set as well\n",
    "logger.info(\"Running evaluation on dev set for comparison...\")\n",
    "\n",
    "dev_metrics, dev_samples = evaluate_model_retrieval(\n",
    "    model=model, data_dir=beir_dir, split=\"dev\", k=10\n",
    ")\n",
    "\n",
    "display_evaluation_results(dev_metrics, title=\"Dev Set Evaluation Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69211dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with baseline (zero-shot only)\n",
    "compare_with_baseline(test_metrics, baseline_results=baseline_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c3f23",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Compare with all baseline models\n",
    "logger.info(\"Generating comprehensive model comparison...\")\n",
    "compare_all_models(test_metrics, baseline_results, finetuned_name=\"Fine-tuned MiniLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4196ed",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Analyze Individual Query Performance\n",
    "\n",
    "Examine specific queries to understand model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd07ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query_performance(samples: List[Dict[str, Any]], num_examples: int = 5):\n",
    "    \"\"\"\n",
    "    Analyze performance on individual queries\n",
    "\n",
    "    Args:\n",
    "        samples: Evaluation samples from evaluate_model_retrieval\n",
    "        num_examples: Number of examples to display\n",
    "    \"\"\"\n",
    "    # Calculate per-query metrics\n",
    "    query_metrics = []\n",
    "\n",
    "    for sample in samples:\n",
    "        gt_set = set(str(x) for x in sample[\"ground_truth\"])\n",
    "        results = [str(x) for x in sample[\"contexts\"]]\n",
    "\n",
    "        # Hit@10\n",
    "        hit = 1.0 if any(r in gt_set for r in results[:10]) else 0.0\n",
    "\n",
    "        # Recall@10\n",
    "        hits_count = sum(1 for r in results[:10] if r in gt_set)\n",
    "        recall = hits_count / len(gt_set) if gt_set else 0.0\n",
    "\n",
    "        # MRR\n",
    "        mrr = 0.0\n",
    "        for rank, r in enumerate(results, start=1):\n",
    "            if r in gt_set:\n",
    "                mrr = 1.0 / rank\n",
    "                break\n",
    "\n",
    "        query_metrics.append(\n",
    "            {\n",
    "                \"query\": sample[\"question\"],\n",
    "                \"hit\": hit,\n",
    "                \"recall\": recall,\n",
    "                \"mrr\": mrr,\n",
    "                \"num_relevant\": len(gt_set),\n",
    "                \"retrieved\": results[:10],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Sort by MRR (worst first)\n",
    "    query_metrics.sort(key=lambda x: x[\"mrr\"])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Worst {num_examples} Queries (by MRR)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, qm in enumerate(query_metrics[:num_examples], 1):\n",
    "        print(f\"\\n{i}. Query: {qm['query'][:80]}...\")\n",
    "        print(\n",
    "            f\"   Hit@10: {qm['hit']:.0f} | Recall@10: {qm['recall']:.4f} | MRR: {qm['mrr']:.4f}\"\n",
    "        )\n",
    "        print(f\"   Relevant docs: {qm['num_relevant']}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Best {num_examples} Queries (by MRR)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, qm in enumerate(query_metrics[-num_examples:][::-1], 1):\n",
    "        print(f\"\\n{i}. Query: {qm['query'][:80]}...\")\n",
    "        print(\n",
    "            f\"   Hit@10: {qm['hit']:.0f} | Recall@10: {qm['recall']:.4f} | MRR: {qm['mrr']:.4f}\"\n",
    "        )\n",
    "        print(f\"   Relevant docs: {qm['num_relevant']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cffec9a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Analyze query performance\n",
    "analyze_query_performance(test_samples, num_examples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbe3ccc",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate Hard Negatives Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55c0347",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Evaluate the hard negatives model on test set\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"Evaluating hard negatives model on test set\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "test_metrics_hardneg, test_samples_hardneg = evaluate_model_retrieval(\n",
    "    model=model_hardneg, data_dir=beir_dir, split=\"test\", k=10\n",
    ")\n",
    "\n",
    "# Display results\n",
    "display_evaluation_results(\n",
    "    test_metrics_hardneg, title=\"Hard Negatives Model - Test Set Results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b79542",
   "metadata": {},
   "source": [
    "### Step 4: Compare All Models (Baseline + Standard Fine-tuning + Hard Negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f174a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Compare all three approaches\n",
    "logger.info(\n",
    "    \"Generating comprehensive comparison: Baseline vs Fine-tuned vs Hard Negatives...\"\n",
    ")\n",
    "\n",
    "# Prepare data for comparison\n",
    "all_models_results = baseline_results.copy()\n",
    "\n",
    "# Add standard fine-tuned model\n",
    "all_models_results[\"Fine-tuned (Standard MNRL)\"] = {\n",
    "    \"metrics\": test_metrics,\n",
    "    \"model_type\": \"Fine-tuned\",\n",
    "    \"base_model\": MODEL_NAME,\n",
    "}\n",
    "\n",
    "# Add hard negatives model\n",
    "all_models_results[\"Fine-tuned (Hard Negatives)\"] = {\n",
    "    \"metrics\": test_metrics_hardneg,\n",
    "    \"model_type\": \"Fine-tuned with Hard Negatives\",\n",
    "    \"base_model\": MODEL_NAME_HARDNEG,\n",
    "}\n",
    "\n",
    "# Display comprehensive comparison\n",
    "compare_all_models(test_metrics, baseline_results, finetuned_name=\"Fine-tuned MiniLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0fb72c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Detailed Comparison: Standard vs Hard Negatives Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_finetuning_methods(\n",
    "    standard_metrics: Dict[str, float],\n",
    "    hardneg_metrics: Dict[str, float],\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare standard fine-tuning vs hard negatives fine-tuning\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metric_names = [\"Hit@10\", \"Precision@10\", \"Recall@10\", \"MRR\", \"MAP@10\", \"NDCG@10\"]\n",
    "\n",
    "    standard_values = []\n",
    "    hardneg_values = []\n",
    "    display_names = []\n",
    "\n",
    "    for metric in metric_names:\n",
    "        if metric in standard_metrics and metric in hardneg_metrics:\n",
    "            standard_values.append(standard_metrics[metric])\n",
    "            hardneg_values.append(hardneg_metrics[metric])\n",
    "            display_names.append(metric)\n",
    "\n",
    "    if not display_names:\n",
    "        logger.warning(\"No common metrics found for comparison\")\n",
    "        return\n",
    "\n",
    "    # Create comparison plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Bar chart comparison\n",
    "    x = range(len(display_names))\n",
    "    width = 0.35\n",
    "\n",
    "    ax1.bar(\n",
    "        [i - width / 2 for i in x],\n",
    "        standard_values,\n",
    "        width,\n",
    "        label=\"Standard MNRL\",\n",
    "        alpha=0.8,\n",
    "        color=\"steelblue\",\n",
    "    )\n",
    "    ax1.bar(\n",
    "        [i + width / 2 for i in x],\n",
    "        hardneg_values,\n",
    "        width,\n",
    "        label=\"Hard Negatives\",\n",
    "        alpha=0.8,\n",
    "        color=\"orange\",\n",
    "    )\n",
    "    ax1.set_xlabel(\"Metrics\")\n",
    "    ax1.set_ylabel(\"Score\")\n",
    "    ax1.set_title(\"Standard Fine-tuning vs Hard Negatives Fine-tuning\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(display_names, rotation=45, ha=\"right\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Improvement percentage\n",
    "    improvements = [\n",
    "        (h - s) / s * 100 if s > 0 else 0\n",
    "        for s, h in zip(standard_values, hardneg_values)\n",
    "    ]\n",
    "\n",
    "    colors = [\"green\" if imp > 0 else \"red\" for imp in improvements]\n",
    "\n",
    "    ax2.bar(x, improvements, color=colors, alpha=0.7)\n",
    "    ax2.set_xlabel(\"Metrics\")\n",
    "    ax2.set_ylabel(\"Improvement (%)\")\n",
    "    ax2.set_title(\"Hard Negatives Improvement over Standard Fine-tuning\")\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(display_names, rotation=45, ha=\"right\")\n",
    "    ax2.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print comparison summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Fine-tuning Methods Comparison\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        f\"\\n{'Metric':<20} {'Standard MNRL':>15} {'Hard Negatives':>15} {'Improvement':>15}\"\n",
    "    )\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for name, std_val, hn_val, imp in zip(\n",
    "        display_names, standard_values, hardneg_values, improvements\n",
    "    ):\n",
    "        arrow = \"↑\" if imp > 0 else \"↓\"\n",
    "        print(\n",
    "            f\"{name:<20} {std_val:>15.4f} {hn_val:>15.4f} {arrow:>2} {abs(imp):>12.2f}%\"\n",
    "        )\n",
    "\n",
    "    print(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a395a5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Compare the two fine-tuning approaches\n",
    "compare_finetuning_methods(test_metrics, test_metrics_hardneg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cf7fa2",
   "metadata": {},
   "source": [
    "## 8. Results Visualization\n",
    "\n",
    "Visualize training results and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb90bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib for notebook display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# For Jupyter/IPython environments\n",
    "try:\n",
    "    from IPython.core.getipython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    if ipython is not None:\n",
    "        ipython.run_line_magic(\"matplotlib\", \"inline\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Set default style\n",
    "plt.style.use(\"default\")\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee7f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_results(results_csv_path: str):\n",
    "    \"\"\"\n",
    "    Plot evaluation results\n",
    "\n",
    "    Args:\n",
    "        results_csv_path: Path to the evaluation results CSV file\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if not Path(results_csv_path).exists():\n",
    "        logger.warning(f\"Results file not found: {results_csv_path}\")\n",
    "        print(f\"File not found: {results_csv_path}\")\n",
    "        print(\n",
    "            f\"Please make sure the model has been trained and evaluation results exist.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Read results\n",
    "    df = pd.read_csv(results_csv_path)\n",
    "    # take the last evaluation result of this epoch\n",
    "    df[\"epoch\"] = pd.to_numeric(df[\"epoch\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"epoch\"])\n",
    "\n",
    "    if df[\"epoch\"].duplicated().any():\n",
    "        if \"steps\" in df.columns:\n",
    "            df[\"steps\"] = pd.to_numeric(df[\"steps\"], errors=\"coerce\").fillna(-1)\n",
    "            df = (\n",
    "                df.sort_values([\"epoch\", \"steps\"])\n",
    "                .groupby(\"epoch\", as_index=False)\n",
    "                .last()\n",
    "            )\n",
    "        else:\n",
    "            df = df.groupby(\"epoch\", as_index=False).last()\n",
    "\n",
    "    print(f\"Loaded results with {len(df)} rows\")\n",
    "    print(f\"Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Define metrics to plot (try both naming conventions)\n",
    "    # sentence-transformers uses format: \"cosine-NDCG@10\"\n",
    "    # Alternative format: \"ndcg_at_10\"\n",
    "    # metric_mappings = {\n",
    "    #     \"NDCG@10\": [\"cosine-NDCG@10\", \"ndcg_at_10\"],\n",
    "    #     \"MAP@100\": [\"cosine-MAP@100\", \"map_at_100\"],\n",
    "    #     \"Recall@10\": [\"cosine-Recall@10\", \"recall_at_10\"],\n",
    "    #     \"Precision@10\": [\"cosine-Precision@10\", \"precision_at_10\"],\n",
    "    # }\n",
    "    metric_mappings = {\n",
    "        \"NDCG@10\": \"cosine-NDCG@10\",\n",
    "        \"MAP@100\": \"cosine-MAP@100\",\n",
    "        \"Recall@10\": \"cosine-Recall@10\",\n",
    "        \"Precision@10\": \"cosine-Precision@10\",\n",
    "    }\n",
    "\n",
    "    # Find which metrics are available\n",
    "    available_metrics = {k: v for k, v in metric_mappings.items() if v in df.columns}\n",
    "\n",
    "    if not available_metrics:\n",
    "        print(\"Warning: cosine metrics not found in CSV\")\n",
    "        print(f\"Available columns: {df.columns.tolist()}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nPlotting metrics: {list(available_metrics.keys())}\")\n",
    "\n",
    "    # Create subplots\n",
    "    n_metrics = len(available_metrics)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (display_name, col_name) in enumerate(available_metrics.items()):\n",
    "        ax = axes[i]\n",
    "        ax.plot(\n",
    "            df[\"epoch\"],\n",
    "            df[col_name],\n",
    "            marker=\"o\",\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "            color=\"steelblue\",\n",
    "        )\n",
    "        ax.set_title(display_name, fontsize=12, fontweight=\"bold\")\n",
    "        ax.set_xlabel(\"Epoch\", fontsize=10)\n",
    "        ax.set_ylabel(\"Score\", fontsize=10)\n",
    "        ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "        # Add value annotations on points (only if not too many points)\n",
    "        if len(df) <= 20:\n",
    "            for x, y in zip(df[\"epoch\"], df[col_name]):\n",
    "                ax.annotate(\n",
    "                    f\"{y:.3f}\",\n",
    "                    (x, y),\n",
    "                    textcoords=\"offset points\",\n",
    "                    xytext=(0, 5),\n",
    "                    ha=\"center\",\n",
    "                    fontsize=7,\n",
    "                    alpha=0.7,\n",
    "                )\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for i in range(n_metrics, 4):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.suptitle(\"Training Evaluation Results\", fontsize=14, fontweight=\"bold\", y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    df = df.sort_values(\"epoch\").reset_index(drop=True)\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Evaluation Results Summary\")\n",
    "    print(\"=\" * 70)\n",
    "    for display_name, col_name in available_metrics.items():\n",
    "        best_idx = df[col_name].idxmax()\n",
    "        best_epoch = df.loc[best_idx, \"epoch\"]\n",
    "        best_score = df[col_name].max()\n",
    "        final_score = df[col_name].iloc[-1]\n",
    "        print(f\"{display_name}:\")\n",
    "        print(f\"  Best:  {best_score:.4f} (epoch {int(best_epoch)})\")\n",
    "        print(f\"  Final: {final_score:.4f}\")\n",
    "        base = df[col_name].iloc[0]\n",
    "        improve = ((final_score - base) / base * 100) if base != 0 else float(\"nan\")\n",
    "        print(f\"  Improvement: {improve:.2f}%\")\n",
    "        print(\"=\" * 70)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c2638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results (if training logs exist)\n",
    "# Uncomment the lines below after training is complete\n",
    "results_path = f\"{OUTPUT_DIR}/eval/Information-Retrieval_evaluation_dev_results.csv\"\n",
    "plot_evaluation_results(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3966e30",
   "metadata": {},
   "source": [
    "### Visualize Hard Negatives Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2894540b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Visualize hard negatives training results\n",
    "results_path_hardneg = (\n",
    "    f\"{OUTPUT_DIR_HARDNEG}/eval/Information-Retrieval_evaluation_dev_results.csv\"\n",
    ")\n",
    "plot_evaluation_results(results_path_hardneg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d3bd0",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "This notebook implements a complete semantic retrieval system, including:\n",
    "\n",
    "1. **Data Loading and Preprocessing**: Load SciFact dataset from HuggingFace and convert to BEIR format\n",
    "2. **Model Training**: Fine-tune sentence-transformers model using MultipleNegativesRankingLoss\n",
    "3. **Evaluation**: Evaluate model performance on test set with multiple retrieval metrics\n",
    "4. **Inference**: Perform semantic retrieval using the trained model"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
